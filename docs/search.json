[{"path":"index.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"online book ECON 5/ POLI 5D: Introduction Social Data Analytics UC San Diego.course three main goals. first goal introduce interesting important social science questions. chapter highlight different application, often highlighting research faculty UC San Diego. cover wide range topics, including colleges promote intergenerational mobility, motivates people vote, identify discrimination labor markets, among others.second goal show data can used inform discussion topics. using lot different datasets course. come experiments run researchers. Others come administrative datasets collected governments. many datasets, time scratch surface possible data.third goal give tools perform data analysis. focus three popular software: Excel (1 chapter), Stata (4 chapters) R (5 chapters). learning coding fundamentals programs, shed light big social science questions.Note authorship: book written David Arnold. R sections (Chapters 6-10) based examples slides Margaret Roberts lectures Econ 5/Poli 5D.","code":""},{"path":"college-instruction-experiments.html","id":"college-instruction-experiments","chapter":"1 College Instruction Experiments","heading":"1 College Instruction Experiments","text":"","code":""},{"path":"college-instruction-experiments.html","id":"experiments","chapter":"1 College Instruction Experiments","heading":"1.1 Experiments","text":"know countries higher consumption chocolate per capita tend win Nobel prizes?\nFigure 1.1: Chocolate Consumption Nobel Prizes\nFigure 1.1, number Nobel laureates per 10 million population (vertical axis) plotted chocolate consumption kilograms per capita given year (horizontal axis). clear graph, countries higher rates chocolate consumption also tend higher rates Nobel laureates. example correlation. correlation statistical relationship two variables.Correlations can interesting. many cases can lead new research questions. often answer questions social scientists actually interested . generally interested understanding causal relationships variables.understand causal relationships, consider relationship chocolate consumption Nobel prizes. think U.S. wanted increase number Nobel laureates, invest chocolate include chocolate lunch meals students? Probably ! unlikely increased chocolate consumption causes individual win Nobel prize. case, likely correlation data just arose due chance. Therefore, uncovered spurious correlation, defined correlation arose due chance. many interesting spurious correlations (See spurious-correlations). example, probably guess consumption mozzarella cheese correlated civil engineering doctorates awarded, see Figure 1.2 surprising correlation.\nFigure 1.2: Example Spurious Correlation tylervigen.com\nspurious correlations can interesting fun, need decide whether given policy implemented, need understand causally impacts individual's outcomes. see , imagine high school trying decide whether invest new -school program designed help students standardized tests. Currently, program campus, know students attend programs offered outside school. order try understand benefits program program, school decides use data see attended standardized test prep programs outperformed students .school finds attended test prep programs score much higher standardized tests . mean test prep program works school invest one?Maybe, maybe . two stories potentially rationalize finding. Story 1 programs effective ways students learn material need perform well standardized tests. , program causal impact SAT scores. Story 2 programs actually effective. However, students self-select test prep programs also tend motivated overall. , theory, vary along many dimensions: academic ability, interest college, parental background. differences actually driving differences standardized test scores.observational data difficult tell story true. However, long-established way identify causal relationship medicine: randomized control trial (RCT). understand RCTs, imagine testing whether given drug reduces blood pressure. study effectiveness recruit group participants. Half participants randomly placed treatment group given drug. half placed control placebo group given placebo drug. Now can just compare outcomes two groups time understand impact drug.key well-done experiment randomization. receives treatment completely random, average, two groups similar. example, test prep example, randomly split students two groups: test prep vs. test prep, evaluate impacts program. Since students longer self-selecting program, little difference two groups terms academic ability, college interest, parental background. ? randomized students program.questions social science can answered experiment. certain cases may prohibitively expensive unethical run experiment. However, experiment can done, hard find convincing evidence. Experiments widely influential many social sciences. example, field development economics, large number experiments undertaken order develop policies aimed alleviating global povery. 2019, Abhijit Banerjee, Esther Duflo, Michael Kremer won Nobel Prize Economics \"experimental approach alleviating global poverty.\"course, studying number social science experiments. first experiment comes Brownback Sadoff (2020). motivation experiment startling statistic. 40 percent community college students earn college degree within 6 years (Shapiro et al. 2017). Given poor outcomes, important consider tools improve performance. Brownback Sadoff (2020) researchers explore one potential tool previously unexplored: financial incentives instructors. often referred pay--performance model. Instructors paid well students perform.get specific, Brownback Sadoff (2020) run following experiment community college Indiana (Ivy Tech):instructors (treatment group) received $50 student passes externally-administered testOther instructors (control group) receive incentive.key design instructors chosen receive incentives randomly selected. Therefore, , average, better teachers instructors control group. Therefore, observe difference student performance treatment control, must due financial incentives instructors!Question: key aspect experiment allows us infer causation ...","code":""},{"path":"college-instruction-experiments.html","id":"community-college-data","chapter":"1 College Instruction Experiments","heading":"1.2 Community College Data","text":"many disciplines, push toward transparency replication. Past work shown influential studies failed replicate (see ). common way provide transparency authors make data research publicly available. means get use actual data experiments studying class.Table 1.3 shows selection data Brownstone Sadoff (2020). actual data much larger, subset variables relevant analysis. referred data table. row data table observation, column variable.\nFigure 1.3: Data Brownstone Sadoff (2020)\nalways important understand structure data proceeding data analysis. One important components understanding structure discern unit observation. unit observation level data reported. discern , just think row dataset represents. row individual, neighborhood, state, country? practice, go examples.unit observation Figure 1.4, displays data table just two observations.\nFigure 1.4: Unit Observation Example 1\nanswer question, need decide row represents. case, row corresponds different student. Therefore observation level student.unit observation Figure 1.5?\nFigure 1.5: Unit Observation Example 2\n, need decide row represents. case, row corresponds particular student particular term. Therefore observation level student-term.Now think unit observation Brownstone Sadoff (2020) data. variable AnonID identifier student. variable CourseNumber identifier course. dataset seems giving us information student given course. Therefore observation level student-course.get analyzing data, actually learn first feature Excel. browsing excel file, often convenient freeze first row always visible. case, first row holds variable names. Therefore, freeze first row can scroll data without remember column corresponds .freeze first row go View click Freeze Top Row.\nFigure 1.6: Freezing First Row\ntalk little bit goal dataset. goal determine providing financial incentives teachers impacts student performance. variable Treatment Arm details whether student instructor received incentives control group. Therefore, can see test scores improve based Treatment Arm student. perform analysis, need learn use functions Excel.Question: Imagine dataset row contains average unemployment rate given state given year. unit--observation dataset ","code":""},{"path":"college-instruction-experiments.html","id":"statistical-functions","chapter":"1 College Instruction Experiments","heading":"1.3 Statistical Functions","text":"function something takes input produces output. example, can think taking average function. input list numbers, output average list numbers. inputs outputs function numbers, statistical function.Excel, statistical functions extremely important understand. research, common start list summary statistics. business, might important know summary statistics products. create summary statistics use statistical functions.order illustrate use statistical functions, consider following dataset information Gross Domestic Product (GDP) ten countries\nFigure 1.7: GDP 2020\nintroduce first statistical function: AVERAGE function. function take list numbers output average numbers. many statistical functions Excel, including:COUNT -- counts number numbersCOUNT -- counts number numbersSUM -- adds numbersSUM -- adds numbersMEDIAN -- retrieves median numbersMEDIAN -- retrieves median numbersMAX, MIN -- retrieves maximum minimum numbers\nrespectivelyMAX, MIN -- retrieves maximum minimum numbers\nrespectivelyMODE -- retrieves mode numbersMODE -- retrieves mode numbersIn order make use statistical functions, first need learn reference cells. Excel, every cell identified column letter row number. example, Figure 1.8, clicked cell holds word \"Japan\". entry column row 4. Therefore, cell A4.\nFigure 1.8: Reference Cell\ncan also reference range cells (useful taking averages). example, reference rows 2-6 column C type C2:C6 (See Figure 1.9). Whenever read colon Excel, read \"\". Therefore, text C2:C6 can read \"cells C2 C6\".\nFigure 1.9: Referencing Range Cells\nSometimes, helpful reference entire column. example, later open Excel spreadsheet thousands students, might make difficult reference exact row data ends. can still compactly take average variable without referencing observations. example, variable column C, can type C:C reference entire column.Now understand reference cells, can begin apply functions. begin AVERAGE functions can compute average list numbers. spreadsheet, interested computing average GDP across ten countries. , can first click unpopulated cell (.e. cell without values yet) type:Note = sign text . tells Excel evaluate function. omit = find cell just populated text AVERAGE(C:C) average value column C.Figure 1.10 depicts example set Summary Statistics table dataset. cell F3, written function calculates average GDP across ten countries.\nFigure 1.10: Computing Average GDP 2020 (Pressing Enter)\nsoon present enter, function evaluated average appear cell F3 (see Figure 1.11).\nFigure 1.11: Computing Average GDP 2020 (Pressing Enter)\nrest summary statistics completely analogous, replacing AVERAGE relevant function. See Figure 1.12 output rest Summary Statistics table.\nFigure 1.12: Summary Statistics Table\nQuestion: try take AVERAGE column specifying AVERAGE(C:C). Yet, press enter, Excel compute average. gone wrong","code":""},{"path":"college-instruction-experiments.html","id":"logical-functions","chapter":"1 College Instruction Experiments","heading":"1.4 Logical Functions","text":"Another type function Excel called logical function. learning logical functions need learn logical statements general. logical statement statement either true false. example, following statements either true false.\"student passed class\"\"student passed class\"\"participant treatment group\"\"participant treatment group\"\"temperature freezing\"\"temperature freezing\"statements either true false. type binary logic incredibly important certain areas mathematics well development computers. entire branch mathematics, termed Boolean algebra, deals variables take binary values.Excel, can type logical statements Excel evaluate whether statement True False. key need type statement correct format Excel understands trying ask.example, dataset GDP 2020, logical statement : \"GDP greater 10 trillion dollars.\" True countries False others. type statement \"GDP greater 10 trillion dollars\" Excel?Figure 1.13 depicts type statement first observation dataset (U.S.). evaluate whether GDP greater 10 trillion U.S. type =(C2>10). equals sign tells Excel evaluate logical statement follows. logical statement follows C2>10 simply asking whether value cell C2 (.e. U.S.'s GDP) greater 10.\nFigure 1.13: GDP Greater 10 U.S. Part 1\npress enter, get result. case, can see GDP U.S. indeed greater 10 trillion dollars. Therefore, function cell D2 evaluated returns answer TRUE, can seen Figure 1.14.\nFigure 1.14: GDP Greater 10 U.S. Part 2\nfilled formula one cell, can also apply remaining observations. example, mean testing whether country's GDP 10, just U.S. couple ways efficiently Excel.One way fill formula click bottom right corner cell D2 drag . allows apply many observations like. However, often want apply logical statement observations. Therefore, continuously dragging inefficient thousands observations data. quickly apply formula cells column, can simply double click bottom right corner cell. example, mean double clicking bottom right corner D2 cell.\nFigure 1.15: Filling Formula\nOftentimes, instead returning \"TRUE\" \"FALSE\", convenient Excel return value another form. example, common way represent \"TRUE\" number 1 \"FALSE\" number 0. accomplish Excel use function.basic format function :=(logical test,value TRUE, value FALSE)example, dataset, want return value 1 GDP greater 10 0 otherwise, can type:=(C2>10,1,0)\nFigure 1.16: Using function (Pressing Enter)\npress enter, function evaluated. fill formula rest observations can follow steps .\nFigure 1.17: Using function (Pressing Enter)\n","code":""},{"path":"college-instruction-experiments.html","id":"summary-statistics","chapter":"1 College Instruction Experiments","heading":"1.5 Summary Statistics","text":"Now return data Brownback Sadoff (2020). goal create table basic summary statistics. However, key variable Brownback Sadoff (2020) experiment whether given student passed test . setting, score 70 higher indicates student passed test. Therefore need create new variable equal 1 individual received TestScore greater equal 70 zero otherwise. show later storing information 1 zero particularly convenient analysis.create variable need take three steps:First Step: Title column variable name \"Passed\"First Step: Title column variable name \"Passed\"Second Step: Use function fill variable first observationSecond Step: Use function fill variable first observationThird Step: Double click bottom right cell step 2 fill variable observationsThird Step: Double click bottom right cell step 2 fill variable observationsFigure 1.18 presents steps 1 2 Excel.\nFigure 1.18: Generating Passed Variable (Steps 1 2)\nsimply can double click bottom right G2 fill rest observations (seen Figure 1.19).\nFigure 1.19: Generating Passed Variable (Step 3)\nNow generated variables need fill summary statistics displayed right side datasheet Figure 1.20\nFigure 1.20: Blank Summary Statistics Table\ncompute average age dataset just need use AVERAGE function reference column C.\nFigure 1.21: Average Age\nAverage test score completely analogous age, now reference column F.\nFigure 1.22: Average Test Score\norder compute fraction passed test, first going make small digression, one clarify one reason convenient store variable Passed 1 individual passed zero otherwise. stored variable way, can simply take average value Passed equal fraction passed test.see true, first write equation average generic variable \\(X\\):\\[\n  \\bar{X} = \\frac{X_1 + ... + X_N}{N}\n\\]\\(\\bar{X}\\) average, \\(N\\) number observations, \\(X_1\\) first observation \\(X_N\\) \\(N^{th}\\) observation. \\(X\\) binary variable equal 1 zero, average fraction individual's 1\n\\[\n  \\bar{X} = \\frac{X_1 + ... + X_N}{N}=\\frac{\\text{Number observations =1 }}{\\text{Total Observations}}\n\\]\nSince variable \"Passed\" equal 1 passed zero otherwise, fraction passed average \"Passed\" variable. Therefore, Excel simply need compute average G column\nFigure 1.23: Computing Fraction Passed\ntake lot averages binary variables book. beneficial road convince average Passed equal fraction passed. still confused concept, try convince datatsets observations fact true.Finally, finish summary statistics table, need fill total number observations. , can use COUNT function, count total number numbers column. Figure 1.24 uses Column G compute total observations, replaced column non-missing numeric data.\nFigure 1.24: Computing Total Observations\n","code":""},{"path":"college-instruction-experiments.html","id":"S:pivottables","chapter":"1 College Instruction Experiments","heading":"1.6 Pivot Tables","text":"far, learned retrieve summary statistics using functions. works well retrieving summary statistics entire dataset. Sometimes however want complicated summary statistics. example, empirical application, might interested passing rates different departments. possible retrieve using functions, easier way pivot table. name \"pivot\" comes \"pivoting\" data way provides useful information.understand trying accomplish pivot table, start looking end goal Figure 1.25. Pivot Table, row separate department. Next department, fraction students passed standardized test department. departments give different tests based material taught course, score 70 greater counts pass departments.\nFigure 1.25: Pivot Table: Pass Rates Department\nNow go construct Figure 1.25. insert pivot table, go Insert tab click \"Insert Pivot Table\", seen Figure 1.26\nFigure 1.26: Inserting Pivot Table\nNext, need select table range. select variables want pivot table. case, need department whether individual passed. select range, can click column (example D), hold , drag another column (example G). Since began D dragged G, variables DepartmentCode, TreatmentArm, TestScore, Passed variables can include pivot table.\nFigure 1.27: Selecting Range Pivot Table\ncan also choose like pivot table appear. default, table placed whichever cell clicked last (case cell I10). Therefore, insert pivot table, click cell like appear.can seen Figure 1.28, Excel drawn blank Pivot table. right can see new panel \"PivotTable Fields\" use build Pivot table.\nFigure 1.28: Blank Pivot Table\nwant row pivot table Department Code. click box next DepartmentCode put pivot table.\nFigure 1.29: Adding DepartmentCode Pivot Table\ndefault (see Figure 1.29), Excel placed variable Values panel, want Rows panel. can move variables windows clicking dragging. example want move DepartmentCode Rows panel.\nFigure 1.30: Adding DepartmentCode Pivot Table\nValues panel, want pass rates. compute pass rates variable \"Passed\"\nFigure 1.31: Adding Passed Pivot Table\ndefault Excel displaying \"SUM\" department (.e. number individuals passed, fraction passed). change displayed statistic click \"\" next \"Values\" label select \"Average\".\nFigure 1.32: Average Pass Rates\nNow reached goal! followed steps, next table see identical Figure 1.25. found discussion difficult follow, urge play around pivot tables. Move variables different panels see table change.","code":""},{"path":"college-instruction-experiments.html","id":"balance-tables","chapter":"1 College Instruction Experiments","heading":"1.7 Balance Tables","text":"key aspect randomized control trial allows us infer causality randomization. randomization successful, individuals selected treatment similar (average) selected control. Therefore, key component analysis experimental data show treatment control similar based observable characteristics.understand balance test might fail, imagine researchers testing whether given drug reduces cholesterol. experimenters receive list 200 participants trial decide first 100 names list receive treatment second 100 receive placebo. fail realize list applicants sorted youngest oldest. Therefore, youngest participants receive treatment. younger applicants, average, probably lower cholesterol begin , differences treatment control may due age, due treatment.perform balance test (.e. calculate average age treatment control) immediately detect error. Now, \"real\" experiment ever run way. ignoring standard protocol well-run experiments! Still, important understand whether differences treated control units proceeding main analysis. Finding persistent differences respect observables evidence randomization somehow failed. empirical application, information age student, see age varies treatment control.want retrieve average age Treatment Arm. can use pivot table! Rows Treatment Arm, Values age. can follow exact steps Section 1.6 build balance table displayed Figure 1.33\nFigure 1.33: Average Age Treatment Group\ncan seen Figure 1.33, average age treatment group (.e. Instructor Incentives group) control group roughly , 24.5 years age. keeping illustrative example simple, generally papers show balance table many observables. may occasionally differences certain characteristics, average, two groups tend similar randomization implemented successfully.","code":""},{"path":"college-instruction-experiments.html","id":"S:impact","chapter":"1 College Instruction Experiments","heading":"1.8 Impact of Financial Incentives","text":"Now checked randomization successful, estimate first result. reminder, studying whether offering financial incentives instructors (50 dollar bonus student passing test) improves performance students. Given 40 percent community college students earn college degree within 6 years (Shapiro et al. 2017), important consider tools improve performance.want compute average pass rate individuals treatment vs. individuals control. can use pivot table accomplish ! Rows TreatmentArm Values Passed. , can follow steps Section 1.6 construct table. pivot table displayed Figure 1.34.\nFigure 1.34: Impact Financial Incentives Pass Rates\nGiving instructors' incentives increases pass rates 7 percentage points. big impact! exploring result , now good time discuss accurately discuss result. One common mistake (students journalists) mix terms percentage point percent.example, 40 percent control students pass test, 47 percent treated students pass test. Therefore, treatment increased pass rates 7 percentage points. use term percentage points comparing two percents two rates. terms percentage point percent interchangeable.accurately use term percent compare 40 47 percent? calculate percent increase pass rates treatment relative control can compute:\\[\n\\frac{\\text{Pass Rate Treatment}-\\text{Pass Rate Control}}{\\text{Pass Rate Control}}= \\frac{0.471-0.400}{0.400}=0.1775\n\\]treatment pass rate 17.75 percent higher relative control. mistake can misleading! example, imagine probability dying given procedure 2 percent. However, doctor tries new surgical technique finds probability dying alternative procedure 1 percent.1 percentage point decrease probability dying procedure. However, 50 percent decrease probability dying procedure. Now, journalist wrote article incorrectly stated new procedure decreases death rates 1 percent, sounds new technique much better old technique. However, wrong conclusion. correct conclusion 1 percentage point decrease probability dying implies 50 percent reduction probability dying, huge advance!take stock learned. Providing financial incentives community college instructors seems increase pass rates average. important finding. Prior work \"pay--performance\" United States shown limited effectiveness K-12 schools (See Neal (2011)). first study show financial incentives can effective tool Community College setting. important general lesson: context treatment matters. Just treatment effective one setting mean effective another (vice-versa).Next, talk else can learn experiment. Well, study done across many different subjects. Maybe effect finacnial incentives differs depending department. may easier get students pass threshold disciplines, making incentives potentially effective.explore question can adjust pivot table bit. proceed two steps:First: Move \"TreatmentArm\" Column PanelFirst: Move \"TreatmentArm\" Column PanelSecond: Move \"DepartmentCode\" RowSecond: Move \"DepartmentCode\" RowIf successfully moved variables around, PivotTable Fields look like 1.35.\nFigure 1.35: Pivot Fields\nFigure 1.36 displays results.\nFigure 1.36: Pivot Table Showing Pass Rates Treatment Control Across Departments\ndepartments, instructor incentives incredibly effective. example, communications department (COMM), instructor incentives led dramatic increase pass rates. control group, pass rates 14 percent, 60 percent treatment group. 46 percentage point increase pass rates, equivalently, 329 percent increase pass rates treatment relative controlIn departments, however, instructor incentives seem particularly effective. example, math department (MATH), pass rates similar treatment group (41 percent) control group (42 percent). Therefore, seem incentives impact departments way.often useful explore heterogeneity way. analysis learned financial incentives necessarily \"one-size fits \" solution. departments able increase pass rates considerably, show modest even zero/negative impact. want apply results settings need think setting similar different .","code":""},{"path":"college-instruction-experiments.html","id":"S:pivotbasic","chapter":"1 College Instruction Experiments","heading":"1.9 Pivot Charts (Basic)","text":"section learn something closely related pivot tables: pivot chart. Pivot charts pivot tables contain information, often charts/figures effective presenting information. popular press articles common see nicely designed figure grab readers' attention.walk make pivot chart Excel.insert pivot chart, go Insert tab click Pivot Chart icon\nFigure 1.37: Inserting Pivot Chart\npivot chart fields similar pivot table fields. can choose different combinations create different figures\nFigure 1.38: Pivot Chart Fields\ngoal now create pivot chart presents first main result. words, chart shows average pass rates TreatmentArm. Therefore, pivot chart fields put TreatmentArm Axis Average Passed Values.\nFigure 1.39: Pivot Chart Fields\ncan see pivot table also created Figure 1.39. pivot table chart linked (.e. changes made pivot table reflected chart).\"(blank)\" horizontal axis serves purpose. can rid altering pivot table. can click arrow bottom right cell reads \"Row Labels\"\nFigure 1.40: Filtering Blank\nNext, window pop . can deselect \"(blank)\" remove Pivot Table\nFigure 1.41: Filtering Blank\nBlank now removed Pivot Table Pivot Chart. current figure technically relevant information understand impact financial incentives student performance, leaves lot desired. need reader understand information clearly, right now missing proper titles labels. something come many times throughout course. convey data analysis effectively, need ensure figures tables properly titled labeled. One \"trick\" pretend never seen table figure created. able understand information? , missing?edit title, can just click \"Total\" box top edit. Make sure always give descriptive titles proper capitalization\nFigure 1.42: Adding Title\nalso need add horizontal vertical axes titles. insert go \"Design\", click \"Insert Chart Element\", select one Axes Titles add text box can edit\nFigure 1.43: Adding Axes Titles\nadded proper axes titles, just one last thing . figure legend right, neither descriptive necessary particular figure. Legends important trying differentiate bars bar chart. figure, can already discern horizontal axes labels. Therefore, can simply click legend press backspace delete . Finally, ultimate figure shows average pass rates TreatmentArm\nFigure 1.44: Average Pass Rates Treatment Arm\n","code":""},{"path":"college-instruction-experiments.html","id":"S:pivotadvanced","chapter":"1 College Instruction Experiments","heading":"1.10 Pivot Charts (Advanced)","text":"Next going use pivot charts display heterogeneity treatment effects across different departments. pivot chart currently displays average Passed TreatmentArm. want present average Passed TreatmentArm separately every value DepartmentCode.Therefore, need place DepartmentCode somewhere Pivot Chart Fields. ? Legend(Series) seems like potentially good choice. Whatever variable put Legend(Series), Excel create plot similar current plot, separate bars every value variable Legend(Series). case, separate bar charts every value DepartmentCodeFigure 1.45 displays results drag DepartmentCode Legend(Series).\nFigure 1.45: Average Pass Rates Treatment Arm Department\ngood time check whether figure interpretable someone create . Right now, unclear different bars represent. legend shows color bar associated department.insert Legend, go \"Design\", press \"Add Chart Element\" navigate \"Legend\"\nFigure 1.46: Average Pass Rates Treatment Arm Department\nFigure 1.46 displays resulting figure. resulting figure.\nFigure 1.47: Average Pass Rates Treatment Arm Department\nelements good figure, possible improve way? think goal analysis. want reader take away figure? Ideally, reader able look different departments see treatment effect varies department. possible, takes bit work. example, \"Control\" bar ACCT department pretty far away \"Instructor Incentives\" bar ACCT department. much easier quickly understand treatment effect two bars adjacent.make bars adjacent, can switch \"DepartmentCode\" horizontal Axis, put \"TreatmentArm\" Legend(Series). words, Pivot Chart fields look like fields \nFigure 1.48: Pivot Chart Fields\ncomplete figure just need give descriptive title correct horizontal axis label, longer Treatment Arm. Figure 1.49 displays final product.\nFigure 1.49: Average Pass Rates Treatment Arm Department\nNow easy reader discern treatment effect every department. general lesson think hard information displayed. Just information , mean necessarily easily digestible.","code":""},{"path":"college-instruction-experiments.html","id":"S:Conclusion","chapter":"1 College Instruction Experiments","heading":"1.11 Conclusion","text":"module, studied whether instructor incentives effective improving student performance. Using experimental data Brownback Sadoff (2020), found instructors receive financial bonuses student passes course leads significant increases passing rates. However, effects varied dramatically department, departments showing enormous effects others negligible effects. part analysis Brownstone Sadoff (2020). concluding section, talk results work.far, found performance increases course incentivized financial incentives. cost increase performance? example, maybe instructors respond incentives increasing workload course. may improve students performance course, may come cost neglecting courses. Therefore, treatment actually increase performance overall, simply forces students spend time one class relative another.However, Brownback Sadoff (2020) collect grades courses student enrolled , just courses experiment. contrast hypothesis , authors find student outcomes treatment improve courses. suggests positive spillovers treatment. Improved performance one course leads improved performance others. students feel overwhelmed poor grades one course may \"give \" rest courses. Therefore, treatment increases performance single course scope increase performance courses.analysis shows evidence positive effects, short-term. real goal improve graduate rates. , richness data Brownback Sadoff (2020) come handy . authors find students treatment 2.8 percentage points likely transfer 4-year university control students. represents 28 percent increase probability transferring. Overall, suggests financial incentives instructors may particularly cost effective way community colleges achieve number important goalsWe focused financial incentives instructors, maybe providing bonuses students even effective. experiment, authors offered free tuition one summer course (face value $400) passed test. Interestingly, authors find intervention impact student performanceTo conclude, paper asks important question: can improve student performance community colleges? particular, financial incentives instructors effective? paper, clear answer question. Prior work focused settings found differing results. running carefully designed experiment, authors can convincingly answer question. Financial incentives increase short-run performance, also increase long-term outcomes, overall course completion transfer rates 4-year universities.","code":""},{"path":"intergenerational-mobility.html","id":"intergenerational-mobility","chapter":"2 Intergenerational Mobility","heading":"2 Intergenerational Mobility","text":"","code":""},{"path":"intergenerational-mobility.html","id":"opportunity-insights","chapter":"2 Intergenerational Mobility","heading":"2.1 Opportunity Insights","text":"section, discussing relationship intergenerational mobility higher education system United Stata. start exploring relationship, first need understand concept intergenerational mobility. concept asks simple question: born low-income parents, chance move income distribution?Generally, think societies high rates intergenerational mobility equal. society high rates intergenerational mobility, means individuals low-income backgrounds opportunities move income distribution. One common path upward mobility higher education system.However, reasons higher education system may may lead higher rates intergenerational mobility. First, colleges may particularly effective increasing incomes. example, -profit colleges often criticized costly low quality (see NYT Article). college accepts large fraction students low-income backgrounds, may still promote intergenerational mobility students actually benefit attending. Second, individuals lower-income backgrounds may lower access higher education system. Rising cost college may making worse time.go studying question? going use dataset made publicly available researchers Opportunity Insights. data made big splash, academia policy circles. Figure 2.1 shows headline Upshot article dataset. article, authors discuss dataset led us new insights access varies dramatically schools. headline reads, \"Colleges Students Top 1 Percent Bottom 60. Find \". school students top 1 percent bottom 60, probably promote intergenerational mobility. students already relatively well-backgrounds. first hint colleges may vary dramatically extent promote intergenerational mobility.\nFigure 2.1: New York Times Graphic Opportunity Insights\nget little bit details data using empirical application. original data comes federal income tax returns linked tax records universities data Pell grants. result dataset (1) children linked parents (children reported dependents) (2) income data children parents (3) children linked universities attended.give us everything need know understand given college promotes intergenerational mobility. allow us know know background students attending given college well eventual earnings. important emphasize huge data task gives us new way study important question.discussed high level concept intergenerational mobility. turn data, need compute metric intergenerational mobility allows us compare across colleges. introducing measure, introduce statistical concept important throughout course, particular, understanding measure intergenerational mobility: quantiles.","code":""},{"path":"intergenerational-mobility.html","id":"quantiles","chapter":"2 Intergenerational Mobility","heading":"2.2 Quantiles","text":"begin discussion quantiles, imagine small dataset 10 individuals. individual reports income (thousands) sort ten individuals based income.\\[          \n\\{5,6,20,29,38,42,55,62,80,88\\} \\nonumber\n\\]can break data equal-sized groups based income. equal sized group quantile. example, imagine break 10 individuals 5 groups (5 quantiles).\\[\n            \\{\\underbrace{5,6}_{Q1},\\underbrace{20,29}_{Q2},\\underbrace{38,42}_{Q3},\\underbrace{55,62}_{Q4},\\underbrace{80,88}_{Q5}\\} \\nonumber\n\\]break observations 5 equal-sized groups, referred breaking data quintiles. Q1 referred first quintile (lowest quintile). Q5 referred fifth quintile (highest quintile). Quintiles important us use define intergenerational mobility. particular, conceptualized intergenerational mobility movement low-income background high income. low income high income exact definition. researchers Opportunity insights use quintiles define terms.particular, individual defined coming low-income parents individual's parents bottom quintile (Q1) income distribution. bottom quintile implies parents bottom 20 percent income distribution. individual defined high income individual top quintile (Q5) income distribution, equivalently, top 20 percent income distribution.Quintiles just one way break data. break data four equal-sized groups, broken data quartiles. break data hundred equal-sized groups, broken data percentiles. general, break data \\(q\\) groups, created \\(q\\) quantilesCertain percentiles often reported descriptive statistics. example, median (50\\(^{th}\\) percentile) often reported summary statistic. 2019, according U.S. Census, median household income $68,703. implies 50 percent households income less $68,703, 50 percent income $68,703.Question: individual 33 percentile earnings, individual ...","code":""},{"path":"intergenerational-mobility.html","id":"mobility-definition","chapter":"2 Intergenerational Mobility","heading":"2.3 Mobility Definition","text":"Now understand quantiles, define college's intergenerational mobility rate. Intuitively, college promote intergenerational mobility (1) admits students low-income backgrounds (2) students become high earners later life. , defined low income bottom quintile earnings, high income top quintile earnings. define mobility rate fraction low-income students, multiplied probability low-income students go become high earners:\\[\n\\text{mobility rate}= \\underbrace{\\text{(frac. students Q1)}}_{\\text{Acess}} \\; \\cdot \\; \\underbrace{\\text{(frac. Q1 reach Q5)}}_{\\text{Success}} \\nonumber\n\\]talk terms independently. \"Access\" depends fraction students enrolled school come low-income backgrounds, Q1. potential schools high access large impacts intergenerational mobility. might necessarily true. example, work found profit colleges tend admit disadvantaged individuals, outcomes individuals tend poor (see Deming, Goldin Katz 2013).1 Therefore, even though Access rate might high, schools tend promote intergenerational mobility. reason due second term: Success. measures fraction students low-income backgrounds reach top quintile earnings.Intuitively, can interpret mobility rate fraction students low-income backgrounds reach high income school. example, 10 percent school's enrollment comes low-income background, half students end top quintile earnings, means 5 percent (overall) student body came low-income backgrounds achieved high income. number equal school's mobility rate..now get specifics data. (exactly) determine students comes low-income background? steps:Take parents child year (e.g. 1985)\nTake parents child year (e.g. 1985)Compute average income parents children 15-19 (.e. 2000-2004)\nCompute average income parents children 15-19 (.e. 2000-2004)parents' income bottom 20 percent period (.e. Q1), classify students coming low-income background\nparents' income bottom 20 percent period (.e. Q1), classify students coming low-income backgroundNow low-income background, (exactly) determine student becomes high earner?Take individuals age calculate earnings 2014\nTake individuals age calculate earnings 2014If individual's income top 20 percent relative individuals age (.e. Q5), classify student high earner\nindividual's income top 20 percent relative individuals age (.e. Q5), classify student high earnerA general lesson understand details proceed analysis. example, low income high income specific definitions. need understand variables measured can dive analysis.Question: Imagine 10 percent College 's students come low-income backgrounds, 20 percent individuals go become high earners. contrast, 5 percent College B's students com low-income backgrounds 50 percent individuals go become high earners. college higher mobility rate?","code":""},{"path":"intergenerational-mobility.html","id":"stata-gui","chapter":"2 Intergenerational Mobility","heading":"2.4 Stata GUI","text":"digging Opportunity Insights dataset, first need learn use Stata. Every time launch Stata, greeted Graphical User Interface (GUI) see Figure 2.2\nFigure 2.2: Stata Graphical User Interface\n4 main panels . start bottom left: Command window. can type code. press enter, code executed. learn . learn use Stata calculator, type codeWhen execute code, resulting output displayed Results window. learn use Stata calculator, results displayed.load data, name variables dataset appear first column. Many times, datasets come labels give us description variable. example, figure one variable named income label Total Annual Income.\nFigure 2.3: Variables Window Example\nProperties window gives us information variables data loaded memory. example, addition name label variable, type (.e. numeric string) format variable (.e. variable displayed). Additionally information size data (number variables, observations, etc.)Now learned various windows, going execute first line code. particular, going use Stata calculater typing display followed mathematical expression.example, compute 2+2 typeThe result visible Results window. display called command Stata. Stata comes many commands useful explore analyze data. Whenever learn new command Stata helpful look help file comes along file. access help file display command type help display command window. open file instructions use given commandThe first thing generally see help file syntax command. Syntax refers format code. similar \"grammar\" software language. use incorrect grammar language, may expressing right meaning. Similarly, use wrong syntax software language, may get correct output, likely, error reportedFigure 2.4 shows Syntax help file display command.\nFigure 2.4: Variables Window Example\nfact di underlined means use display command need type di shortcut. Many commands Stata shortcuts associated . figure shortcuts, just need open help file. fill [display_directive] couple waysWe can use Stata perform calculations just like calculator. example, subtraction, type:multiplication type:division type:can also use display print text. can helpful writing programs. example, something goes wrong program, , might want display error code. display words Stata, just need put words like display quotation marks. example, display \"Hello World\" type:Words referred strings Stata (later string variables numeric variables). reference string, need put quotation marks. example, type display Hello World Stata, get error depicted Figure 2.5.\nFigure 2.5: Forgetting Quotation Marks Referencing Strings\nHello found Stata looked command variable named Hello, find one, therefore reported error. command realize just meant display text, text quotation marks.","code":"display 2+2\n4di 6-4\n2di 2*4\n8di 12/2\n6display \"Hello World\"\nHello World"},{"path":"intergenerational-mobility.html","id":"do-files","chapter":"2 Intergenerational Mobility","heading":"2.5 Do-files","text":"far, discussed execute code typing command window. practice, seldom use command window execute code. Instead, use -file.understand importance -file, imagine project large number steps. Maybe need clean data (usually necessity) run large number results. single project end thousands lines code. entail complete project use command line?need type lines code one time. make mistake beginning project. example, accidently deleted observations relevant analysis. disaster! now need remember , type code , execute .better way approach project use -file. -file allows run multiple lines code sequentially. can save progress periodically, just save progress writing paper text editing program like Microsoft Word. , realize made mistake early project, can just alter one line code re-run entire project.begin, need learn open -file. top GUI icon looks like piece paper pencil (see Figure 2.6).\nFigure 2.6: Opening New -File Stata\nFigure 2.7 shows larger version -file icon.\nFigure 2.7: -File Icon\nopen new -file can simply click icon. , see blank file. going type code (see Figure 2.8).\nFigure 2.8: Blank -File\nexample, want Stata compute sum 2+2, type display 2+2 dofile, seen Figure 2.9.\nFigure 2.9: Display 2+2 -File\nnow code written, execute ? actually couple different ways. first pressing \"\" button, icon top right -file looks like piece paper arrow.press button without highlighting text, execute every single line code file sequence appear. Depending goal, may want run select portions code. example, often need \"debug\" code, efficient run lines time. case, can highlight lines like executed, press \"\", highlighted lines code executed.example, Figure 2.10, hitting \"\" execute first two lines code.\nFigure 2.10: Executing Lines 1 2\nFigure 2.11, hitting \"\" execute second third line code.\nFigure 2.11: Executing Lines 2 3\nactually hit \"\" execute entries dofile. Stata built-shortcuts can use. shortcut set keys can press order perform operation pressing button. example, Mac, press \"Cmd + Shift + D\" highlighting sections code, highlighted sections executed. PC, shortcut \"Ctrl + D\".Lastly, key aspect -file can save progress. example, homework assignment multiple sittings, better way start left ! save -file press \"Save\" button top left corner page, seen Figure 2.12\nFigure 2.12: Saving -File\nsave -file somewhere convenient computer, can quickly find next time need .","code":""},{"path":"intergenerational-mobility.html","id":"working-directory","chapter":"2 Intergenerational Mobility","heading":"2.6 Working Directory","text":"Next going discuss working directory. understand working directory, first need understand every file folder computer \"address\" associated . us used navigating either Finder Mac File Explorer PC open file need.way open files. fact, originally, convenient user interface computers access files way. Instead, needed direct computer location file typing file's \"address\". \"address\" file folder know path name. string text tells computer file folder located.working directory default path Stata. Stata looks files default. example, imagine dataset named, \"interesting_data.dta\" want load Stata. command load data Stata use command. Therefore, might want type:, clear tells Stata clear data currently loaded memory. Stata look file \"interesting_data.dta\"? working directory!Therefore, start loading data, need learn working directory currently set . check Stata, use pwd command, short \"print working directory\". check working directory Stata, just type pwd command window. typed command window, path name came :/Users/davidarnold/Documents/StataThis convenient folder working directory. want working directory set place put data downloading . , folder \"/Users/davidarnold/Dropbox/Teaching/EP5/online/02_week/data\".course, suggest creating folder dedicated course. , Folder Dropbox located within \"Teaching\" folder. named folder course \"EP5\", can name whatever like.Additionally, given large number datasets using, convenient separate somehow. divided course weeks. Given application corresponds second week, named folder \"02_week\". Lastly, inside folder want quickly understand data , therefore, dedicated data folder holds datasets explore week.structure described displayed Figure 2.13\nFigure 2.13: Example Structure Folder Hierarchy Course\nOne brief aside continue understanding working directory. best time organize folders computer now! mean? means place everything download one place. desktop cluttered numerous files. Create folders computer neatly divide work. means different folders Research Teaching. Within Teaching, need different folders every course teach. Within course, need different folders every week course. Therefore, need access dataset set notes given week course, can quickly easily. organization now save lot time future, just course, even generally.Returning working directory, now know data , need change directory location data. can use cd command short \"change directory.\"path , different depending put downloaded data section. One issue commonly confuses students even though can navigate file, actually know full path file.PC, retrieving full path relatively simple. file explorer panel top shows different subfolders. Clicking arrow next path convert full path name. can simply copy paste pathname Stata change working directory.Mac, slightly complicated. right click folder press \"option\" key, option \"copy pathname\". Pressing option copy path name (.e. can press Cmd+p paste , right click select paste.)","code":"use interesting_data.dta, clearcd \"/Users/davidarnold/Dropbox/Teaching/EP5/online/02_week/data\"\n/Users/davidarnold/Dropbox/Teaching/EP5/online/02_week/data"},{"path":"intergenerational-mobility.html","id":"describe-and-browse","chapter":"2 Intergenerational Mobility","heading":"2.7 Describe and Browse","text":"section going learn basics loading data Stata. get started, load main data chapter, contains data Opportunity Insights allow us colleges vary extent promote intergenerational mobility. load data type:, first part code (part following \"cd\") look different depending path data computer. Now data loaded memory, yet understand actually data. first command can used start exploring data describe command. describe command allows us quickly review variables dataset.Figure 2.14 displays results command, table variables, well format variable label associated variable.\nFigure 2.14: Describe Command\ncan note important variables using dataset. First, variable name indicates institution. dataset 2,199 institutions United States. variables focus par_q1 kq5_cond_parq1. label associated par_q1 \"Fraction parents quintile 1 (bottom quintile)\". terms definition mobility rate, access portion. school large fraction students bottom 20 percent income distribution (quintile 1) high access.component mobility rate success rate, captured variable kq5_cond_parq1. label associated variable \"Probability kid quintile 5 conditional parent quintile 1\". words, fraction students low-income background go earn top 20 percent earners age group.mobility rate product access rate success rate. can see, variable dataset, eventually need create .describe command gives us good sense variables data, also useful look directly data. browse command opens spreadsheet similar Excel spreadsheet. allows look data understand values variable stored. browse data, simply type command window:dataset, open data table displayed Figure 2.15\nFigure 2.15: Browse Command\nopen new dataset often helpful us browse data. example, now know exactly state variable stored (abbreviations rather full names). can also see clearly data table variables stored strings variables stored numbers.String variables highlighted red. Numeric variables stored black. Sometimes variable may seem like numeric variable, contain non-numeric strings. example, value \"$24,000\" stored string \"$\" \",\" non-numeric characters.","code":"cd \"/Users/davidarnold/Dropbox/Teaching/EP5/online/02_week/data\"\nuse college_mobility.dta, clear\n/Users/davidarnold/Dropbox/Teaching/EP5/online/02_week/data\n\n(Preferred Estimates of Access and Mobility Rates by College)describebrowse"},{"path":"intergenerational-mobility.html","id":"summarize","chapter":"2 Intergenerational Mobility","heading":"2.8 Summarize","text":"Now sense variables, generate descriptive statistics understand . descriptive statistic statistic quantitatively describes variable (common ones mean, median, standard deviation). Stata, summarize command useful retrieving summary statisticsThe basic syntax summarize command issummarize varlistwhere varlist list variables like see summary statistics . example, imagine first want understand, average, large institutions dataset? Well, variable count average number students enrolled institution. retrieve summary statistics variable type:table tells us things. First, tells us average size cohort students across institutions dataset 946 students. Additionally, can see minimum 50 maximum 26989.67. minimum 50 actually construction. Institutions less 50 students per cohort dropped dataset. Understanding descriptive statistics key variables important starting data analysis.","code":"summarize countsummarize count\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       count |      2,199    946.5153    1508.825         50   26989.67"},{"path":"intergenerational-mobility.html","id":"generate","chapter":"2 Intergenerational Mobility","heading":"2.9 Generate","text":"Now understand explore data, get back question interest: colleges vary extent promote intergenerational mobility? just one problem. mobility rates data.remind , mobility rate college given :\\[\n\\text{mobility rate}= \\underbrace{\\text{(frac. students Q1)}}_{\\text{Acess}} \\; \\cdot \\; \\underbrace{\\text{(frac. Q1 reach Q5)}}_{\\text{Success}} \\nonumber\n\\]dataset, variables capture access term (par_q1) \nsuccess term (kq5_cond_parq1). can construct college's mobility rate. use generate commandThe basic syntax generate command isIn command, newvar name new variable like create. can give variable name like, generally good practice keep names short, descriptive possible. term exp expression. fill exp depends variable creatingBefore get generating mobility rates, take common examples. Suppose two variables x1 x2 want create new variables functions two variables. examples might want accomplishAddition: gen sum = x1+x2Addition: gen sum = x1+x2Subtraction: gen diff = x1-x2Subtraction: gen diff = x1-x2Multiplication: gen mult = x1*x2Multiplication: gen mult = x1*x2Division: gen div = x1/x2Division: gen div = x1/x2For us, want create variable product par_q1 kq5_cond_parq1. Therefore, can type:create new variable, default label associated . working project many different variables, good idea label variables can remember represent later date. example, want give variable mobility_rate label type:","code":"gen newvar = expgen mobility_rate = par_q1*kq5_cond_parq1label var mobility_rate \"Mobility rate of institution\""},{"path":"intergenerational-mobility.html","id":"binary-variables","chapter":"2 Intergenerational Mobility","heading":"2.10 Binary Variables","text":"binary variables create class result evaluating logical statement. review, logical statement statement either true false. example, statement \"College X California\" either true false. \"UCSD\" statement true, \"ASU\" false.often useful create binary variables (binary meaning two) data based logical statements. example, data intergenerational mobility, might interested comparing mobility rates across different regions. example, maybe want compare mobility rates California mobility rates parts country. , might want create binary indicator variable equal 1 state California zero otherwise. can create binary variable equal 1 logical statement true zero false typeThe logical statement (state==\"CA\") true state California false . Note double-equals sign ==. use double-equals test equality two things. Additionally, note quotation marks around CA. Reminder: state string variable always reference string variables quotation marks.statement mirrors general syntax can create binary variables Stata. general syntax isThis code generate new variable (named newvar) equal 1 logical statement parentheses true zero false.check whether results conform expectations can type br state CA. Figure 2.16 show small selection data shows us CA equal 1 colleges \"CA\" zero otherwise.\nFigure 2.16: Generating Binary Indicator California Colleges\nerrors arise enough valuable go now. First, common forgot \"double equals\" sign, instead put one equals sign, .execute code, get error Figure 2.17.\nFigure 2.17: Forgetting Double Equals Testing Equality\nerror report state=\"CA\" displayed command looking variable name, valid format. test equality, need two equals signs.Another common error (just generating new variables) forget quotation marks referencing string variables. example type:get error code displayed Figure 2.18.\nFigure 2.18: Forgetting Quotation Marks Referencing Strings\nerror report type mismatch occurs state string variable, yet comes equals sign interpreted string. tell Stata characters CA strings, put CA quotation marks!many binary variables can create relations. example, imagine two generic variables, x1 x2. can create variable equal one x1 greater x2 zero otherwise.variable equal 1 x1 less x2 zero otherwise.variable equal 1 x1 greater equal x2 zero otherwise.variable equal 1 x1 less equal x2 zero otherwise.","code":"gen CA = (state==\"CA\")gen newvar = (logical statement)gen CA2 = (state=\"CA\")gen CA3 = (state==CA)gen x1_greater_x2 = (x1>x2)gen x1_less_x2 = (x1<x2)gen x1_greater_x2 = (x1>=x2)gen x1_less_eq_x2 = (x1<=x2)"},{"path":"intergenerational-mobility.html","id":"if-statements","chapter":"2 Intergenerational Mobility","heading":"2.11 If Statements","text":"statements Stata used want execute code, condition met. commands use Stata can combined statement.general syntax using statements isThe command executed observations logical statement true. look help files commands, often notice [] syntax. means can combine command statement. example, Figure 2.19 can see can use statements summarize command.\nFigure 2.19: Help File Summarize Command\ngeneral note syntax presented Stata useful . Words appear brackets can used using command, need . word brackets need include order use summarize command. However, imagine want compute average mobility rates, just restricted colleges California. college California CA==1, binary variable created . Therefore can compute average mobility rate California colleges typing:Note put brackets, even though brackets syntax. brackets tell can use statement, brackets actually part syntax.mobility rates California colleges 0.028. compare number mobility rates non-Californian colleges:mobility rates non-Californian colleges 0.018, mobility rates, average, higher Californian schools. ? Well, either 2 factors: higher access higher success. can check one vs. driving result summarizing access success separately.now non-Californian colleges:first discuss access results (.e. variable par_q1). Across colleges CA, average fraction students low-income backgrounds around 14.4 percent. contrast, non-California colleges, average fractions students low-income backgrounds around 12.3 percent. Therefore, average across schools, access higher CA.\nNow discuss success results (.e. variable kq5_cond_parq1). Across colleges CA, average fraction students low-income backgrounds become high earners around 24.8 percent. Across colleges CA, average fraction students low-income backgrounds become high earners around 19.2 percent. Therefore mobility rates higher CA due (1) greater access (2) greater success.can also use statements reference string variables. example, imagine want see mobility rate UCSD. data, can use variable name contains institution name summarize mobility name == \"University California, San Diego\". Note use quotation marks reference string value.taken average name == \"University California, San Diego\". one observation meets restriction, table just showing us mobility rate UCSD, equal 0.048, bit higher average 0.028 across Californian institutions.section begun explore variation across colleges terms intergenerational mobility rates. next section learn data visualization technique useful understanding variation data generally.","code":"command if (logical statement)sum mobility_rate if CA==1     Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nmobility_r~e |        168    .0275095    .0154819          0   .0991846sum mobility_rate if CA==0     Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nmobility_r~e |      2,031    .0175132     .012647          0   .1635797sum par_q1 kq5_cond_parq1 if CA==1     Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      par_q1 |        168    .1443805    .0889914   .0321324   .4606968\nkq5_cond_p~1 |        168    .2481358    .1638556          0   .8497473sum par_q1 kq5_cond_parq1 if CA==0     Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      par_q1 |      2,031    .1232655    .0878519   .0111896   .6097748\nkq5_cond_p~1 |      2,031    .1919684    .1360417          0   .9192932sum mobility_rate if name == \"University Of California, San Diego\"    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nmobility_r~e |          1    .0483275           .   .0483275   .0483275"},{"path":"intergenerational-mobility.html","id":"histogram-theoretical","chapter":"2 Intergenerational Mobility","heading":"2.12 Histogram (Theoretical)","text":"histogram representation distribution numeric variable . empirical application, want understand distribution mobility rates across colleges. keep things simple, go hypothetical example setting probably familiar : understanding distribution test scores class.imagine class 10 students, depicted :\\[\n  \\{65,71,75,76,80,84,85,86,90,98\\}\n\\]want understand distribution test scores. create histogram, first thing must decide \"bin\" data (sometimes referred \"bucket\"). Binning data means divide range series intervals. example, test scores, use following intervals:Interval 1: [60,70)Interval 2: [70,80)Interval 3: [80,90)Interval 4: [90,100]Reminder: brackets mean inclusive, parentheses mean excluding. score 70 fall interval 2, 1. Next compute fraction observations within bin. 1 test score (65) falls within interval 1, 10 percent observations interval 1. Three test scores fall within interval 2, 30 percent observations interval 2. can compute fraction interval plot result histogram, depicted Figure 2.20.\nFigure 2.20: Histogram Test Scores\nhistogram, height bar represents fraction test scores fall within given interval. Now understand basic premise, go example real datasets.example 1 going look distribution Rotten Tomatoes scores movies released 2015 Figure 2.21. rotten tomato score represents fraction critics rated film positive. value 50 indicates 50 percent critics rated movie positively. score ranges minimum 0 maximum 100.\nFigure 2.21: Histogram Rotten Tomatoes Scores Movies Released 2015\nlearn figure? movies get low scores, 5-10 range. can tell ? first bin starts around 5 ends around 15. height bar little 0.10. means bit 10 percent movies got score 5-15 range. move across range possible values, actually find height bars relatively stable. Therefore, can describe distribution relatively uniform. means across range movie scores, seem like scores especially likely relative others (perfectly uniform distribution height bars identical across entire range x-axis.)next example, going explore distribution olympic athlete ages Figure 2.22.\nFigure 2.22: Histogram Olympic Athlete Ages\nlearn histogram? Well, Olympic athletes ages 19 32. values bars consistently high, indicating large fraction sample. lower end, athletes 19, relatively less common. upper end, athletes even 40 years old, clearly outliers. far away data, 20s. sometimes describe feature distribution \"long-right tail\". say distribution skewed right.Finally, look distribution calories meals ordered Chipotle Figure 2.23, comes article Upshot.2\nFigure 2.23: Histogram Calories Chipotle Meals\ncan see, Chipotle meals 1,000 calorie range. many burritos burrito bowls end around 1000 calories. However, order chips well, adds calories meal, can see bump histogram around 1600 calories. upper end distribution see meals 2,000 calories, total amount calories recommended day! Overall, 2 percent meals calorie counts extreme.","code":""},{"path":"intergenerational-mobility.html","id":"histogram-data","chapter":"2 Intergenerational Mobility","heading":"2.13 Histogram (Data)","text":"empirical application, want understand distribution mobility rates across colleges. common values mobility rate? outliers? study , going create histogram Stata.begin, need learn histogram command, used plot histograms Stata. simplest possible syntax simply typeWhere varname name whatever variable like create histogram . example, want plot histogram mobility_rate, type:\nFigure 2.24: Histogram College Mobility Rates\nnotice vertical axis Figure 2.24 density. prior examples, always vertical axis labeled \"Fraction Observations.\" necessary understand class, density integrates 1, vertical axis sometimes rescaled way. purposes, intuitive display vertical axis fraction observations. change appearence histogram way, just displayed vertical axis.plot fraction observations instead density, can use option histogram command.Stata, additional options available commands. give way alter command way. can read list options given command opening help file. options looking frac options, changes vertical axis fraction observations instead density:\nFigure 2.25: Histogram College Mobility Rates (Changing Vertical Axis)\nNow histogram, start interpreting . First, \"mass\" histogram (.e. bars highest)? colleges fall terms mobility rates. Well, looks like spike near 0.02. example, highest bar range reaches 0.25, indicating 25 percent colleges fall interval. value 0.02 represent? college mobility rate 0.02 2 percent students coming low-income backgrounds achieving high income.Two percent seems like low number, colleges even lower rates mobility. example, first interval starts roughly zero. height bar around 0.04, indicating 4 percent schools mobility rates near zero. , due either (1) accepting students low-income backgrounds (access) (2) graduates earn enough categorized high income (success).end distribution, can see outlier schools high mobility rates relative others. Although difficult see graph, schools mobility rates 0.10, indicating 10 percent students school come low-income backgrounds achieve high income later life.Overall example distribution long-right tail. schools cluster 0.01 0.03 range, outliers upper end mobility distribution.Sometimes helpful tweak graphs. cases, may like default settings, generally, able change . example, maybe like many bins created. can directly control number bins specifying bin() option. example, Figure 2.26 create histogram 20 bins.\nFigure 2.26: Histogram College Mobility Rates (Changing Number Bins)\n20 bins original histogram, width bin larger now (bin heights upper end distribution small difficult see figure).Instead directly controlling many bins, can also control width bins. test score example, chose bins width 10, chosen bins width 5 instead. choice 10 arbitrary. cases, may want change width bins. change width bin can specify width() option, .\nFigure 2.27: Histogram College Mobility Rates (Changing Width Bins)\nFigure 2.27, interval bin 0.01. Given range data, creates 20 equal-sized bins, figure looks similar figure created specified 20 bins.","code":"histogram varnamehistogram mobility_ratehistogram mobility_rate, frachistogram mobility_rate, frac bin(20)histogram mobility_rate, frac width(0.01)"},{"path":"intergenerational-mobility.html","id":"histogram-aesthetics","chapter":"2 Intergenerational Mobility","heading":"2.14 Histogram (Aesthetics)","text":"Although created histogram, still leaves lot desired. figures effective, need clearly labeled titled. make visually appealing, also helpful change colors add additional elements.change color bars graph, can specify color() option. find effective histograms also make bars slightly translucent. allows plotting multiple histograms figure, still able distinguish different variables. code , going change color bars blue, also make bars transparent. specifying color(blue%40) one command options. %40 control translucent bars appear. can specify number zero 100 .\nFigure 2.28: Histogram College Mobility Rates (Changing Color Bars)\nNext, add title figure. always important include title describes figure presenting. can specifying title() option:\nFigure 2.29: Histogram College Mobility Rates (Adding Title)\nnotice three /// end first line code. Sometimes easier read code separate various elements different lines. However, Stata, press enter continue code next line, Stata interpret two lines connected. words, include /// interpret second part title(\"Histogram Mobility Rates Across U.S. Colleges\") new block code. title command Stata, option graphical commands. Therefore, needs follow type graphing command. adding /// telling Stata two lines code actually single block code executed single unit.Next, work axis labels. horizontal axis currently labeled mobility_rate. professional particularly descriptive variable names titles axes. change \"Intergenerational Mobility Rates\" specifying xtitle() option. can also make vertical axis title descriptive.\nFigure 2.30: Histogram College Mobility Rates (Changing Axis Labels)\nnext step strictly necessary, find graphs aesthetically pleasing also change background color. can specifying graphregion(fcolor(white))\nFigure 2.31: Histogram College Mobility Rates (Changing Background)\nSometimes helpful highlight important elements graph. add vertical line mobility rate UCSD, 0.048. add xline() option figure. particular, place black line 0.048 x-axis dashed specifying xline(0.048, lc(black) lp(dash)). option lc(black) specifies line color (lc) black lp(dash) indicates line pattern (lp) dashed., can also add text graph label dashed line. add text, use text() option. order use text option need specify y-coordinate, x-coordinate, text appear graph. example, want add label UCSD, line can add text(0.2 0.055 \"UCSD\"). place text UCSD value (y=0.2,x=0.055) graph.Figure 2.32 displays plot adds vertical line labels UCSD's mobility rate.\nFigure 2.32: Histogram College Mobility Rates (Adding Vertical Line Text)\n","code":"histogram mobility_rate, frac color(blue%40)histogram mobility_rate, frac color(blue%40) ///\n  title(\"Histogram of Mobility Rates Across U.S. Colleges\")histogram mobility_rate, frac color(blue%40) ///\n  title(\"Histogram of Mobility Rates Across U.S. Colleges\") ///\n  xtitle(\"Intergenerational Mobility Rates\") ///\n  ytitle(\"Fraction of Observations\") histogram mobility_rate, frac color(blue%40) ///\n  title(\"Histogram of Mobility Rates Across U.S. Colleges\") ///\n  xtitle(\"Intergenerational Mobility Rates\") ///\n  ytitle(\"Fraction of Observations\") ///\n  graphregion(fcolor(white)) histogram mobility_rate, frac color(blue%40) ///\n  title(\"Histogram of Mobility Rates Across U.S. Colleges\") ///\n  xtitle(\"Intergenerational Mobility Rates\") ///\n  ytitle(\"Fraction of Observations\") ///\n  xline(0.048, lcolor(black) lp(dash)) ///\n  text(0.2 0.055 \"UCSD\") ///\n  graphregion(fcolor(white)) "},{"path":"intergenerational-mobility.html","id":"conclusion","chapter":"2 Intergenerational Mobility","heading":"2.15 Conclusion","text":"module, studied college's intergenerational mobility rate. found wide ranges mobility rates across U.S. colleges. colleges rates close zero, can due either (1) low access low-income students (2) low success rates students. Next, discuss else can learn data.Figure 2.33 plot access (fraction low-income backgrounds) vs. success (percent low-income students achieving high income).\nFigure 2.33: Access vs. Success Across Colleges\nFigure 2.33, rates college mobility increase vertical horizontal axes. words, higher mobility rates observed colleges high access success. can see figure, UCSD highlighted. UCSD relatively high success rate. many colleges higher levels success. However, access rate particularly high. can see many many markers right UCSD. means many colleges higher access UCSD. reason UCSD's mobility rate relatively high primarily due high success rate graduates.Although data used snapshot point time, Opportunity Insights also data colleges changing time. Figure 2.34 plots fraction students coming different quantiles income distribution time.\nFigure 2.34: Trends Access UCSD\ncan seen figure, fraction coming Q5 (top 20 percent earners) much greater income quantiles. starts 0.5, indicating half students parents top 20 percent income distribution. However, number trending time. 2013, fraction coming top quintile decreased around 0.4 40 percent.can also plot similar figure success rates quintile parental income. Figure 2.35 plots success rate separately parental background students.\nFigure 2.35: Trends Success UCSD Quintile\ncan seen figure, different quantiles relatively similar success rates. Researchers Opportunity Insights study across many different colleges, find, average, success rates similar across quintiles parental background.works \"mismatch hypothesis\" sometimes discussed policy circles. hypothesis increasing access elite universities lower-income students may actually benefit low-income students unprepared curriculum. words, maybe better go low-ranked school chance fail higher-ranked school. Opportunity Insights find evidence , average. find similar success rates low-income high-income students.note Figure 2.35 success seems trending downward time. might slightly misleading. Income data measured 2014. UCSD students recent cohorts likely attend form graduate school, earnings 2014 might reflect lifetime earnings. explain dip later cohorts.UCSD relatively high mobility rates, far highest. Next, now look colleges highest mobility rates. Figure 2.36 shows list schools highest mobility rates.\nFigure 2.36: Schools Highest Mobility Rates\nmany top schools common? mid-tier state schools. schools appear effective promoting intergenerational mobility rates. key tend high access rates, well reasonably high success rates. trends access look time?\nFigure 2.37: Trends Schools Highest Mobility Rates\nGlendale Cal State LA two schools high mobility rates. schools, access rate actually decreasing time. 2000 35 percent students coming low-income backgrounds. 2010, declined little 20 percent.end module, discuss key takeaways researchers Opportunity Insights. First, access varies widely parental income, success . Despite numerous policies aimed increasing low-income enrollment elite universities (e.g. Harvard), see increases low-income enrollment time universitiesAt time, many schools high access rates trending towards lower access recent years. Opportunity Insights suggests changing admissions criteria increasing transfers community colleges effective policies promoting intergenerational mobility (among potential policy solutions).","code":""},{"path":"intergenerational-mobility.html","id":"command-descriptions","chapter":"2 Intergenerational Mobility","heading":"Command Descriptions","text":"Setup Commandscd filepath  changes working directory Stata replace\nfilepath file path choosing. spaces file path, need put quotation marks.cd filepath  changes working directory Stata replace\nfilepath file path choosing. spaces file path, need put quotation marks.clear -- clears data Stataclear -- clears data StataData Exploration Commandsdescribe - describes dataset gives information variables, types variables, variable labels number observations.describe - describes dataset gives information variables, types variables, variable labels number observations.browse - opens data can look values (like excel spreadsheet).browse - opens data can look values (like excel spreadsheet).summarize varname - retrieves summary statistics mean, min max variable named varname. summarize variables just type summarize. Add option , detail retrieve statistics, percentiles.summarize varname - retrieves summary statistics mean, min max variable named varname. summarize variables just type summarize. Add option , detail retrieve statistics, percentiles.tab varname -- tab retrieves table frequencies. can used string variables well categorical variables. example, categorical named region, command tab region display number observations region.tab varname -- tab retrieves table frequencies. can used string variables well categorical variables. example, categorical named region, command tab region display number observations region.trying decide tab summarize, use summarize variable takes many different values. Use tab variable takes values.Data Manipulation/Data Generation Commandsgen newvar = exp - gen used generate new variables. newvar name new variable, exp denotes expression. example, want new variable named sum addition x1 x2 can type gen sum = x1 + x2.gen newvar = exp - gen used generate new variables. newvar name new variable, exp denotes expression. example, want new variable named sum addition x1 x2 can type gen sum = x1 + x2.replace var = exp - replaces values variable var value exp. example, numeric variable -99 refers missing values (sometimes case surveys). can type replace var = . var==-99 replace variable missing variable currently equal -99.replace var = exp - replaces values variable var value exp. example, numeric variable -99 refers missing values (sometimes case surveys). can type replace var = . var==-99 replace variable missing variable currently equal -99.Graphing Commandshistogram var, frac -- computes histogram (approximation distribution continuous variable). many options change format graph. version code , frac specified vertical axis depicts fraction observations falls within bin.","code":""},{"path":"intergenerational-mobility.html","id":"references","chapter":"2 Intergenerational Mobility","heading":"References","text":"","code":""},{"path":"discrimination-in-police-stops.html","id":"discrimination-in-police-stops","chapter":"3 Discrimination in Police Stops","heading":"3 Discrimination in Police Stops","text":"","code":""},{"path":"discrimination-in-police-stops.html","id":"sopp-intro","chapter":"3 Discrimination in Police Stops","heading":"3.1 SOPP Intro","text":"common way civilians interact police traffic stops (50,000 day). Given frequency encounters, detecting eliminating racial discrimination police behavior important policy goal. However, can difficult detect without data. chapter, work data Stanford Open Policing Project (SOPP). SOPP aggregating data police stops across states cities researchers, journalists policymakers can improve interactions police public. data far information 200 million stops.big data. Figure 3.1 shows subset cities data collected .\nFigure 3.1: Stanford Open Policing Project Data\ndata research starting get notice many news outlets, including CNN, Economist, Huffington Post, NBC News Daily Show (see Figure 3.2)\nFigure 3.2: Trevor Noah Discussing Results Stanford Open Policing Project Data\nmany different \"decisions\" officers make driven discrimination:decision stop vehicle \ndecision stop vehicle notConditional stop, whether issue citation \nConditional stop, whether issue citation notConditional stop, whether search car \nConditional stop, whether search car notConditional stop, whether arrest individual \nConditional stop, whether arrest individual notThis data allows researchers study potential margins discrimination. Given huge amount data many margins study, helpful narrow focus. studying disparities traffic stop rates race motorists. possibilities explore disparities later stages traffic stops (citation/search/arrest) characteristics motorist (gender/age). data massive, focus San Diego.","code":""},{"path":"discrimination-in-police-stops.html","id":"sopp-data","chapter":"3 Discrimination in Police Stops","heading":"3.2 SOPP Data","text":"Now narrowed focus, start exploring data San Diego, always need set working directory data located use use command load data.begin, use describe figure variables dataset\nFigure 3.3: San Diego Stops Data Stanford Open Policing Project Data\nobservation dataset stop made officer somewhere San Diego. stop_time contains time stop. Remember variable, return later. goal understand discrimination stopped. Therefore, frequently using subject_race variable dataset.Subject race categorical variable. Whenever categorical variable variable interest, helpful understand frequency category. example, fraction stops Black drivers, White Drivers, Hispanic Drivers, etc. Stata, use tab command.gives us fraction stops race San Diego. begin understand whether disparity traffic stops race, can compare composition traffic stops overall population see certain groups /underrepresented \nFigure 3.4: Racial Composition Traffic Stops Population\ncan seen Figure 3.4, Black drivers -represented police stops. 11.2 percent stops Black drivers 6.7 percent population Black. Asian/Pacific Islander underrepresented traffic stops (8.5 percent stops vs. 15.9 percent population). Hispanic drivers slightly overrepresented traffic stops (30.7 percent stops vs. 28.8 percent population) White drivers slightly underrepresented (42.5 percent stops vs. 45.1 percent population). One important caveat demographic information 2010 Census, traffic stop data 2014-2017. Therefore, numbers may directly comparable demographics changing time San Diego.However, analysis present striking disparities traffic stops. disparities focus analysis section. particular, learn one test can used understand whether disparities driven racial discrimination part police officers.","code":"cd \"/Users/davidarnold/Dropbox/Teaching/EP5/online/03_week/data\"\nuse san_diego_stops.dta, replace \n/Users/davidarnold/Dropbox/Teaching/EP5/online/03_week/datadescribetab subject_race          subject_race |      Freq.     Percent        Cum.\n-----------------------+-----------------------------------\nAsian/Pacific Islander |     32,482        8.52        8.52\n                 Black |     42,631       11.18       19.71\n              Hispanic |    116,912       30.67       50.38\n                 Other |     27,170        7.13       57.51\n                 White |    161,954       42.49      100.00\n-----------------------+-----------------------------------\n                 Total |    381,149      100.00"},{"path":"discrimination-in-police-stops.html","id":"veil-of-darkness-vod","chapter":"3 Discrimination in Police Stops","heading":"3.3 Veil of Darkness (VOD)","text":"Racial disparities refer differences groups different races. Many factors can cause racial disparities, common focus try understand disparity driven racial discrimination. example, interested exploring racial discrimination part police officers making traffic stops. many methods proposed prove racial discrimination. Today focus one: veil darkness test, test developed Grogger Ridgeway (2006)3.basic idea behind veil darkness test easier observe race individual driver day sunlight relative night dark. Therefore, police officers targeting minority drivers, difficult perform targeting night. can therefore compare racial composition traffic stops daylight vs. nighttime try infer racial discrimination part police officers.Figure 3.5 basic assumptions behind veil darkness test presented. sunset, light , officers can observe race drivers. sunset (dusk), unclear whether race observed . dusk, however, assume officers can longer observe race making decision stop given driver.\nFigure 3.5: Assumptions Behind Veil Darkness Test\ndata, imagine see stops given type drive fall dramatically dusk. might suggest officers targeting individuals dusk. make firm conclusions, need think carefully comparing across different times day.example, 12 noon light . 12 midnight dark . However, many differences 12 noon 12 midnight. drivers road probably completely different. Therefore, racial composition stops different times, maybe due officers, due drivers road?get around issue, Grogger Ridgeway (2006) focus intertwilight period, times sometimes light sometimes dark.example, 6:30 PM sometimes light San Diego (summer). However, different times year (winter) dark 6:30 PM. Therefore, maybe can compare composition traffic stops around 6:30 PM summer winter understand whether police officers likely stop minority drivers light .example, imagine 40 percent drivers stopped Black light (6:30 PM), 35 percent drivers stopped Black dark (still 6:30 PM, different part year). hypothetical example, decrease stops Black drivers must due officers inability racial profile dark.recall, data information time given traffic stop occurred, well race driver. However, also need information whether light dark time given stop. add dataset need merge external data sunset times. introduce us data wrangling Stata","code":""},{"path":"discrimination-in-police-stops.html","id":"data-wrangling","chapter":"3 Discrimination in Police Stops","heading":"3.4 Data Wrangling","text":"start empirical research project social sciences, must ask question: data can use answer question? identified data, rarely comes format ready data analysis. Generally, number steps must taken prepare data. steps often referred data wranglingFor example, Census confidential income data available researchers go application process. data, however, stored separately every state. create master dataset, need combine individual datasets together. form data wrangling. Stata, accomplished append commandNext, imagine interested relationship economic conditions opioid crisis United States. Bureau Labor Statistics provides county-level information unemployment. Centers Disease Control Prevention (CDC) provides county-level opioid prescription data. study question, need combine two datasets together county level. another example data wrangling, Stata, accomplished merge command.Lastly, imagine individual-level data voting rates, interested comparing voting rates across counties. words, convenient analysis dataset county level, rather individual level. another example data wrangling, Stata, can accomplished collapse command.section, learn three commands: append, merge, collapse, particular focus (1) appropriate (2) implement Stata.","code":""},{"path":"discrimination-in-police-stops.html","id":"append","chapter":"3 Discrimination in Police Stops","heading":"3.5 Append","text":"append command used want add rows dataset. common many situations. example, common datasets stored separately year, want analyze data across years. also common datasets stored separately region want analyze data entire country.understand use append command consider simple hypothetical example depicted Figure 3.6.\nFigure 3.6: Hypothetical Data Want Append Together\nexample, two classes: B, two students. dataset, variables. goal append create new dataset 4 observations, 2 school.accomplish Stata? class data stored classA.dta class B data stored classB.dta can combine typing:done first load class data typing use classA.dta, clear. appended class B data typing append using classB.dta. order code work, classA.dta classB.dta need working directory. Additionally, variable names classA classB need . example, one datasets student id variable named sid, need make names consistent across datasets merging. can rename variable rename command. example, rename variable sid student type rename sid student.example, two datasets combine. Imagine , example, class C, class D .can continue append datasets typing, example, append using classC.dta followed append using classD.dta .","code":"use classA.dta, clear \nappend using classB.dta "},{"path":"discrimination-in-police-stops.html","id":"merge-basics","chapter":"3 Discrimination in Police Stops","heading":"3.6 Merge Basics","text":"merge command used want add columns dataset. example, often interested relationships different variables. variables dataset, need way combine datasets. lot cutting-edge research social sciences stems merging together previously unlinked datasets observe new relationships.Whenever merge two datasets Stata, need identify two things. First, variable can data merged ? example, two individual-level datasets, need unique identifier merge individuals. trying merge two datasets students, unique student identifier good option. merging, need identify variable merge verify stored format datasets. many cases, many ways hold information. example, matching two datasets state level, information stored abbreviations (e.g. CA), full state name (e.g. California), numeric state code (e.g. value 6 equal California Federal Information Processing System: FIPS). merge datasets information datasets must stored way.second task identify type merge. three types merges Stata: one--one, many--one one--many. understand differences types merges go simple examples. first go one--one merge, depicted Figure 3.7.\nFigure 3.7: Example one--one Merge\ndataset, unique value student every observation. referred one--one merge matching student school dataset single student gpa dataset.many--one merge one--many merge variable merged multiple observations values one teh two dataset. example, Figure 3.8, merging two datasets based variable school. student-level dataset, multiple observations per school. particular, students 1 2 attend school , students 3 4 attend school B. school-level dataset single observation per school.\nFigure 3.8: Example many--one Merge\nWhether merge many--one one--many depends order load datasets. conceptual level, two types merges . understand clearly, now going learn merge datasets Stata. begin simple one--one merge, proceeding many--one one--many merge.merge two datasets Stata, process takes steps. First, must load one two datasets memory. Whichever dataset load first referred master dataset. Next, use merge command combine master dataset second two datasets. dataset merging called using dataset.try simple datasets. Imagine two datasets (school.dta gpa.dta) want combine .\nFigure 3.9: Merging Student-Level Datasets\nfirst step load one two datasets.loaded school.dta first, school.dta master dataset. now merge gpa.dta.using merge command always need make sure components. First, specified student variable merge . Therefore, student school.dta matched corresponding student gpa.dta. Second, specified 1:1 student unique observation datasets. Lastly, specified merging gpa.dta specifying using gpa.dta.\nFigure 3.10: Merging Student-Level Dataset School-level Dataset\ntwo main differences . First, now merging variable school, variable common datasets. Second, school.dtadataset, multiple observations school. pop.dta, single observation per school. Therefore many--one merge instead one--one merge. Therefore, code merge two datasets now:specify m:1 multiple observations per school master dataset, single using. one--many merge (1:m) conceptually similar. loaded pop.dta first, pop.dta master dataset. case, code merge two datasets given byThe reason switched 1:m single observation per school master dataset multiple using dataset now pop.dta master dataset school.dta using dataset. Therefore, one--many merge vs. many--one merge entirely dependent dataset load first.also option perform many--many merge m:m, however, use option. data transformed ways probably intend. usual solution problem transform data way can perform one--many many--one merge instead","code":"use school.dta, clear merge 1:1 student using gpa.dtause school.dta, clear\nmerge m:1 school using pop.dtause pop.dta, clear\nmerge 1:m school using school.dta"},{"path":"discrimination-in-police-stops.html","id":"merge-advanced","chapter":"3 Discrimination in Police Stops","heading":"3.7 Merge Advanced","text":"Sometimes, two datasets merge completely. example, consider two datasets Figure 3.11.\nFigure 3.11: Example Merge Unmatched Observations\nexample, student 1 test score, homework score, student 6 homework score test score. see happens merge two datasets Stata.Two observations matched. One master dataset (test_score.dta) using dataset (hw_score.dta). new variable (named _merge) added dataset keep track matched vs. unmatched observations. equal 1 observations master dataset, 2 observations using, 3 observations master using dataset.Whether keep unmatched observations dataset depends bit analysis. really need homework score test score analysis, can probably safely keep matched observations. Stata merge command, can typeYou can also specify keep matched observations directly merge command. keeping observations match two datasets, need generate _merge variable, equal 3 matched observations. Instead, can specify option keep(3) keeps matched observations. , order prevent Stata adding uniformative variable _merge, can also specify nogen variable generated. example uses options context hypothetical example.can see, generated table 4 observations now, datasets. specified keep observations datasets specifying keep(3).Now understand use merge command, go common mistakes. first deal matching variable. matching variables need named exact way.example (Figure 3.12), two datasets previous example, student variable named student_id one student \nFigure 3.12: Example Merge Different Names Matching Varaibles\nsee happens try merge two together:variable student_id found variable student_id exist using dataset. order merge two datasets together properly, need make sure names variables . , can use rename command ensure names .Similarly, matching variables must type variable. example, Figure 3.13, student variable stored string first dataset (highlighted red), stored numeric variable second dataset (highlighted black).\nFigure 3.13: Example Merge Different Types Matching Variable\nsee happens try merge two togetherThis error code describing format variables different master vs. using dataset. fix issue, need convert one variable formats. example, type destring student, replace convert student variable numeric variable test_score3.dta dataset.Lastly, discussed one--one merges, many--one merges one--many merges, many--many merges? reason discussed never many--many merges Stata. cases, right way proceed find faced many--many merge restructure data. Usually can use data wrangling (like collapse) transform data one--one, many--one one--many merge. command joinby accomplish something like many--many merge, discuss command book.","code":"/* change working directory */\ncd /Users/davidarnold/Dropbox/Teaching/EP5/online/03_week/data\n/Users/davidarnold/Dropbox/Teaching/EP5/online/03_week/data/*load test score data*/\nuse test_score.dta, clear \n\n/*merge to the hw data*/\nmerge 1:1 student using hw_score.dta\n    Result                      Number of obs\n    -----------------------------------------\n    Not matched                             2\n        from master                         1  (_merge==1)\n        from using                          1  (_merge==2)\n\n    Matched                                 4  (_merge==3)\n    -----------------------------------------keep if _merge==3\n(2 observations deleted)use test_score.dta, clear \nmerge 1:1 student using hw_score.dta, keep(3) nogen\n    Result                      Number of obs\n    -----------------------------------------\n    Not matched                             0\n    Matched                                 4  \n    -----------------------------------------use test_score2.dta, clear \nmerge 1:1 student_id using hw_score.dta\nvariable student_id not found\nr(111);\n\nend of do-file\nr(111);use test_score3.dta, clear \nmerge 1:1 student using hw_score.dta\nkey variable student is str1 in master but float in using data\n    Each key variable -- the variables on which observations are matched -- must be of the\n    same generic type in the master and using datasets.  Same generic type means both numeric\n    or both string.\nr(106);\n\nend of do-file\nr(106);"},{"path":"discrimination-in-police-stops.html","id":"build-data","chapter":"3 Discrimination in Police Stops","heading":"3.8 Build Data","text":"Now understand merge command, use merge traffic stop data data sunset times can implement veil darkness test. begin, need choose period time San Diego sometimes light sometimes dark, depending time year. choose time period 6:30-7:00 PM. Sometimes dark 6:30-7:00 PM (winter example), sometimes light (summer example). need identify whether light dark every stop data.problem, however, data stops currently comes two separate files, one years 2014-2015 one years 2016-2017. Additionally, sunset data comes separate file. Therefore, three separate files need combine create one final dataset.create dataset stops 2014-2017, need append stop-level data.might look nothing happened Stata. check now years data single dataset can type:Now verified current data memory stops 2014-2017. next task merge data sunset times. , need make sure two things. First, variable can merge ? general, need load datasets check variables store information. case, single variable meets criteria: date. Second, one--one many--one merge? Third, date variable datasets stored format?get started, need figure many observations given date dataset. Intuitively, expect multiple observations per date traffic stop data. words, officers stop one individual per day. sunset data, well single sunset time every day, expect single observation per date. data always stored expect, always good check . way going check unique command.unique command come installed Stata. need install version Stata typing ssc install unique Command window. general, might find command online help perform data analysis. Many commands can installed typing ssc install commandname. Therefore, running line code , type ssc install unique Command window press enter.interpret output? Well, number unique values date 1186. words, many different days dataset traffic stops. contrast, 381,149 total stops. Therefore, multiple stops per day. Whenever see records unique values, know many--BLANK merge. next job us figure BLANK replaced one. order need load sd_sunset.dta.dataset, 1186 unique values date 1186 records. value date unique observation dataset. Therefore, can conclude many--one merge. many observations per value date stops data, one observation per value date sunset data.last thing need check format date variable datasets. look first observation sd_sunset.dtaNow, go back stop-level data check date held format.looks like date variable format. , need data wrangling order get format. us, can now merge data sunset data:specified keep(3) nogen, keeping observations correspond dates dataset. Now, going want use data analysis later, save combined dataset new name: sd_analysis_sample.dta typing:code place new file working directory named sd_analysis_sample.dta. Next time want work data, can now load file directly, need repeat steps appending merging datasets together.","code":"use san_diego_stops_2014_2015.dta, clear\nappend using san_diego_stops_2016_2017.dtatab year       year |      Freq.     Percent        Cum.\n------------+-----------------------------------\n       2014 |    138,735       36.40       36.40\n       2015 |    112,201       29.44       65.84\n       2016 |    102,181       26.81       92.65\n       2017 |     28,032        7.35      100.00\n------------+-----------------------------------\n      Total |    381,149      100.00unique dateNumber of unique values of date is  1186\nNumber of records is  381149use sd_sunset.dta, clearunique dateNumber of unique values of date is  1186\nNumber of records is  1186list date if _n==1\n      |      date |\n      |-----------|\n   1. | 01jan2014 |\n      +-----------+use san_diego_stops_2014_2015.dta, clear\nappend using san_diego_stops_2016_2017.dtalist date if _n==1\n        |      date |\n        |-----------|\n     1. | 01jan2014 |\n        +-----------+merge m:1 date using sd_sunset.dta, keep(3) nogen    Result                      Number of obs\n    -----------------------------------------\n    Not matched                             0\n    Matched                           381,149  \n    -----------------------------------------save sd_analysis_sample.dta, replace "},{"path":"discrimination-in-police-stops.html","id":"date-time","chapter":"3 Discrimination in Police Stops","heading":"3.9 Date-Time","text":"Now information perform veil darkness test, need restrict stops test applies. remind , explore whether composition trafic stops race varies whether light dark , controlling time stop.order , restrict traffic stops occur 6:30-7:00 PM. San Diego, parts year, dark 6:30-7:00 PM. times light . , course, time slot work. fact, methods Grogger Ridgeway (2006) use times sometimes light sometimes dark. Performing analysis requires statistical techniques beyond level class, instead focus one particular block time: 6:30-7:00 PM.need restrict stops occur time period. harder think! need learn Stata stores time day.Stata, times day often held terms milliseconds midnight. ? Well, store string variables, add string variables. example, understand string \"6:00 PM\" equal 6:00 PM night, need way store information can things like add subtract additional time (.e. add hour 6:00 PM result 7:00 PM). Strings interpreted words Stata added way.see Stata stores times, can summarize variable stop_time.stored terms milliseconds midnight (0 midnight exactly 1 hour later (1:00 ) \\(60\\cdot 60 \\cdot 1000 = 3,600,000\\), 60 minutes hour, 60 seconds minute, 1000 milliseconds second). many milliseconds 6:30 PM?\\[\n\\underbrace{18}_{\\text{hours}} \\cdot \\underbrace{3,600,000}_{\\text{ms per hour}} + \\underbrace{1,800,000}_{\\text{ms per half hour}}=66,600,000\n\\]\nrestricting 6:30 PM 7:00 PM, restrict stops values stop_time 66,600,000 (6:30 PM) 68,400,000 (7:00 PM).procedure get right answer, typing large numbers can sometimes lead transcription errors careful. better way use Stata commands math . clock() command automatically translates time usually interpret (.e. 9:00 ) milliseconds midnight automatically.example, see many milliseconds midnight 6:00 PM type:\"18:30:00\" 6:30 PM 24-hour clock. \"hms\" tells Stata time interpreted hours, minutes, seconds. Now understand use clock command, can use restrict stops occurred 6:30 PM 7:00 PM:Next, need generate indicators tell us whether given stop occurred sunlight dark. define traffic stop occurring sunlight occurs sunset. variable sunset data contains time sunset, can compare stop_time generate indicator light:words, creating variable equal 1 time stop occured sunset, zero otherwise. define traffic stop occurring dark occurs dusk (dusk 30 minutes sunset). variable dusk data contains time dusk, can compare stop_time generate indicator dark:Now, stops occur sunset dusk. stops, sure driver race observed. neither fully light fully dark times (.e. dark==0 & light==0). going drop stops analysis:Now ready start analysis implement veil darkness.","code":"  use sd_analysis_sample.dta, replace  sum stop_time     Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n   stop_time |    381,149    4.70e+07    2.26e+07          0   8.63e+07  di clock(\"18:30:00\",\"hms\")\n66600000/* drop if before 6:30 PM */\ndrop if stop_time<clock(\"18:30:00\",\"hms\")\n/* drop if after 7:00 PM */\ndrop if stop_time>clock(\"19:00:00\",\"hms\") \n(296,355 observations deleted)\n\n(76,052 observations deleted)gen light = (stop_time<sunset)gen dark = (stop_time>dusk)drop if light==0 & dark==0 \n(414 observations deleted)"},{"path":"discrimination-in-police-stops.html","id":"implement-vod","chapter":"3 Discrimination in Police Stops","heading":"3.10 Implement VOD","text":"need compare composition traffic stops times sunlight vs. darkness implement veil darkness test. stop rates minorities drop dark, indicates targeted sunlight. retrieve composition traffic stops race, can use tab command:want composition, restrict stops sunlight, can add conditional statement code:Now look composition traffic stops dark:tables, trying see certain races much less likely stopped darkness relative sunlight. true, good evidence race targeted sunlight hours. table , composition traffic stops seems relatively similar sunlight vs. dark. However, little difficult absorb information multiple tables. really get information across, might useful put information figure.","code":"tab subject_race           subject_race |      Freq.     Percent        Cum.\n-----------------------+-----------------------------------\nAsian/Pacific Islander |        747        8.97        8.97\n                 Black |      1,084       13.02       21.99\n              Hispanic |      2,847       34.19       56.17\n                 Other |        561        6.74       62.91\n                 White |      3,089       37.09      100.00\n-----------------------+-----------------------------------\n                 Total |      8,328      100.00tab subject_race if dark==0           subject_race |      Freq.     Percent        Cum.\n-----------------------+-----------------------------------\nAsian/Pacific Islander |        375        8.40        8.40\n                 Black |        589       13.20       21.60\n              Hispanic |      1,528       34.24       55.84\n                 Other |        319        7.15       62.98\n                 White |      1,652       37.02      100.00\n-----------------------+-----------------------------------\n                 Total |      4,463      100.00tab subject_race if dark==1           subject_race |      Freq.     Percent        Cum.\n-----------------------+-----------------------------------\nAsian/Pacific Islander |        372        9.62        9.62\n                 Black |        495       12.81       22.43\n              Hispanic |      1,319       34.13       56.56\n                 Other |        242        6.26       62.82\n                 White |      1,437       37.18      100.00\n-----------------------+-----------------------------------\n                 Total |      3,865      100.00"},{"path":"discrimination-in-police-stops.html","id":"bar-graphs","chapter":"3 Discrimination in Police Stops","heading":"3.11 Bar Graphs","text":"Eventually, going use bar graph visualize veil darkness test discrimination. section, going learn construct bar graphs Stata generally. begin, going set hypothetical example. Imagine trying construct bar graph shows average GPA across different schools, based data :\nFigure 3.14: Dataset Schools GPA\ncommand graph bar used make bar graphs Stata. lots ways command can altered, make sure look help file! start, going create bar graph shows average GPA school.basic syntax graph bar command (stat) (mean) want average value y-variable displayed. also options help file. example, command plot count, minimum, maximum, median, among statistics. (xvar) variable defines groups horizontal axis.example, want average variable gpa, type (mean) gpa. want average ? class variable. type (class). together:\nFigure 3.15: Simple Bar Graph\nalways need descriptive titles properly labeled axes. can add way added histograms:\nFigure 3.16: Simple Bar Graph Titles/Labels\nSometimes need change aesthetics figures make easily readable. One external package find useful install blindschemes, provides color schemes Stata graphs distinguishable individuals color blind. install blindschemes type:blindschemes comes number different schemes. can think graphics schemes capturing overall theme aesthetic graph. set scheme plotplainblind, one schemes comes blindschemes package.Now can re-run exact code , overall aesthetic different. changed scheme graph.\nFigure 3.17: Simple Bar Graph Titles/Labels\nmany options constructing bar graphs. Sometimes, option looking available, still ways construct required bar graph. words, may need use data wrangling get data right format construct bar graph. next section learn another data wrangling technique generally useful, particularly useful generating bar graph need order visualize veil darkness test.","code":"graph bar (stat) yvar, over(xvar)graph bar (mean) gpa, over(school)graph bar (mean) gpa, over(school) ///\n    title(\"Average GPA by Classroom\") ///\n    ytitle(\"Average GPA\") ssc install blindschemesset scheme plotplainblindgraph bar (mean) gpa, over(school) ///\n    title(\"Average GPA by Classroom\") ///\n    ytitle(\"Average GPA\") "},{"path":"discrimination-in-police-stops.html","id":"collapse","chapter":"3 Discrimination in Police Stops","heading":"3.12 Collapse","text":"collapse command transforms data different unit analysis. useful? Imagine individual-level data income, care average income state analysis. collapse command allow quickly transform data state level. Imagine want understand averages variable years. Collapsing data year level can quick way understand variable changing time. lot data, collapsing data first may make code much efficient.basic syntax collapse command (stat) statistic. default mean. also use count later chapter. varlist1 variable (list variables) like retrieve mean . varlist2 variable (list variables) define groups statistic calculated.\nFigure 3.18: Wages Workers Different States\ngoal now transform dataset dataset just two observations: one \"CA\" one \"AZ\". terms variables, want two variables, one contains average weekly income one contains average age. words, taking dataset currently worker level collapsing dataset state level. Stata type:collapsed dataset depicted Figure 3.19\nFigure 3.19: Collapsed State-level Dataset\nOne important note order retrieve original dataset, need re-load Stata. simple \"undo\" button gets back original worker-level dataset.","code":"collapse (stat) varlist1, by(varlist2)collapse (mean) weekly_income age, by(state)"},{"path":"discrimination-in-police-stops.html","id":"visualizing-veil-of-darkness","chapter":"3 Discrimination in Police Stops","heading":"3.13 Visualizing Veil of Darkness","text":"section, want construct data visualization allow us see clearly results veil darkness test. look hypothetical examples begin know end goal look like. Figure 3.20 presents hypothetical example results look like.\nFigure 3.20: Veil Darkness Test Finds Discriminatory Policy Behavior\nevidence discriminatory behavior ? case, answer yes. light , 60 percent stopped drivers Black. dark , falls 50 percent. fall fraction stops Black drivers darkness explained discriminatory officers unable observe race dark, thus, unable target minority drivers stops. continuing, make sure understand data presented Figure 3.20 .Figure 3.21 presents different hypothetical example bar graph might look like.\nFigure 3.21: Veil Darkness Test Finds Evidence Discriminatory Policy Behavior\nevidence discriminatory behavior ? case, answer . light , 50 percent stopped drivers Black. dark , remains steady 50 percent. appear data officers targeting minority drivers day.Starting left , now constructed dataset (1) traffic stops occur 6:30-7:00 PM, (2) indicators whether sunlight (dark==0) darkness (dark==1). can use data generate similar visualization graphs .need compare composition traffic stops times sunlight vs. darkness. can use collapse command create dataset statistics. start, create dataset number stops race, daylight darkness.count number observations per race daylight darkness, can use count option collapse command. option count number observations non-missing values variable. begin, create variable (eventually store counts) non-missing observations.Now might immediately clear created variable. Right now just variable equal 1 every observation. However, collapse data, variable hold information need construct bar graph. collapse data subject-race--dark condition level:specify two variables , resulting dataset includes observation every unique combination two variables. dataset, 5 different values race variable, 2 different values dark variable. resulting dataset case 10 observations. row correspond given race given light condition.Figure 3.22 displays resulting collapsed dataset. can see, race light condition, now total number stops occurred race, within given light condition. example, first row indicates dataset, 375 stops Asian/Pacific Islanders 6:30-7:00 PM light (.e. dark==0).\nFigure 3.22: Collapsed Dataset\nplot, however, want plot fraction stops race light condition. pretty small dataset, manually. example, fraction stops light Asian/Pacific Islanders. Based 3.22, know equal :\\[\n\\frac{\\text{Stops Asian/Pacific Islander Light}}{\\text{Stops Light}} = \\\\ \\frac{375}{375+589+1528+319+1652} =  0.084\n\\]\nobservations. Also, need values dataset. easier can add variable dataset contains information. going two steps. First, going create variable contains total stops within given light condition. , going generate variable want dividing obs_count new variable.create new variable going use egen command. egen stands extensions generate, useful trying create new variables. ever want create variable, figure exactly , type help egen see possible egen command.First, present code calculating total stops light condition, explaining works:bysort dark: tells Stata code comes : done separately values dark variable. egen total_stops = total(obs_count) create new variable equal sum total variable obs_count. simple terms, total number stops daylight darkness. look resulting dataset. Make sure understand total_stops represents moving next section.\nFigure 3.23: Collapsed Dataset Total Stops Light Condition\nNow (1) total stops race daylight vs. darkness (2) total stops overall daylight vs. darkness can create required variables:Now can use variable show composition traffic stops (race) daylight darkness:\nFigure 3.24: Fraction Stops Race Light Condition\ngood start. information need. However, next impossible external reader understand information presented means. 0 1? bar graph actually showing? next section introduce things improve data visualization interpretable reader.","code":"gen obs_count = 1collapse (count) obs_count, by(subject_race dark) bysort dark: egen total_stops = total(obs_count)  gen fraction_stops = obs_count/total_stopsgraph bar fraction_stops, over(dark) over(subject_race)"},{"path":"discrimination-in-police-stops.html","id":"improving-aesthetics","chapter":"3 Discrimination in Police Stops","heading":"3.14 Improving Aesthetics","text":"always, first task add descriptive titles labels:\nFigure 3.25: Fraction Stops Race Light Condition\ngenerated data, know dark==0 represents daylight dark==1 represents darkness, reader . need relabel zero light 1 dark reader understand graph represents. , can use relabel option:\nFigure 3.26: Fraction Stops Race Light Condition\nNext, try make graph even easier read. practice, expect need next option often, helps case. option asyvars, treats first () group y-variable. case? Basically, makes Light Dark labels appear legend instead x-axis. make graph look little cleaner easier interpret quickly:\nFigure 3.27: Fraction Stops Race Light Condition\nlast step can change colors bars. code , bar(1, fcolor(sky)) changes Light bars sky blue. bar(2,color(vermillion)) changes Dark bars vermillion.\nFigure 3.28: Fraction Stops Race Light Condition\nnow spend little time interpreting main result. Overall, appears fraction stops given race depend dramatically light condition. can see graph fact blue orange bars tend around height. orange bar (.e. dark==1) dramatically lower races, evidence police targeting race daylight.Like empirical design, test assumptions limitations. fatal flaw given set research limitations. research limitations important think carefully . next section discuss limtations, well feature factors studied Stanford Open Policing Project data.","code":"graph bar fraction, over(dark) over(subject_race) ///\n    title(\"Composition of Traffic Stops by Light Conditions\") ///\n    ytitle(\"Fraction of All Stops\") graph bar fraction, ///\n  over(dark,relabel(1 \"Light\" 2 \"Dark\")) over(subject_race) ///\n  title(\"Composition of Traffic Stops by Light Conditions\") ///\n  ytitle(\"Fraction of All Stops\") graph bar fraction, asyvars ///\n  over(dark,relabel(1 \"Light\" 2 \"Dark\")) over(subject_race) ///\n  title(\"Composition of Traffic Stops by Light Conditions\") ///\n  ytitle(\"Fraction of All Stops\") graph bar fraction, asyvars ///\n  over(dark,relabel(1 \"Light\" 2 \"Dark\")) over(subject_race) ///\n  title(\"Composition of Traffic Stops by Light Conditions\") ///\n  ytitle(\"Fraction of All Stops\") ///\n  bar(1, fcolor(sky)) ///\n  bar(2, fcolor(vermillion))"},{"path":"discrimination-in-police-stops.html","id":"conclusion-1","chapter":"3 Discrimination in Police Stops","heading":"3.15 Conclusion","text":"discuss implicit assumptions veil darkness test. First, assumed driver race unobservable dusk. artificial lighting, like street lamps? certain areas officers can detect race, method underestimate level discrimination. words, driver race observed sunlight darkness, logic test method identify discrimination longer holds.Another assumption driving behavior dark. minority drivers adjust driving behavior potential racial profiling daylight? Kalinowski, Ross, Ross (2021)4 find evidence true certain areas.data lot possibilities explored small portion . final section think possible questions explore.First, focused one place: San Diego. can look test finds different results across different areas. find different results, can ask ? factors predict high levels discriminatory behavior? policies used reduce discriminatory behavior?Second, focused initial decision stop driver. discrimination stages stop. example, police officers likely search contraband driver minority? something researchers Stanford Open Policing Project explored. figure SOPP website shows search rates race across different U.S.\nFigure 3.29: Disparities Search Rates Across U.S. Cities\ncan seen Figure 3.29, search rates tend higher Black Hispanic drivers across cities U.S.still just scratches surface can done data! main takeaway chapter: data can powerful tool informs policy. Stanford Open Policing Project one effort improve transparency policing making data publicly available researchers journalists.","code":""},{"path":"classroom-technology.html","id":"classroom-technology","chapter":"4 Classroom Technology","heading":"4 Classroom Technology","text":"","code":""},{"path":"classroom-technology.html","id":"disrupting-education-intro","chapter":"4 Classroom Technology","heading":"4.1 Disrupting Education Intro","text":"Many developing countries rapidly expanded access education. However, places see measurable gains learning. example, India, 50 percent students grade 5 read grade 2 level, despite enrollment rates around 95 percent (Pratham 2017)5.One hypothesis behind puzzling fact termed mismatch hypothesis. curriculum beyond student's level understanding, may learn little, even attendance high. words, mismatch level understanding curriculum taught. Muralidharan, Singh Ganimiam (2019)6, authors study whether technology aims \"Teach Right Level\" can used improve learning outcomes. important question: need find interventions work! necessarily true investment education results better outcomes investing wrong things.technology authors study called Mindspark. Mindspark computer software provides learning materials appropriate student's understanding level. using information questions student gets right vs. wrong. level material increases difficulty student learned previous concepts. Mindspark centers provide 6-day--week instruction 90 minutes (45 minutes Mindspark, 45 minutes teaching assistants). question Muralidharan, Singh Ganimiam (2019) : program improve learning outcomes?answer question, authors recruited participants low-income neighborhoods Delhi. demonstration sessions Mindspark, parents introduced program. participants interested participating needed fill baseline assessment. , parents told half participants receive voucher waived tuition Mindspark program (participants chosen lottery). Students chosen lottery told obtain free access centers February 2016 (experiment concluded). result 619 participants, 305 control (.e. access Mindspark) 314 treatment (access Mindspark)begin, load dataset mindspark_data.dtaAs usual, begin describing dataset\nFigure 4.1: Describing Mindspark Data\nsubset variables Muralidharan, Singh, Ganimian (2019). interested treated impacts learning outcomes. information Math Hindi tests baseline endline. Baseline means test taken student offered free Mindspark. Endline means test taken treated students received Mindspark using software several months. start, summarize variables get sense average test scores well variation test scores baseline.baseline, average score math test 0.318. indicates average across students, average score 31.8 percent math test. Hindi test, average bit higher 0.428. Now look endline scores.endline, scores higher. average math score increased 0.503 average Hindi score increased 0.552. ultimate goal, however, understand test scores change depending whether given access Mindspark. words, test scores endline depend whether treat==1 treat==0? start analyzing data, however, going learn new statistical technique: regression.","code":"cd \"/Users/davidarnold/Dropbox/Teaching/EP5/online/04_week/data\"\nuse mindspark_data.dta, replace\n/Users/davidarnold/Dropbox/Teaching/EP5/online/04_week/datasum per_math1\nsum per_hindi1    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n   per_math1 |        619    .3177475    .1115478          0   .7428572\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n  per_hindi1 |        619    .4278406      .16484          0         .9sum per_math2\nsum per_hindi2    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n   per_math2 |        539     .503101    .1725604          0   .9411765\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n  per_hindi2 |        539    .5521954    .1911098          0   .9666666"},{"path":"classroom-technology.html","id":"linear-regression-theory","chapter":"4 Classroom Technology","heading":"4.2 Linear Regression (Theory)","text":"Linear regression estimates linear relationship variables. getting estimation part, first review linear equations. linear equation equation can written following form\\[\ny=mx+b\n\\]\\(m\\) slope \\(b\\) intercept. example, linear relationship degress Celsius (C) degrees Fahrenheit (F)\\[\nF= 1.8 \\cdot C + 32\n\\]\nFigure 4.2: Converting Celsius Fahrenheit\nexample, direct linear relationship two variables, Celsius Fahrenheit. Often social sciences, interested estimating relationship two variables. much education increase income? precipitation impact voter participation? One way understand two variables related estimating linear relationships. Linear Regression way estimate linear relationships one variablesWe want model outcome individuals \\(Y_i\\) function explanatory variable individual \\(X_i\\) (math: \\(Y_i=f(X_i)\\)). \\(Y_i\\) often referred dependent variable (outcome variable). \\(X_i\\) often referred independent variable (explanatory variable). assume function \\(f(X_i)\\) linear, can write model \\[\nY_i = \\beta_0 + \\beta_1 \\cdot X_i + \\varepsilon_i \\nonumber\n\\]Imagine hypothesize students watch much TV night test perform worse. want understand test scores (outcome variable \\(Y_i\\)) related hours TV watched (explanatory variable \\(X_i\\)). posit linear relationship two\\[\n\\text{Test Score}_i = \\beta_0 + \\beta_1 \\cdot \\text{Hours Watched TV}_i + \\varepsilon_i\n\\]parameters model parameters estimating. linear regression model, two parameters need estimate: \\(\\beta_0\\) \\(\\beta_1\\) parameters model. use \"hats\" denote estimates parameters. \\(\\hat{\\beta}_0\\) estimate intercept. \\(\\hat{\\beta}_1\\) estimate slope coefficient. choose \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) construct regression line best fits data.best fit actually mean? choose \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) minimize sum squared errors (\\(\\sum \\varepsilon_i^2\\)). Choosing parameters minimize sum squared errors referred Ordinary Least Squares commonly simply referred linear regression.go finding best fit? consider estimating relationship two variables Figure ??. horizontal axis hours spent watching TV, vertical axis student performed test. example, four data points.\nFigure 4.3: Hypothetical Dataset: Finding Best Fit Line\ndraw line assess whether fit good bad. Consider line Figure 4.4.\nFigure 4.4: Hypothetical Dataset: Finding Best Fit Line\nwell line fit? measure take difference line actual values, known residual. residuals negative (meaning actual value less predicted based line), others positive (mean actual value grater predicted based line). terms \"fit\" care whether error positive negative, assess fit first \"square\" residuals. square residuals positive number, larger positive numbers indicating higher error.construct one number tells us well line fits data construct sum squared residuals. example, number given \\[\nSSR = (12)^2+(0)^2+(15)^2+(8)^2=433\n\\]Now, try draw new line assess whether fit improved. Consider line Figure 4.5\nFigure 4.5: Hypothetical Dataset: Finding Best Fit Line\ncan compute sum squared residuals, now equal :\\[\nSSR = (5)^2+(-6)^2+(8)^2+(2)^2=129\n\\]\ncomparing previous line, can see fits better. sum squared residuals smaller. Intuitively, indicates points trying predict (test scores) closer line Figure 4.5 Figure 4.4.Now, linear regression picks line minimizes SSR. Lucky us, need keep drawing hypothetical lines computing SSR figure solution. mathematical solution problem learn future statistics econometrics courses. means purposes computers can find line best fits data fast.","code":""},{"path":"classroom-technology.html","id":"predicted-values","chapter":"4 Classroom Technology","heading":"4.3 Predicted Values","text":"Now learned linear regression line estimated, talk can information. first concept introduce predicted values fitted values. understand concept, first going set real-world application.application, interested relationship life expectancy country average income country. propose modeling life expectancy linear function average income:\\[\n\\text{Life Expectancy}_i = \\beta_0 + \\beta_1 \\cdot \\text{Average Income 1000s}_i + \\varepsilon_i\n\\]Figure 4.6 plots life expectancy country vs average income, stored $1,000 US dollars. exampe, value 50 indicates average income country 50,000 U.S. dollars per year.\nFigure 4.6: Life Expectancy vs. Average Income (1000s) Across Countries\nImagine now use linear regression fit line data Figure 4.6. Imagine find \\(\\hat{\\beta_0}=67.97\\) \\(\\hat{\\beta_1}=0.24\\). values define line intercept equal 67.97 slope equal 0.24. question now , can information?Well, one thing can form predictions life expectancy given value average income. words, tell country's average income, use linear regression estimates predict life expectancy country . social sciences, sometimes less emphasis prediction. However, many settings, one might interested forming predictions. example, imagine work company trying predict buy good based characteristics individuals. use linear regression prediction problem. policy settings, decisions often depend predictions. example, policymaker may want predict whether given defendant recidivate released parole. Linear regression one (powerful) way form predictions. likely first tool learn machine learning classes focuses primarily prediction.Imagine tell country average income $50k. use regression results form best guess life expectancy country? Well, begin, plot line best fits data. orange line Figure 4.7 line best fit. line intercept equal 67.97 slope coefficient equal 0.24.\nFigure 4.7: Life Expectancy vs. Average Income (1000s) Across Countries\nOne way interpret line best (linear) prediction life expectancy given average income. words, want predict life expectancy country average income $50k, can just find line predict country. Figure 4.7, added dashed line average income $50k. average income $50k, appears value line around 80. need just eyeball graph figure information . exact parameters line regression estimates. know intercept 67.97 slope coefficient 0.24. Therefore, can use linear equation form predicted life expectancy country income equal $50k:\\[\n\\text{Predicted Life Expectancy}_i = 60.97 + 0.24 \\cdot 50 = 79.97\n\\]general, want form predicted value observation value \\(X_i\\), can form\\[\n\\text{Predicted Y}_i = \\hat{Y}_i= \\hat{\\beta_0} + \\hat{\\beta}_1 \\cdot X_i\n\\]\ncommon denote predicted value observation \\(\\) \\(\\hat{Y}_i\\).Now, course, prediction prone error. Even complex predictions use potentially hundreds variables always right. example, Netflix uses complicated machine-learning algorithm predict shows recommend . sometimes wrong. Sometimes interested recommended shows.Therefore, often helpful characterize error associated model. given observation, error associated model given \\[\n\\text{Error}_i = \\text{Actual}_i-\\text{Predicted}_i\n\\]example, U.S.'s average income 60k. use regression model predict Life Expectancy, get:\\[\n\\text{Predicted Life Expectancy}_i = 67.97 + 0.24 \\cdot 60 = 82.37\n\\]words, based U.S.'s average income, expect life expectancy 82.37 years. look data, see actually 77 years. Therefore, error associated U.S. equal \\[\n\\text{Error}_i = \\text{Actual}_i-\\text{Predicted}_i = 77-82.37=-5.37\n\\]\nTherefore, actual life expectancy U.S. 5.37 years lower life expectancy predicted linear model.alluded , ways form predictions. certain cases, linear model might appropriate way model data. Maybe relationship appear particularly linear. example, Figure 4.6, appears low incomes, increase income results steep rise life expectancy. , higher incomes, near $60k, $70k, appear increases income associated increases life expectancy. given free reign draw curve data, curve start steep low incomes, get flatter high incomes. words, curve nonlinear.going cover nonlinear methods course. turns can sometimes improve model simply changing variables bit. going try next estimate linear regression models life expectancy function natural logarithm average income. words, estimate linear regression following form.\\[\n\\text{Life Expectancy}_i = \\beta_0 + \\beta_1 \\cdot \\text{Ln(Average Income)}_i + \\varepsilon_i\n\\]key difference now Ln(Average Income) explanatory variable. Ln() stands natural logarithm. probably seen graph one variables \"logged\". can done reasons. First, can reduce influence outliers. natural logarithm concave function, simple terms, means shrinks large numbers shrinks small numbers. second reason often see logged graphs relationship two variables Y X may nonlinear, relationship Y ln(X) may linear.different bases logarithmic function. always use natural logarithm. far widely used interpretable. ever hear read \"log\" class, always interpret \"natural logarithm\".look \"logging\" changes scatterplot. Figure 4.8 plots life expectancy agains natural logarithm average income. can see, relationship appears linear now. Taking natural logarithm shrunk values income countries high incomes countries low incomes. result setting relationship life expectancy log average income appears linear relationship life expectancy average income.\nFigure 4.8: Life Expectancy vs. Log Average Income Across Countries\nnow imagine find \\(\\hat{\\beta_0}=28.69\\) \\(\\hat{\\beta_1}=4.72\\). ask predicted life expectancy country average income $50k.common mistake forgetting remember exactly modeling. modeling life expectancy function natural logarithm income. form correct predicted value, need plug \\(Ln(50000)\\) equation best-fit line:\\[\n\\text{Predicted Life Expectancy}_i = 28.69 + 4.72 \\cdot Ln(50000) = 79.76\n\\]\nNow, imagine forgotten take logarithm. instead plugged 50 (like X-variable income thousands), get:\\[\n\\text{Predicted Life Expectancy}_i = 28.69 + 4.72 \\cdot 50 =264.69\n\\]immediately realize 264.69 way high life expectancy. always think numbers get forming predictions. seems wildly , may forgotten transform variable appropriately.","code":""},{"path":"classroom-technology.html","id":"slope-coefficient","chapter":"4 Classroom Technology","heading":"4.4 Slope Coefficient","text":"social scientists, often interested changes explanatory variable (\\(X\\) variable) associated changes outcome variable (\\(Y\\) variable). slope coefficient (\\(\\beta_1\\)) linear regression model informative .Recall modeled Life Expectancy linear function Average Income (1000s) found\\[\n\\text{Predicted Life Expectancy}_i = 67.97 + 0.24 \\cdot \\text{Average Income 1000s}_i\n\\]slope coefficient captures changes Average Income (1000s) associated changes Predicted Life Expectancy.understand interpretation slope coefficient (0.24), go simple example. Imagine tell two countries, Country Country B. Country Average Income $50k. Country B Average Income $51k. expected difference Life Expectancy two countries?can form Predicted Life Expectancy countries:\n\\[\n\\text{Predicted Life Expectancy } = 67.97 + 0.24 \\cdot 50 \\nonumber \\\\\n\\text{Predicted Life Expectancy B} = 67.97 + 0.24 \\cdot 51 \\nonumber\n\\]Taking difference two yields\\[\n\\text{Difference Predicted Life Expectancy} = 0.24 = \\text{slope coefficient}\n\\]slope coefficient generally reported order understand magnitude relationship. general interpretation :Interpretation slope coefficient: 1-unit change X associated slope-coefficient unit change Y variableWhen actually discuss results, though, want clear unit represents. example, example, write 1-unit change Average Income (1000s) associated 0.24 year increase Life Expectancy.discussion highlights important keep mind discussing results: always clear units. example, case, X-variable (Average Income) denoted thousands. 50 plugged calculation predicted life expectancy Country , 50,000. highlight confusion can occur careful units, turn back regression X equal Ln(Average Income):\\[\n\\text{Predicted Life Expectancy}_i = 28.69 + 4.72 \\cdot \\text{Ln(Average Income)}\n\\]\ninterpret slope coefficient now? 1-unit change Ln(Average Income) associated 4.72 year increase life expectancy.reason coefficient much bigger now simply 1-unit change Ln(Average Income) much larger 1000 dollar change income. example, imagine compare two countries, one \\(\\text{Ln(Average Income)}=9\\) another \\(\\text{Ln(Average Income)}=10\\). Average Income levels country \\(\\text{Ln(Average Income)}=9\\) 8,103 U.S. dollars (.e. \\(Ln(8103) \\approx 9\\)). Average Income levels country \\(Ln(Average Income)=10\\) $22026 dollars. 1-unit change Ln(Average Income) quite large translating dollars levels.Concept CheckTry figure increase \\(Ln(Average Income)=10\\) \\(Ln(Average Income)=11\\) levels. Stata, want figure want transform \\(Ln(Average Income)=10\\) \\(Average Income\\), can type di exp(10). gives correct answer exponential function inverse natural logarithm function \\(exp(Ln(average income))=average income\\).almost applications, interested relationship outcome \\(Y\\) explanatory variable \\(X\\). way gauge magnitude relationship regression framework slope coefficient. slope coefficient captures much expect \\(Y\\) change response 1-unit change \\(X\\). slope coefficient plays pivotal role social science research.","code":""},{"path":"classroom-technology.html","id":"regression-stata","chapter":"4 Classroom Technology","heading":"4.5 Regression (Stata)","text":"Now understand concept linear regression, learn estimate linear regression Stata. begin, going set general linear regression framework.\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon\n\\]basic syntax reg command :produce table many statistics. now focus intercept \\(\\hat{\\beta_0}\\) slope \\(\\hat{\\beta_1}\\). see works practice, load data life expectancy average incomes across different countries, named gapminder.dta.Reminder, modeling life expectancy function income per capita (thousands). means dependent variable (Y-variable) life expectancy independent variable (X-variable) income per capita (thousands). Now estimate regression:lot numbers table . ones focus column \"Coefficient.\" first coefficient row begins average_income. slope coefficient. number tells 1-unit change average_income predicted change Y-variable. case, regression tells us 1-unit change average income (denoted 1,000s) associated 0.24 year increase life expectancy. simpler terms, $1,000 dollar increase average income associated 0.24 year increase life expectancy.intercept linear regression model appears next row, next label _cons. label _cons stands constant. terms intercept constant often used interchangeably linear regression models.output, able draw linear regression line. good exercise try draw pen paper, instead using Stata. can draw correctly, indicates understanding various components output.easy check work using built-functions Stata. Stata can plot regression line twoway lfit command. lfit stands linear fit. basic syntax command type:plot linear regression line output :example, plot linear regression line, along scatter plot, can type:code generate Figure 4.9 .\nFigure 4.9: Life Expectancy vs.Average Income Across Countries\n","code":"reg yvar xvar/*load data*/ \ncd \"/Users/davidarnold/Dropbox/Teaching/EP5/online/04_week/data\"\nuse gapminder.dta, replace \n/Users/davidarnold/Dropbox/Teaching/EP5/online/04_week/datareg life_expectancy average_income      Source |       SS           df       MS      Number of obs   =       186\n-------------+----------------------------------   F(1, 184)       =    178.14\n       Model |  4150.14525         1  4150.14525   Prob > F        =    0.0000\n    Residual |  4286.67682       184  23.2971567   R-squared       =    0.4919\n-------------+----------------------------------   Adj R-squared   =    0.4891\n       Total |  8436.82208       185  45.6044436   Root MSE        =    4.8267\n\n--------------------------------------------------------------------------------\nlife_expecta~y | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n---------------+----------------------------------------------------------------\naverage_income |   .2363563   .0177087    13.35   0.000      .201418    .2712945\n         _cons |   67.97336   .4874037   139.46   0.000     67.01174    68.93498\n--------------------------------------------------------------------------------twoway lfit yvar xvarreg yvar xvartwoway scatter life_expectancy average_income, ///\n    msymbol(circle_hollow) msize(small) ///\n    || lfit life_expectancy average_income, lw(0.4) lc(red) ///\n    title(\"Regression of Life Expectancy on Average Income\") ///\n    xtitle(\"Average Income (1000s)\") ///\n    ytitle(\"Life Expectancy\") ///\n    graphregion(color(white) fcolor(white)) "},{"path":"classroom-technology.html","id":"predict","chapter":"4 Classroom Technology","heading":"4.6 Predict","text":"section, discuss form predicted values Stata. remind , predicted value observation equal :\\[\n\\text{Predicted Value}_i = \\hat{\\beta_0} + \\hat{\\beta}_1 X_i\n\\]\nexample\\[\n\\text{Predicted Life Expectancy}_i = \\hat{\\beta_0} + \\hat{\\beta}_1 \\cdot \\text{Average Income}_i\n\\]\ncan manually construct predicted values every country dataset. just need take value average income form predicted value using linear regression output. However, can also quickly efficiently using predict command. basic syntax predict command isYou just need replace word newvar whatever want variable holds predictions named.first step forming predictions first estimate regression (predict uses results recently executed regression estimation). words, predict command work estimate regression first. going estimate regression prior section:Now regression estimated, can form predictionsNow variable predicted_life_expectancy added dataset. look value predictions first three countries dataset..first country Afghanistan. life expectancy Afghanistan equal 63.4 years. average income Afghanistan equal 1,920 U.S. dollars per year. Given average annual income, linear model predicts life expectancy Afghanistan 68.4 years.notice predictions prone error. cases, may want generate new variable error associated given observation. Now formed predicted_life_expectancy can form error asLet's summarize new error variableThe mean zero (construction, regression line chosen average predictions correct). large variance though, countries life expectancy much lower expected (negative error), much higher.look 3 countries negative errors (implying life expectancy much lower predicted income):can also look 3 countries positive errors (implying life expectancy much higher predicted income):sometimes interesting explore observations hard predict model. countries negative positive errors, likely many factors impacting life expectancy, besides average income. hard time predicting life expectancy countries.","code":"predict newvarreg life_expectancy average_income      Source |       SS           df       MS      Number of obs   =       186\n-------------+----------------------------------   F(1, 184)       =    178.14\n       Model |  4150.14525         1  4150.14525   Prob > F        =    0.0000\n    Residual |  4286.67682       184  23.2971567   R-squared       =    0.4919\n-------------+----------------------------------   Adj R-squared   =    0.4891\n       Total |  8436.82208       185  45.6044436   Root MSE        =    4.8267\n\n--------------------------------------------------------------------------------\nlife_expecta~y | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n---------------+----------------------------------------------------------------\naverage_income |   .2363563   .0177087    13.35   0.000      .201418    .2712945\n         _cons |   67.97336   .4874037   139.46   0.000     67.01174    68.93498\n--------------------------------------------------------------------------------predict predicted_life_expectancy\n(option xb assumed; fitted values)list country life_expectancy average_income predicted_life_expectancy if _n<=3\n     |     country   life_e~y   averag~e   predic~y |\n     |----------------------------------------------|\n  1. | Afghanistan       63.4       1.92   68.42716 |\n  2. |     Albania       77.9       13.3    71.1169 |\n  3. |     Algeria       76.2       10.6   70.47874 |\n     +----------------------------------------------+gen error = life_expectancy - predicted_life_expectancysum error\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       error |        186   -2.05e-08     4.81365  -16.57843   8.367805sort error\nlist country life_expectancy predicted_life_expectancy error if _n<=3\n     |                  country   life_e~y   predic~y       error |\n     |------------------------------------------------------------|\n  1. |                  Lesotho         52   68.57843   -16.57843 |\n  2. | Central African Republic       52.4    68.1927   -15.79269 |\n  3. |                    Qatar       76.2   88.65453   -12.45454 |\n     +------------------------------------------------------------+sort error\nlist country life_expectancy predicted_life_expectancy error ///\nif _n>=184\n     |  country   life_e~y   predic~y      error |\n     |-------------------------------------------|\n184. |    Japan       84.7   77.38034   7.319656 |\n185. | Maldives       79.1   71.04599   8.054008 |\n186. |     Cuba       78.7   70.33219   8.367805 |\n     +-------------------------------------------+"},{"path":"classroom-technology.html","id":"macros","chapter":"4 Classroom Technology","heading":"4.7 Macros","text":"continuing discussion regression, going discuss concept macros. Macros simply words characters store certain output. Stata, many commands store results commands macros. example, sum command, value mean stored macro r(mean). see work practice, continue example studies relationship life expectancy average income.begin, summarize life_expectancy variable:mean life expectancy equal 72.45. Stata automatically stored mean r(mean)\naccess value within macro, type:often useful want reference value later code. Copying value results window can prone transcription errors. Using macros store information lead fewer errors.regressions, Stata also stores number useful results. particular, Stata stores value intercept slope coefficient. general, _b[varname] stores coefficient estimate variable named varname.example, estimate:can retrieve coefficient average_income typing:can retrieve intercept typing:Using macros can helpful generating new variables. example, can form predicted value observation forming:also \"manually\" copying values intercept slope coefficient output, using macros ensures make transcription errors.Stata store useful macros , can also construct macros Stata. can useful typing something long. Instead repeatedly typing, can store information macro. example, one common way use macros store list variable. next example, create global macro stores list variables.Now whenever type \\$vars Stata, Stata interpret \"average_income life_expectancy\". see happens type:One thing \"local\" macro.difference local macro global macro. practice, reference global macro dollar sign $, local apostrophes `macro'. can store information, locals can accessed within given Stata session. Many advise use local macros. Setting globals can conflict aspects Stata. purposes, rare actually need use macros, let alone global vs. local. section introduces macros important programming tool understand conceptually, required fully understand nuances local global macros Stata understand rest course material.","code":"sum life_expectancy\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nlife_expec~y |        186    72.44624    6.753106         52       84.9di `r(mean)'\n72.446237reg life_expectancy average_income \n      Source |       SS           df       MS      Number of obs   =       186\n-------------+----------------------------------   F(1, 184)       =    178.14\n       Model |  4150.14525         1  4150.14525   Prob > F        =    0.0000\n    Residual |  4286.67682       184  23.2971567   R-squared       =    0.4919\n-------------+----------------------------------   Adj R-squared   =    0.4891\n       Total |  8436.82208       185  45.6044436   Root MSE        =    4.8267\n\n--------------------------------------------------------------------------------\nlife_expecta~y | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n---------------+----------------------------------------------------------------\naverage_income |   .2363563   .0177087    13.35   0.000      .201418    .2712945\n         _cons |   67.97336   .4874037   139.46   0.000     67.01174    68.93498\n--------------------------------------------------------------------------------di _b[average_income]\n.23635625di _b[_cons]\n67.973358gen predicted_value = _b[_cons] + _b[average_income]*average_income global vars = \"average_income life_expectancy\"sum $vars\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\naverage_in~e |        186    18.92431    20.03912       .673        111\nlife_expec~y |        186    72.44624    6.753106         52       84.9local vars = \"average_income life_expectancy\"\nsum `vars'\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\naverage_in~e |        186    18.92431    20.03912       .673        111\nlife_expec~y |        186    72.44624    6.753106         52       84.9"},{"path":"classroom-technology.html","id":"explore-data","chapter":"4 Classroom Technology","heading":"4.8 Explore Data","text":"section, going introduce data experiment run Muralidharan, Singh, Ganimian (2019). begin exploring data, going explore achievement gaps. motivation paper many students India grade level terms understanding. students grade level, may actually learn much attending school. section document (1) average students performing well grade level (2) year school associated additional year understanding.variables exploring dataset mindspark_levels.dta. class grade student enrolled . mathelevel assessed level student's understanding. Therefore, individual's class greater assessed math level, behind understanding curriculum.begin, provide scatter plot assessed math level actual grade. first step load data:basic scatterplot create data displayed \nFigure 4.10: Assessed Math Grade Level vs. Actual Grade Level\nFigure 4.10 't seem particularly informative. reason many points overlapping. know 600 participants experiment, see handful dots graph. reason many individuals values variables. words, many people, example, assessed math level equal 4 actual grade level equal 6. figure, however, tell many. One way can improve graph use jitter() option.Scatter plots give us transparent way view data, variables categorical (like grade mathlevel example), can less informative many observations overlap. jitter() adds small amount noise (small perturbations) data points longer overlap graph. way works parentheses place number determines much noise added. (5) common choice. value (100) make graph completely dominated noise.understand jitter() , try scatter plot.\nFigure 4.11: Assessed Math Grade Level vs. Actual Grade Level\ncan see figure, students performing grade level, none performing grade level. large clusters grades several years grade level. example, students grade 6, largest clusters 4th grade math level 3rd grade math level. gives us sense scope issue. Students understanding well grade level.Next, try get sense learning evolving time. basically two potential \"stories\" . Story 1: students grade level, year still learning (.e. increasing grade 1 associated 1-year increase assessed math level). Story 2: students performing grade level makes learning difficult (.e. increasing grade 1 associated less 1-year increase assessed math level). Figuring story true important. story 1 true, even though students behind, still learning information every year. fall behind time. story 2 true, serious issue learning. Students behind falling behind every year.can study question regression framework. interested relationship assessed math level grade\\[\n\\text{Math Level}_i = \\alpha + \\beta \\cdot \\text{Grade}_i + \\varepsilon_i\n\\]\\(\\beta\\) tells us much expect assessed math level increase 1-unit increase Grade. much lower 1, students fall farther farther behind year. estimate regression Stata:Reading table , find \\(\\hat{\\beta}=0.293\\). indicates 1-year increase grade associated 0.293 year increased assessed math level. words, students learning time, much lower rate envisioned curriculum. Ideally, 1 year schooling associated additional year learning.results highlight need intervention. documented (1) students (average) assessed performing grade level, (2) large variation assessed grade level within given grade, (3) learning time slower intended. Maybe intervention \"Teaches Right Level\" can help alleviate issues.","code":"cd \"/Users/davidarnold/Dropbox/Teaching/EP5/online/04_week/data\"\nuse mindspark_levels.dta, clear\n/Users/davidarnold/Dropbox/Teaching/EP5/online/04_week/dataset scheme plotplainblind\ntwoway scatter mathlevel classtwoway scatter mathlevel class, jitter(5) ///\n    xlabel(5(1)10) ///\n    xtitle(\"Grade enrolled in\") ///\n    ytitle(\"Assessed grade level of student achievement\") reg mathlevel class       Source |       SS           df       MS      Number of obs   =       253\n-------------+----------------------------------   F(1, 251)       =     19.02\n       Model |  22.8033097         1  22.8033097   Prob > F        =    0.0000\n    Residual |   300.87258       251  1.19869554   R-squared       =    0.0705\n-------------+----------------------------------   Adj R-squared   =    0.0667\n       Total |  323.675889       252  1.28442813   Root MSE        =    1.0948\n\n------------------------------------------------------------------------------\n   mathlevel | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       class |   .2926913   .0671066     4.36   0.000     .1605275     .424855\n       _cons |   1.756712   .4928781     3.56   0.000      .786008    2.727416\n------------------------------------------------------------------------------"},{"path":"classroom-technology.html","id":"summary-statistics-1","chapter":"4 Classroom Technology","heading":"4.9 Summary Statistics","text":"getting experimental data, need first make sure randomization implemented correctly. remind , experiment gave free Mindspark tuition random subset students. randomization failed reason? students received Mindspark tuition already better test scores students ? valid experiment. check valid experiment, confirm treated control students similar based observables. order present results, going learn new command outreg2, useful building publication-ready tables Stata.cases, outreg2 used export regression estimates different formats (hence name). However, section going learn use create summary statistics. Summary statistics incredibly important present research. open social science article, good chance first table table summary statistics.One type summary statistic table already discussed balance table. experiment, need check treatment control individuals \"balanced\" based observable characteristics. Mindspark experiment, demographic characteristics can check balance respect . understand eventual goal, Figure 4.12 (beginning) Table 1 Muralidharan, Singh Ganimian (2019). need replicate format exactly, hopefully can get something looks professional.\nFigure 4.12: Balance Table Muralidharan, Singh Ganimian (2019)\nbasic syntax outreg2 appropriate goals :Now, quite bit syntax previous code. better worse, commands deal formatting tend lots options. good flexibility, can make frustrating learn initially. go part command step--step:outreg2 using table.doc -- tells Stata save results command file called table.doc (saved working directory)outreg2 using table.doc -- tells Stata save results command file called table.doc (saved working directory)word replace -- tells Stata format word document already exists working directory replacedword replace -- tells Stata format word document already exists working directory replacedsum(log) -- tells Stata summary stats table. outreg2 usually used regression tables, summary statistics tablesum(log) -- tells Stata summary stats table. outreg2 usually used regression tables, summary statistics tableeqkeep(N Mean) -- tells Stata keep certain statistics summary stats table. case, observation count average (can also store SD, min, max)eqkeep(N Mean) -- tells Stata keep certain statistics summary stats table. case, observation count average (can also store SD, min, max)keep(varlist) -- tells Stata variables include. Replace varlist list variables want appear tablekeep(varlist) -- tells Stata variables include. Replace varlist list variables want appear tableLet's use syntax create summary statistics table data:Figure 4.13 presents basic summary statistics table. can also customize exporting adding option title(\"Table 1: Summary Statistics\"). add title table.\nFigure 4.13: Basic Summary Statistics Table\nNow, exactly wanted. wanted balance table, shows statistics variables treatment status. order create table going combine outreg2 bys command.code generates Figure 4.14:\nFigure 4.14: Basic Balance Table\nOne limitation current table treat 0 treat 1 descriptive way describe data. However, given now table Word format, can manually make necessary changes. example, Figure 4.15 edits labels provide table ready publication.\nFigure 4.15: Final Balance Table\ncase, average age, fraction female, socioeconomic status index (ses) relatively similar treated students control students. Looks like randomization succesful!","code":"outreg2 using table.doc, word replace ///\nsum(log) eqkeep(N mean) keep(varlist)outreg2 using tab1_basic.doc, word replace \\\\\\\nsum(log) eqkeep(N mean) keep(st_age1 st_female1 ses_index) bys treat: outreg2 using balance_tab.doc, word replace \\\\\\\nsum(log) eqkeep(N mean) keep(st_age1 st_female1 ses_index) \\\\\\\nlabel \\\\\\\ntitle(\"Table 2: Characteristics of Treatment and Control Students\")"},{"path":"classroom-technology.html","id":"binary-regression","chapter":"4 Classroom Technology","heading":"4.10 Binary Regression","text":"far, regression examples examples continuous variables. Life expectancy average income, example, can take value within certain ranges. Therefore, examples continuous variables. section, going discuss interpretation regression estimates either dependent (Y-variable) /independent (X-variable) binary.start, consider hypothetical example. Imagine interested whether temperature impacts voting. individual-level dataset indicates whether individual voted (\\(\\text{Vote}_i=1\\) voted zero otherwise), well temperature Fahrenheit location voted. propose modeling via linear regression framework:\\[\n\\text{Vote}_i = \\alpha + \\beta \\text{Temperature}_i + \\varepsilon_i\n\\]Imagine find \\(\\hat{\\alpha}=0.1\\) \\(\\hat{\\beta}=0.01\\). interpret numbers? said interpretation slope coefficient \"expected increase \\(Y\\) 1-unit change \\(X\\)\". use interpretation, expect variable \\(Vote\\) increase 0.01 Temperature increases degree. 0.01 increase binary indicator implies 1 percentage point increase probability value variable equal 1. seems little abstract, consider another way interpret coefficient.One way understand interpretation return thinking predicted values. predicted value \\(Vote\\) temperature 50 degrees.\\[\n\\text{Predicted Vote} = 0.1 + 0.01*50 = 0.6\n\\]Recall earlier sections often take average binary variables. example, Chapter 1, outcome variable equal 1 individual passed test zero otherwise. showed average variable equal fraction individuals passed test., mean predicted value vote equal 0.6? means temperature 50 degrees, expect 60 percent individuals vote.can similarly form predicted value vote 1 degree hotter (.e. 51 degrees).\\[\n\\text{Predicted Vote} = 0.1 + 0.01*51 = 0.61\n\\]Therefore, expect 61 percent individuals vote 51 degrees. Therefore, 1-degree increase temperature associated 1 percentage point increase fraction voting. example, dependent variable binary independent variable still continuous. next example, flip , independent variable continuous dependent variable binary.next imagine interested effect Universal Basic Income (UBI) payment total earnings individuals. UBI policy every individual receives payment every month, regardless current income. answer question important policy implications. might first seem obvious earnings increase individual receives UBI payment. also possible individuals receive UBI payment respond working less, extreme cases, quit working altogether. responses common, clear UBI payments actually increase earnings recipients overall.now experiments UBI either complete progress. experiments, individuals receive UBI payments, others . Imagine access data one experiments propose modeling earnings function whether given access UBI payment (.e. UBI binary variable equal 1 individual receives UBI payment zero otherwise):\\[\n\\text{Total Earnings}_i = \\alpha + \\beta \\cdot \\text{UBI} + \\varepsilon_i\n\\]\ninterpret regression coefficients forming predicted values. predicted earnings someone receive UBI?\\[\n\\text{Predicted Earnings} = \\alpha + \\beta \\cdot 0 =\\alpha\n\\]predicted earnings someone receive UBI?\\[\n\\text{Predicted Earnings} = \\alpha + \\beta \\cdot 1 =\\alpha + \\beta\n\\]\n\\(\\alpha\\) predicted earnings individuals receive UBI payment. words, expected earnings control workers. \\(\\beta\\) difference earnings participants receive UBI, relative . words, predict control workers earnings 50,000, UBI recipients earnings 60,000, \\(\\hat{\\alpha}=50,000\\) \\(\\hat{\\beta}=10,000\\).Finally, change prior example slightly dependent independent variable binary. Instead studying impact UBI earnings, study impact employment:\\[\n\\text{Employment}_i = \\alpha + \\beta \\cdot \\text{UBI} + \\varepsilon_i\n\\]Now \\(Y\\) \\(X\\) binary. interpret coefficients predicted values.predicted value employment someone receive UBI?\\[\n\\text{Predicted Employment} = \\alpha + \\beta \\cdot 0 =\\alpha\n\\]\nwords, receive UBI, expect \\(\\alpha\\) percent employed. Now, predicted employemnt someone receive UBI?\\[\n\\text{Predicted Employment} = \\alpha + \\beta \\cdot 1 =\\alpha + \\beta\n\\]words, receive UBI, \\(\\alpha + \\beta\\) percent employed. \\(\\beta\\) percentage point difference employment rates receive UBI vs. . example, imagine 80 percent control workers employed, 75 percent UBI recipients employed. case \\(\\hat{\\alpha}=0.80\\), \\(\\hat{\\beta}=-0.05\\). case, conclude received UBI decreases probability employed 5 percentage points.lessons chapter internalize rest book. order write clearly results, important understand variables continuous binary. \\(Y\\) variable binary, can interpret \\(\\beta\\) coefficients changes probability \\(Y=1\\). example, 1-unit increase temperature associated \\(\\beta\\) percentage point change probability voting. \\(X\\) variable binary indicator, \\(\\alpha\\) average \\(Y\\) \\(X=0\\), \\(\\alpha+\\beta\\) average \\(Y\\) \\(X=1\\).","code":""},{"path":"classroom-technology.html","id":"mindspark-results","chapter":"4 Classroom Technology","heading":"4.11 Mindspark Results","text":"Now understand interpret regressions binary variables, going estimate impact Mindspark test scores. , estimate linear regression following form:\\[\n\\text{Endline Test Score}_i = \\alpha + \\beta \\cdot \\text{Treat}_i + \\varepsilon_i\n\\]\nEndline test score ranges 0 1 represents percent questions individual got correct test. Treat binary indicator variable equal 1 individual treatment group (.e. given free Mindspark tuition) zero individual control group (.e. given free Mindspark tuition).begin, load dataset mindspark_data.dtaNow, see treatment impacts endline math scores. endline math scores. Remember, variable per_math2 contains endline math test scores. Therefore, estimate impact treatment variable, can type:found slope coefficient \\(\\hat{\\beta}=0.078\\). Remember, general formula interpreting slope coefficients 1-unit change treat expected increase per_math2 0.078. However, actually report results, make clear units represent. want reader understand per_math2 variable contains fraction correct math score order understand interpretation. Therefore, plain (interpretable) English, find students offered free Mindspark tuition saw 7.8 percentage point increase math scores relative control students offered Mindspark tuition.Concept CheckWhat average score control individuals given regression output? average score treated individuals given regression output? check answer can type sum per_math2 treat==0 sum per_math2 treat==1So overall, Mindspark reasonably large impact test scores. Test scores quite low begin , 7.8 percentage point increase sizeable gain learning. Next, see effective Mindspark Hindi:regression tells us treated students offered free Mindspark tuition saw 6.5 percentage point increase Hindi test scores relative control students. Overall, program seems effective increasing math Hindi scores.","code":"cd \"~/Dropbox/Teaching/EP5/online/04_week/data\"\nuse mindspark_data.dta, clear \n/Users/davidarnold/Dropbox/Teaching/EP5/online/04_week/datareg per_math2 treat      Source |       SS           df       MS      Number of obs   =       539\n-------------+----------------------------------   F(1, 537)       =     28.72\n       Model |  .813404311         1  .813404311   Prob > F        =    0.0000\n    Residual |    15.20668       537   .02831784   R-squared       =    0.0508\n-------------+----------------------------------   Adj R-squared   =    0.0490\n       Total |  16.0200843       538  .029777108   Root MSE        =    .16828\n\n------------------------------------------------------------------------------\n   per_math2 | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       treat |   .0777008   .0144978     5.36   0.000     .0492214    .1061802\n       _cons |   .4647551   .0101847    45.63   0.000     .4447484    .4847619\n------------------------------------------------------------------------------reg per_hindi2 treat      Source |       SS           df       MS      Number of obs   =       539\n-------------+----------------------------------   F(1, 537)       =     16.12\n       Model |  .572530953         1  .572530953   Prob > F        =    0.0001\n    Residual |   19.076815       537  .035524795   R-squared       =    0.0291\n-------------+----------------------------------   Adj R-squared   =    0.0273\n       Total |  19.6493459       538  .036522948   Root MSE        =    .18848\n\n------------------------------------------------------------------------------\n  per_hindi2 | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       treat |   .0651886   .0162382     4.01   0.000     .0332904    .0970868\n       _cons |   .5200244   .0114073    45.59   0.000     .4976159    .5424329\n------------------------------------------------------------------------------"},{"path":"classroom-technology.html","id":"regression-tables","chapter":"4 Classroom Technology","heading":"4.12 Regression Tables","text":"section going learn take results prior section translate publication-ready tables. accomplish , going use outreg2 command. remind , installed command, need first type ssc install outreg2 command window using command.goal present two regressions nice, neatly formatted table:present multiple regressions single table, take advantage est store command. est store stands estimates store. Basically allows save results regressions. show use command, estimate impact treatment math scores:Now want save estimates can typeThe part code reads reg_math giving estimates name use later. can change name depending application. example, replaced reg_math reg_math_scores code code executed without error. Next, estimate impact treatment Hindi scores store estimates reg_hindi. save, space, suppress output regression:Now regression estimates stored, can construct regression table using outreg2 command:go step codeoutreg2 [reg_math reg_hindi] -- tells Stata use outreg2 create regression table. table two columns: one displays math regression results one displays Hindi results.outreg2 [reg_math reg_hindi] -- tells Stata use outreg2 create regression table. table two columns: one displays math regression results one displays Hindi results.using reg_table.doc -- tells Stata create new file named reg_table.doc working directory contains regression table.using reg_table.doc -- tells Stata create new file named reg_table.doc working directory contains regression table., word replace -- tells Stata table Word format, file named reg_table.doc already exists, replace new table., word replace -- tells Stata table Word format, file named reg_table.doc already exists, replace new table.Now see command generates.\nFigure 4.16: Regression Table\ncertainly aesthetic changes needed table. get , discuss actual elements table. Many regression tables academic articles similar format one seen .First, clarify numbers column correspond given regression estimate. Column 1 estimates impact Mindspark math test scores, column 2 estimates impact Mindspark Hindi test scores. Generally, column titled dependent variable. Therefore, case, can see dependent variable column 1 per_math2 dependent variable column 2 per_hindi2., row, independent variables. case, regressions independent variable: treat. addition independent variables, regression estimates (generally) display constant (also known intercept) well.coefficient estimates given numbers parentheses. See Figure 4.17, highlights coefficient estimates table. confused numbers correspond , go back original regression output Stata, see estimates regressions map table.\nFigure 4.17: Regression Table: Coefficient Estimates\nNow, lot elements table explain yet. Standard errors (parentheses) p-values (indicated stars) concepts discuss course. research, generally, important understand magnitude treatment effect, also whether statistically significant. defined concept statistical significance. learn statistics econometrics courses. standard errors p-values (indicated stars) related statistical significance results.Finally, R-squared measure much variation can explain using explanatory variables. concept R-squared something learn future statistics econometrics courses well subject cover course.Now understand format table, edit table order make easily interpretable reader. First, add descriptive title. can using title() option outreg2. Second, can delete superfluous items, \"VARIABLES\", \"reg_math\" \"reg_hindi\". can directly editing table Word. Lastly, better use full phrases describe variables, variable names code. can make edits Word. making changes, finally nice, publication-ready regression table:\nFigure 4.18: Final Regression Table\n","code":"reg per_math2 treat \nreg per_hindi2 treatreg per_math2 treat\n      Source |       SS           df       MS      Number of obs   =       539\n-------------+----------------------------------   F(1, 537)       =     28.72\n       Model |  .813404311         1  .813404311   Prob > F        =    0.0000\n    Residual |    15.20668       537   .02831784   R-squared       =    0.0508\n-------------+----------------------------------   Adj R-squared   =    0.0490\n       Total |  16.0200843       538  .029777108   Root MSE        =    .16828\n\n------------------------------------------------------------------------------\n   per_math2 | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       treat |   .0777008   .0144978     5.36   0.000     .0492214    .1061802\n       _cons |   .4647551   .0101847    45.63   0.000     .4447484    .4847619\n------------------------------------------------------------------------------est store reg_mathreg per_hindi2 treat\nest store reg_hindioutreg2 [reg_math reg_hindi] using reg_table.doc, word replace"},{"path":"classroom-technology.html","id":"conditional-regressions","chapter":"4 Classroom Technology","heading":"4.13 Conditional Regressions","text":"far found Mindspark effective increasing test scores math Hindi average. maybe effective students relative others? section, going explore treatment effect heterogeneity. Often important understand treatment effect heterogeneity trying understand treatment work settings. example, find different results implemented -girls -boys school? find different results implemented wealthier neighborhoods? less useful treatment areas achievement already high begin ? can use existing experiment see treatment effect varies different characteristics, gender, socioeconomic status, baseline achievement.estimate treatment effect heterogeneity? Well, want estimate treatment effect females , example, restrict sample females. , among smaller sample participants, can estimate treatment effect.easiest way implement idea Stata use conditional regressions. case, word conditional implies individuals meet certain condition included estimation.example, want estimate treatment effect linear regression framework, restricted females, can type:part code st_female1==1 implies regression reg per_math2 treat estimated females. result, regression table, can see Number obs equal 414. number females experiment endline test scores. comparision, estimate treatment effect, restricted males:Now, first regression, find \\(\\hat{\\beta}^{female}\\) = 0.071. implies treatment increases percent score correct 7.1 percentage points females. second regression, find \\(\\hat{\\beta}^{male}\\) = 0.102. Treatment increases percent score correct 10.2 percentage points males. Therefore, overall, treatment effect slightly larger males relative females. However, cases, appears overall, treatment quite effective.Next, consider treatment effects vary socieconomic status. data, variable named ses_index. index variable aggregates information many different variables. example, setting, SES index captures different aspects household wealth. constructed variables capture ownership various consumer goods services. higher SES index indicates higher wealth. SES index scaled average equal zero sample. Therefore, individuals positive values SES index wealthier average student experiment, individuals negative values SES index poorer average student experiment. test whether treatment effect different students -average socioeconomic status vs. -average socieconomic status.first regression estimated individuals -average SES index.second regression estimated individuals -average SES index.regression 1, find \\(\\hat{\\beta}^{lowSES}\\) = 0.087. Treatment increases percent score correct 8.7 percentage points individuals -average SES. regression 2, find \\(\\hat{\\beta}^{highSES}\\) = 0.075. Treatment increases percent score correct 7.5 percentage points individuals -average SES. Therefore, treatment effect slightly larger low-SES, quite similar overall.last heterogeneity explore baseline achievement. Maybe students benefit \"left behind\". many ways one explore question. going see individuals median level baseline test scores (within grade) different treatment effects median level test scores.first thing need create new variable indicates whether individual student performed median median initial baseline assessment, within grade level. two steps:Step 1: create variable stores median test score within individual's grade.Step 1: create variable stores median test score within individual's grade.Step 2: create indicator variable equal 1 individual's test score median test score zero otherwise.Step 2: create indicator variable equal 1 individual's test score median test score zero otherwise.Step 1: going combine bys egen. used commands compute total number stops race light condition previous chapter. generate median test score within grade, can type:Now generate new variable indicates whether given student's test score equal median test score within gradeNow indicator variable, can now estimate two regressions: (1) regression restricted -median performers (2) regression restricted -median performers.find \\(\\hat{\\beta}^{}\\) = 0.095. Treatment increases percent score correct 9.5 percentage points -median scoring students. find \\(\\hat{\\beta}^{}\\) = 0.088. Treatment increases percent score correct 8.8 percentage points -median scoring students. treatment effect slightly larger -median scoring students, quite similar overall.summarize results chapter, tested heterogeneity treatment effects (1) gender, (2) SES status (3) baseline achievement. found evidence heterogeneity, impacts large positive groups. provides evidence external validity treatment. expand program students, perhaps different baseline characteristics, might predict still effective given ability raise test scores different types students.","code":"reg per_math2 treat if st_female1==1      Source |       SS           df       MS      Number of obs   =       414\n-------------+----------------------------------   F(1, 412)       =     17.78\n       Model |  .518287614         1  .518287614   Prob > F        =    0.0000\n    Residual |  12.0091958       412  .029148533   R-squared       =    0.0414\n-------------+----------------------------------   Adj R-squared   =    0.0390\n       Total |  12.5274834       413   .03033289   Root MSE        =    .17073\n\n------------------------------------------------------------------------------\n   per_math2 | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       treat |   .0707653    .016782     4.22   0.000     .0377763    .1037543\n       _cons |   .4636151    .011838    39.16   0.000     .4403447    .4868854\n------------------------------------------------------------------------------reg per_math2 treat if st_female1==0      Source |       SS           df       MS      Number of obs   =       125\n-------------+----------------------------------   F(1, 123)       =     12.67\n       Model |  .323197879         1  .323197879   Prob > F        =    0.0005\n    Residual |  3.13679016       123  .025502359   R-squared       =    0.0934\n-------------+----------------------------------   Adj R-squared   =    0.0860\n       Total |  3.45998804       124  .027903129   Root MSE        =    .15969\n\n------------------------------------------------------------------------------\n   per_math2 | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       treat |   .1017787   .0285899     3.56   0.001     .0451867    .1583707\n       _cons |   .4684034   .0198077    23.65   0.000     .4291953    .5076114\n------------------------------------------------------------------------------reg per_math2 treat if ses_index<0      Source |       SS           df       MS      Number of obs   =       268\n-------------+----------------------------------   F(1, 266)       =     20.29\n       Model |  .509060251         1  .509060251   Prob > F        =    0.0000\n    Residual |  6.67313366       266  .025086969   R-squared       =    0.0709\n-------------+----------------------------------   Adj R-squared   =    0.0674\n       Total |  7.18219391       267  .026899603   Root MSE        =    .15839\n\n------------------------------------------------------------------------------\n   per_math2 | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       treat |   .0872535   .0193697     4.50   0.000     .0491161    .1253909\n       _cons |   .4296612   .0139997    30.69   0.000     .4020969    .4572256\n------------------------------------------------------------------------------reg per_math2 treat if ses_index>0      Source |       SS           df       MS      Number of obs   =       271\n-------------+----------------------------------   F(1, 269)       =     12.72\n       Model |  .380236782         1  .380236782   Prob > F        =    0.0004\n    Residual |  8.04393779       269  .029903114   R-squared       =    0.0451\n-------------+----------------------------------   Adj R-squared   =    0.0416\n       Total |  8.42417457       270  .031200647   Root MSE        =    .17293\n\n------------------------------------------------------------------------------\n   per_math2 | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       treat |   .0751004   .0210607     3.57   0.000     .0336356    .1165653\n       _cons |   .4957346   .0143607    34.52   0.000      .467461    .5240081\n------------------------------------------------------------------------------bys st_grade1: egen median_test_score=median(per_math1)gen above_median = (per_math1 > median_test_score)reg per_math2 treat if above_median==1      Source |       SS           df       MS      Number of obs   =       239\n-------------+----------------------------------   F(1, 237)       =     19.25\n       Model |   .52552737         1   .52552737   Prob > F        =    0.0000\n    Residual |   6.4715909       237  .027306291   R-squared       =    0.0751\n-------------+----------------------------------   Adj R-squared   =    0.0712\n       Total |  6.99711827       238  .029399657   Root MSE        =    .16525\n\n------------------------------------------------------------------------------\n   per_math2 | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       treat |   .0941481   .0214608     4.39   0.000     .0518699    .1364264\n       _cons |   .5290498    .014493    36.50   0.000     .5004981    .5576014\n------------------------------------------------------------------------------reg per_math2 treat if above_median==0      Source |       SS           df       MS      Number of obs   =       300\n-------------+----------------------------------   F(1, 298)       =     21.99\n       Model |  .480080178         1  .480080178   Prob > F        =    0.0000\n    Residual |  6.50520836       298  .021829558   R-squared       =    0.0687\n-------------+----------------------------------   Adj R-squared   =    0.0656\n       Total |  6.98528854       299  .023362169   Root MSE        =    .14775\n\n------------------------------------------------------------------------------\n   per_math2 | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       treat |   .0800939   .0170791     4.69   0.000      .046483    .1137049\n       _cons |   .4063055   .0123553    32.89   0.000     .3819907    .4306202\n------------------------------------------------------------------------------"},{"path":"classroom-technology.html","id":"conclusion-2","chapter":"4 Classroom Technology","heading":"4.14 Conclusion","text":"final section summarize learned far present additional results Muralidharan, Singh, Ganimian (2019). motivation Muralidharan, Singh, Ganimian (2019) certain settings, may mismatch students' level understanding curriculum taught. One potential tool can used alleviate problem technology. Muralidharan, Singh, Ganimian (2019), authors test Mindspark, software aims teach---right level successful increasing test scores. chapter, used data experiment found large test score gains.Whenever find experimental result, might want think carefully driving result. example, treatment setting really combination things.Mindspark software aims teach---right levelThe Mindspark software aims teach---right levelThis software combined instruction time teaching assistantThis software combined instruction time teaching assistantAlthough focus second component, every Mindspark center includes instructional time teaching assistant. maybe actually need \"technology\" component. Maybe just need instruction time?Well, Berry Mukherjee (2016)7 studies impact private tutoring Delhi. similar intervention, without software teach---right level. Instead teaching focused current grade level student (assessed level). Berry Mukherjee (2016) find impact intervention. suggests individual targeting aspect software essential results.key difficulty Mindspark software attempts overcome huge heterogeneity understanding classroom. instructor, teaching effectively learning levels vary widely difficult. Mindspark programmed, can automatically. can actually look data see Mindspark updates material time different students.\nFigure 4.19: Dynamic Updating Mindspark\nPanel Figure 4.19 plots grade level questions proposed Mindspark time, different grade levels. can see, individuals (average) starting well-grade level. example, solid black line grade 9, questions posed students grades 4 5. time, assessed level increases. 5 months later, difficulty questions increased almost entire year. figure plotting individuals learning time.Panel B, instead, plots grade level questions proposed Mindspark time different levels baseline achievement. key takeaway graph regardless baseline achievement, Mindspark succesful improving updating time. rates learning actually quite similar different levels baseline achievement.now relatively certain Mindspark succesful increasing learning, imply invest resource students? Whenever proposed policy, bound cost-benefit analysis. Policymakers generally number different potential policies may pursue, want invest right policy. order , need cost benefits proposed policy. Running experiment great way estimate benefits proposed intervention. Now know benefits Mindspark experiment, time turn costs., compare cost Mindspark traditional government-run schools. Government-run schools spend 240 minutes per week Math Hindi. cost per pupil around INR 1500 (US $22). Mindspark spends 180 minutes per week Math Hindi. Cost per pupil around INR 1000 (US $15). Using estimates paper, Mindspark led learning less time government-run schools, well utilizing less financial resources.conclude, using technology teach---right-level seems like promising intervention places tremendous heterogeneity baselines levels understanding. , potential even better educational design! Maybe best intervention blend Mindspark standard teaching. Giving teachers information level students aid instruction classroom. automated aspect software free teacher time activities also improve student performance.","code":""},{"path":"classroom-technology.html","id":"animated-concepts","chapter":"4 Classroom Technology","heading":"Animated Concepts","text":"","code":""},{"path":"legacy-of-colonial-medicine.html","id":"legacy-of-colonial-medicine","chapter":"5 Legacy of Colonial Medicine","heading":"5 Legacy of Colonial Medicine","text":"","code":""},{"path":"legacy-of-colonial-medicine.html","id":"colonial-medicine-intro","chapter":"5 Legacy of Colonial Medicine","heading":"5.1 Colonial Medicine Intro","text":"1921 1956 French colonial governments Africa organized medical campaigns treat prevent sleeping sickness. Sleeping sickness lethal parasitic disease transmitted bite tsetse fly occurs regularly parts sub-Saharan Africa. colonial era, French colonies military organized campaigns exclusively focused treating sleeping sickness.However, treatments time (atoxyl Lomidine) questionable efficacy \nsevere side effects. Historians anthropologists linked sleeping sickness campaigns mistrust modern medicine.section, use data Lowes Montero (2021) study colonial medical campaigns undertaken generations ago impact health behaviors today. , use measures exposure medical campaigns constructed Lowes Montero (2021) newly digitized military records France.\nFigure 5.1: Sleeping Sickness Visits 1921-1956\nFigure 5.1 shows variation sleeping sickness across areas Cameroon French Equatorial Africa (present-day Central African Republic, Chad, Republic Congo, Gabon). can seen figure, large variation across areas. places (blue), medical campaign visits related sleeping sickness period. areas (red), visited many 24 times years 1921-1956. study variation visit rates correlates measures trust medicine today.Lowes Montero (2021), authors study two key outcomes, come Demographic Health Surveys (DHS). first vaccination index captures fraction possible vaccines children received. second studies whether individuals refuse free, non-invasive blood test. Consent blood test high samples, blood test taken finger prick free individuals survey. Refusal blood test might signal distrust medicine generally.begin, load dataset colonial_medicine.dtaAs usual, describe data learn variables.\nFigure 5.2: Describing Colonial Medicine Data Set\n75,881 survey participants, demographics individual, key outcome variable (refused_any_blood_test), well key explanatory variable (Times_Prospected). understand key variables, first summarize outcome variable.data individual level includes basic demographics (age, sex, etc.) well key outcome: refused_any_blood_testSo around 11 percent overall refuse blood test. goal understand refusal rate depends often individual's area visited sleeping sickness campaigns past. variable captures Times_Prospected:value 0.2 indicates average, individuals location visited 20 percent years 1921-1956 (35 years total). simpler terms, value 0.2 indicates 7 years, 7 20 percent 35.now data answer initial question: people live places historical medical campaigns prevalent likely refuse blood tests today? first, highlight new aspects data need understand.First, browse data, see variable wealth_index looks like string variable, yet highlighted blue, red\nFigure 5.3: Value Labels Wealth Index\ntake average wealth_index, get number outWe discuss behavior future chapter value labels. Next, browse data, see variable refused_any_blood_test missing values (period indicates value missing).\nFigure 5.4: Missing Values\nimportant careful missing values generally, particularly Stata, discuss future chapter well. get answering main question, first need handle data issues.","code":"cd \"/Users/davidarnold/Dropbox/Teaching/EP5/online/05_week/data\"\nuse colonial_medicine.dta, replace\n/Users/davidarnold/Dropbox/Teaching/EP5/online/05_week/datasum refused_any_blood_test    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nrefused_an~t |     11,993    .1110648    .3142255          0          1sum Times_Prospected    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nTimes_Pros~d |     67,036    .2941624    .1943945          0         .8sum wealth_index    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nwealth_index |     75,881    3.212319     1.43691          1          5"},{"path":"legacy-of-colonial-medicine.html","id":"value-labels","chapter":"5 Legacy of Colonial Medicine","heading":"5.2 Value Labels","text":"value label associates description particular numeric value variable. Sometimes convenient store variables numeric, example, like apply mathematical operations. However, might hard remember value data represents. Value labels allow store information numeric, still able quickly reference number represents.order discuss value labels, going look American National Election Survey. American National Election Survey (ANES) survey elections U.S. large range questions come survey. make easier users dataset, many variables value labels attached.going load small subset dataset information just questions:browse data, can see variables value labels.\nFigure 5.5: Browsing ANES\nvote better_or_worse values look like characters, highlighted blue instead red. going ? look closely, can see actual value associated given cell. example, Figure 5.6, clicked first cell table. can see highlighted value equal 2. value table.\nFigure 5.6: Browsing ANES\nvalue label text highlighted blue (\"2. , vote\"). use tab can see different values labels given variable. try vote:4 different value labels. need references values variables, need use actual numeric values, labels . labels just forget numbers mean. example, wanted see many individuals refused question, can type:try thing, referencing value label, get error:states type mismatch vote numeric variable, string variable. quick way see numeric values associated labels type:","code":"cd /Users/davidarnold/Dropbox/Teaching/EP5/online/05_week/data\nuse anes_subset.dta, cleartab vote \n      PRE: Did R vote for |\n        President in 2012 |      Freq.     Percent        Cum.\n--------------------------+-----------------------------------\n              -9. Refused |          2        0.05        0.05\n-8. Don't know (FTF only) |         14        0.33        0.37\n            1. Yes, voted |      3,117       73.00       73.37\n       2. No, didn't vote |      1,137       26.63      100.00\n--------------------------+-----------------------------------\n                    Total |      4,270      100.00count if vote==-9\n  2count if vote == \"-9. Refused\"\ntype mismatch\nr(109);\n\nend of do-file\nr(109);tab vote, sum(vote)\n PRE: Did R |\n   vote for |   Summary of PRE: Did R vote for\n  President |          President in 2012\n    in 2012 |        Mean   Std. dev.       Freq.\n------------+------------------------------------\n  -9. Refus |          -9           0           2\n  -8. Don't |          -8           0          14\n  1. Yes, v |           1           0       3,117\n  2. No, di |           2           0       1,137\n------------+------------------------------------\n      Total |   1.2320843   .72453319       4,270"},{"path":"legacy-of-colonial-medicine.html","id":"missing-values","chapter":"5 Legacy of Colonial Medicine","heading":"5.3 Missing Values","text":"Continuing exploration ANES dataset, now discuss missing values. numeric variables, Stata stores missing values period. numeric variable code missing information period, change continuing analysis. see , consider variable better_or_worse, asks respondent scale 1 5 much worse relative year ago?can see -8 -9 actually missing values, right now stored numbers. take average better_or_worse, get:average incorporates values -9 -8. terms scale variable, lower overall average, lead pretty misleading results. example, imagine instead storing missing values -9, stored -1000. , even though individuals missing data, average value better_or_worse might low, low 1. naively took average, might conclude average, people report much better . wrong conclusion, need average response among individuals actually answered question.need replace individuals either value -8 -9 missing. can make use operator, given symbol |. code replace value better_or_worse missing current value either -8 -9.Now take average, missing values incorporatedWhen compute summary statistics run regressions, observations missing values dropped automatically calculation. However, need careful constructing new variables observations missing values. logical statements Stata (creating indicator variables), missing value interpreted infinity. reason chosen missing values interpreted infinity little complicated, can lead confusing errors!see , Imagine want create indicator equal 1 individual thinks life gotten worse relative 1 year ago. individual responded 4 5 believes life gotten worse relative 1 year ago. one potential way code new variable type:see worse_off equal individuals missing values better_or_worse.equal 1 individuals, ? reason missing values interpreted infinity logical statements. infinity greater 4, everyone missing values coded worse_off=1.avoid problem, need change code takes account possibility missing values. example, might want replace worse_off equal missing better_or_worse equal missing.","code":"tab better_or_worse   PRE: R how much better |\nworse off than 1 year ago |      Freq.     Percent        Cum.\n--------------------------+-----------------------------------\n              -9. Refused |         11        0.26        0.26\n-8. Don't know (FTF only) |          2        0.05        0.30\n       1. Much better off |        298        6.98        7.28\n   2. Somewhat better off |        904       21.17       28.45\n        3. About the same |      1,981       46.39       74.85\n    4. Somewhat worse off |        763       17.87       92.72\n        5. Much worse off |        311        7.28      100.00\n--------------------------+-----------------------------------\n                    Total |      4,270      100.00sum better_or_worse    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nbetter_or_~e |      4,270    2.937002    1.176809         -9          5replace better_or_worse=. if better_or_worse==-8 | better_or_worse==-9sum better_or_worse    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nbetter_or_~e |      4,257    2.972986    .9814891          1          5gen worse_off = (better_or_worse>=4)sum worse_off if better_or_worse==.    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n   worse_off |         13           1           0          1          1replace worse_off = . if better_or_worse==."},{"path":"legacy-of-colonial-medicine.html","id":"binned-scatterplots-theory","chapter":"5 Legacy of Colonial Medicine","heading":"5.4 Binned Scatterplots (Theory)","text":"chapter discuss new data visualization technique known binned scatterplots. understand technique, first take simple example. Imagine, dataset Figure 5.7 shows Earnings (vertical axis) test scores (horizontal axis):\nFigure 5.7: Relationship Earnings Test Scores\nexample, plotted data can clearly see positive relationship test scores earnings. Individuals higher test scores tend also higher earnings. 1,000 people dataset, 10,000, million? graph soon get pretty crowded.graph million observations difficult interpret, also likely crash computer!However, instead plotting data, imagine bin data intervals. example, Figure 5.8 binned data intervals dividing x-axis three bins. bin 1 observations scores 80. Bin 2 observations scores 80 90. Bin 3 scores 90.\nFigure 5.8: Binning Data\nNow instead plotting data, just plot averages within bin, depicted Figure 5.9\nFigure 5.9: Example Binned Scatterplot\nbinned scatterplot allows us transparently observe relationship two variables. However, now imagine 1000 students instead original 12. Well, binned scatter plot still 3 points. Note decision bin data three intervals merely illustrative purposes. general, can decide many bins create. key even sample size grows, can still show relationship two variables binned scatterplot. Even sample size grows million, number points graph still determined number bins.","code":""},{"path":"legacy-of-colonial-medicine.html","id":"binned-scatterplots-stata","chapter":"5 Legacy of Colonial Medicine","heading":"5.5 Binned Scatterplots (Stata)","text":"section use data Lowes Montero (2021) implement binned scatterplot study legacy colonial medicine Africa. order construct binned scatterplot make use binscatter command.reminder, 1921-1956, French colonies Africa visited many medical campaigns often forcing medical care questionable efficacy severe side effects. places visited small fraction years. places visited often years. people lived places many medical campaigns likely distrust medicine today?begin load dataset:First, need drop values either missing information refused_any_blood_test Times_Prospected. refused_any_blood_test key outcome equal 1 individual refused free blood test zero otherwise. Times_Prospected key explanatory variables captures fraction years 1921-1956 individual's region visited sleeping sickness campaigns. either values missing observation, included analysis.parallel line | indicates \"\". put simply, line says, drop observations either refused_any_blood_test Times_Prospected missing.Next, use external command named binscatter can used produced binned scatterplots. install version Stata, first need type.execute code , able use binscatter command. basic syntax binscatter command :syntax, replace yvar whatever variable want plot vertical axis xvar whatever variable want plot horizontal axis. replace # however many bins want create.example, want understand relationship refusing take blood test (y-axis) share years visited medical campaigns 1921-1956 (x-axis). illustrative purposes, specify nquantiles(10) implies 10 bins scatterplot. One nice thing binscatter command can utilize everything learned creating graphs generally. words, make sure graph appropriate titles axes labels.\nFigure 5.10: Binscatter: Refusing Blood Test vs. Share Years Visited\ncan see graph, share years visited increases, fraction individuals refuse blood test also increases. provides supporting evidence initial hypothesis: areas history colonial medicine still distrust medicine today. line depicted binned scatterplot regression line shows strength relationship.also helpful interpretation retrieve slope regression line. can get parameter specifying reportreg option binscatter command.\nFigure 5.11: Regression Line: Refusing Blood Test vs. Share Years Visited\nConcept CheckFrom regression line, can conclude increase 7 years medical campaign visits associated 3.8 percentage point increase rate blood test refusal. understand (1) scale variables (2) slope coefficient, able convince true.Hint: Remember 1-unit change X-variable associated \\(\\beta\\)-unit change Y-variable. hardest thing problem understanding units X Y variables example.","code":"cd \"/Users/davidarnold/Dropbox/Teaching/EP5/online/05_week/data\"\nuse colonial_medicine.dta, replace\n/Users/davidarnold/Dropbox/Teaching/EP5/online/05_week/datadrop if refused_any_blood_test==. | Times_Prospected==. \n(65,951 observations deleted)ssc install binscatterbinscatter yvar xvar, nquantiles(#)binscatter refused_any_blood_test Times_Prospected, nq(10)  ///\n    title(\"Binscatter: Refusing Blood Test vs. Share of Years Visited\") ///\n    xtitle(\"Share of years visited, 1921-1956\") ///\n    ytitle(\"Refused any blood test\")binscatter refused_any_blood_test Times_Prospected, nq(10) reportreg "},{"path":"legacy-of-colonial-medicine.html","id":"conclusion-3","chapter":"5 Legacy of Colonial Medicine","heading":"5.6 Conclusion","text":"final section summarize learned far present additional results Lowes Montero (2021). goal Lowes Montero (2021) understand medical campaigns Africa 1921-1956 impact perceptions medicine today. Colonial medical campaigns undertaken prevent sleeping sickness often forced medical treatment. treatments questionable efficacy came severe side effects. Lowes Montero (2021) utilize newly digitized French military records order measure exposure region medical campaigns past, combine information health surveys understand impact past campaigns health behaviors today.explored exposure campaigns impacted choice refuse blood test. blood test free survey participants, overall high takeup, individuals rejected blood test. one proxy trust modern medicine. found prior section areas exposure medical campaigns likely refuse blood test.One thing may notice read Lowes Montero (2021), however, results appear exactly Lowes Montero (2021). One reason Lowes Montero (2021) also control potential factors correlated historical exposure medical campaigns well health behaviors today. Maybe places exposure colonial medical campaigns vary along number dimensions, dimensions explain correlation found. Turns , can estimate binned scatterplots control factors. goes beyond course, Lowes Montero (2021) authors control factors impact health decisions, age, gender, urban-rural status, among others. Figure 5.12 displays binned scatterplot constructed also controlling factors.\nFigure 5.12: Binscatter: Refusing Blood Test vs. Share Years Visited\nLowes Montero (2021) also study colonial medical campaigns impact vaccination rates today. , construct vaccination index, share completed vaccines (9) children 5. , find places years colonial medical visits lower vaccination rates, shown Figure 5.13.\nFigure 5.13: Binscatter: Vaccination vs. Share Years Visited\npaper finds negative experiences past transmit across generations. finding important implications trust institutions generally. also real policy implications. Lowes Montero (2021) find World Bank projects related health less successful areas high exposure colonial medicine.","code":""},{"path":"resume-experiments.html","id":"resume-experiments","chapter":"6 Resume Experiments","heading":"6 Resume Experiments","text":"","code":""},{"path":"resume-experiments.html","id":"correspondence-studies","chapter":"6 Resume Experiments","heading":"6.1 Correspondence Studies","text":"motivation week's empirical application pervasive evidence inequality labor market. example, Black workers twice likely white workers unemployed (Council Economic Advisors, 1998). However, often difficult isolate discrimination employers source given disparity. example, argue researchers factors go employer's decision hire worker, therefore differences hiring rates across different races due factors unobserved.week studying extremely clever method identifying discrimination labor market: resume audit studies, also known correspondence studies. paper studying Bertrand Mullainathan (2004)8 send fictitious resumes help-wanted ads Boston Chicago. unique angle manipulate perception race altering name resume:Examples white-sounding names paper: Emily Walsh Greg BakerExamples white-sounding names paper: Emily Walsh Greg BakerExamples Black-sounding names paper: Lakisha Washington Jamal JonesExamples Black-sounding names paper: Lakisha Washington Jamal JonesAll characteristics applicant (experience, education, job history) randomized. compare callback rates (interview) Black white applicants.design clever? Well, factors relevant job held fixed construction. Disparities callback rates races can longer \"explained away\" factors. researchers generated resumes. factors observed similar white Black applicants construction. Therefore, find disparity now, strong evidence discrimination labor market.getting data though, need learn use program studying rest book: R. point, also like credit Kosuke Imai's Textbook: Quantitative Social Science: Introduction, example drawn .9","code":""},{"path":"resume-experiments.html","id":"installing-rrstudio","chapter":"6 Resume Experiments","heading":"6.2 Installing R/RStudio","text":"course using R Rstudio. R Rstudio work together perform data analysis. two programs instead one?R software actually work. software executes functions gives results. Rstudio software provides nice interface use R. basically, R actual machinery, Rstudio improve user experience. strictly need Rstudio use R, helps lot! Whenever \"open R\" course, actually opening RStudio.RStudio work without R, first thing need download R. can go https://cran.r-project.org/ download recent version software. R available PC Mac users. download package can install following instructions installer. One part instructions sometimes trips students need decide location CRAN mirror. general advice choose CRAN mirror closest physical proximity . potentially improve download speed balance resources across different servers. practice, really worry decision. CRAN mirror allow download identical version R.","code":""},{"path":"resume-experiments.html","id":"rstudio-interface","chapter":"6 Resume Experiments","heading":"6.3 RStudio Interface","text":"open RStudio, greeted RStudio Interface depicted Figure 6.1\nFigure 6.1: R Studio Interfacte\ndiscuss 4 windows turn. top left Editor Window. write R script holds code. may see window initially yet opened R script (equivalent -file Stata). open new R script can go File > New File > R Script.Editor Window Console. code executed run. example, write code take average variable Editor Window, execute code, result displayed Console.top right Environment, going store objects, variables, datasets. get object chapter.Lastly, bottom right window host tabs helpful. One use frequently Plots tab. figures create R displayed. Another important tab Packages tab. chapter discuss importance packages R. Lastly, also use Help tab frequently. tab can search documentation use certain functions R. similar help files often used Stata chapters.now understand different components interface, going get started coding R. first thing open new R script. discussed , , can navigate File > New File > R Script.Now, R script, type:line code computes sum 5 3. order execute , options. first way highlight code, press Run button top right Editor window.\nFigure 6.2: Run Button\nclicking Run, code sent Console executed:way execute code using shortcuts. Mac, click line code, press Command + Enter, line code sent console. PC, Ctrl + Enter. want run multiple lines, can simply highlight lines want run, press either Command + Enter Ctrl + Enter R execute lines code sequential order.Just like Stata, can multiplication using *, division using / exponents using ^. R, order operations controlled parentheses:can also utilize complicated mathematical functions, like log function, square root, absolute value:ever unsure function , can utilize help files. example, log function, may sure base . bring help file log function can type:Reading help file inform us default, base log function equal \\(e\\), implies default take natural logarithm. Additionally, help file tells compute logarithms different bases, base 10.Stata, helpful document code. empirical projects, want able go back code later date quickly understand code . Comments essential . , comments lines within script meant run. describe code .line # beginning commented . comment descriptions code, R try execute , leading errors output. , example, imagine want calculate hypotenuse particular triangle, sides length 5 4. code , also documenting code .Now imagine forgot comment first line description. see happenIt reports error Calculate executable code R. placing # beginning line, however, R ignore line try execute .","code":"\n5+35+3\n[1] 8(5+3)/3\n[1] 2.666667\n5 + 3/2\n[1] 6.5log(5)\n[1] 1.609438\nsqrt(2)\n[1] 1.414214\nabs(-1)\n[1] 1\n?log#Calculate the hypotenuse of a triangle\n#One side = 5\n#The other side = 4\n#The hypotenuse is:\nsqrt(5^2 + 4^2)\n[1] 6.403124Calculate the hypotenuse of a triangle\n#One side = 5\n#The other side = 4\n#The hypotenuse is:\nsqrt(5^2 + 4^2)\nError: <text>:1:11: unexpected symbol\n1: Calculate the\n              ^"},{"path":"resume-experiments.html","id":"objects","chapter":"6 Resume Experiments","heading":"6.4 Objects","text":"Objects R pieces information:number, called ``numeric'' R (e.g. \\(5.23\\))text string, called ``character'' R (e.g. \"UCSD awesome\")dataset (e.g. look resume dataset soon)Many !R can store objects name choice. order create object, can use assignment operator <-. example, want create object named graduationdate, equal 2010, typeThis add Environment (top right window) new object called graduation date, equal 2010. Instead assignment operator <-, can also use =. generally suggested use assignment operator R creating objects, equals sign also work, /Now object stored memory, every time type word graduationdate, R interprets value 2010. means can use object calculations. example, calculate number years 2022 graduation date, can type:assign new value object name, overwrite object (careful ).R can also represent types values objects, strings characters:can use class function retrieve type objectSo far, every object created single value associated . Next, discuss vectors. vector represents collection information type stored specific order. example, instead graduation date single individual, maybe information graduation date four individuals. collection information constitutes vector. can create vector R using function c(), stands \"concatenate\". example, create vector fo graduation dates:can perform calculations vector well. example, student, compute number years graduation year 2022.graduationdate numeric vector, element number. can also create character vectors, element word. example, imagine next want create vector contains information school four individual students. type:Stata, whenever reference character values (also known string values), put values quotation marks. lets R know information composed characters, numbers.One important note vector hold information different types. words, vector holds numeric character values. example, imagine try combine two vectors using c() function:Now may look like succeeded, look carefully first entries. entry surrounded quotation marks. R changed numeric entries character entries.Lastly, one helpful note can use arithmetic operations vectors. example, imagine vector individual incomes vector planned raises individual. get total income raise, can add two vectors together.terms data analysis, can really think vectors variable. first variable graduation date, second variable school. Often, data analysis, want compute summary statistics variables. R, host functions helpful summarizing numeric vectors:length(): length vector (number elements)min(): minimum valuemax(): maximum valuerange(): range datamean(): mean valuesum(): total sum elementsLet's try can get good sense functions :Notice many functions require numeric input. example, make sense take average school:However, make sense ask, many elemnents vector school?","code":"\ngraduationdate <- 2010\ngraduationdate = 20102022-graduationdate\n[1] 12graduationdate <- 2014\ngraduationdate\n[1] 2014\nschool <- \"UCSD\"class(graduationdate)\n[1] \"numeric\"\nclass(school)\n[1] \"character\"#Create a vector of graduation dates\ngraduationdate <- c(2010, 2009, 2015, 2001)\ngraduationdate\n[1] 2010 2009 2015 2001#Calculate years since graduation\n2022 - graduationdate\n[1] 12 13  7 21\n#Create a vector of schools\nschool <- c(\"UCSD\", \"UCB\", \"UCLA\", \"UCR\")c(graduationdate, school)\n[1] \"2010\" \"2009\" \"2015\" \"2001\" \"UCSD\" \"UCB\"  \"UCLA\" \"UCR\" #Create a vector of income\nincome <- c(65, 70, 20, 100)\n\n#Create a vector of raises\nraise <- c(5, 10, 3, 20)\n\n#Calculate current income for each person\nincome + raise\n[1]  70  80  23 120#Maximum income\nmax(income)\n[1] 100#Minimum graduate date\nmin(graduationdate)\n[1] 2001#Length of the vector income\nlength(income)\n[1] 4#Range of graduation dates\nrange(graduationdate)\n[1] 2001 2015#Mean years since graduation\nmean(2022 - graduationdate)\n[1] 13.25#Sum of income\nsum(income)\n[1] 255mean(school)\nWarning in mean.default(school): argument is not numeric or\nlogical: returning NA\n[1] NAlength(school)\n[1] 4"},{"path":"resume-experiments.html","id":"working-directory-1","chapter":"6 Resume Experiments","heading":"6.5 Working Directory","text":"section, going discuss load data. First, introduce set working directory R. , load datafile named resume.csv. remind , dataset fictitious resumes sent Bertrand Mullainathan (2004) order identify labor-market discrimination. going study callback rates applicants white-sounding names applicants Black-sounding names. find differences callback rates, must driven race, factors resumes designed similar researchers.remind Stata chapters, working directory \"address\" computer R looks data. also address outputs results, datasets, figures tables. order work data R, need good understanding working directory. retrieve current working directory R, can type:set working directory, use setwd() function. set working directory wherever data located. chapter, want set directory wherever placed data file resume.csv. example, computer, type:folder placed data. uncertain get address R, can also find Session tab top screen. click Session, one options Set Working Directory. can navigate Choose Directory. allow navigate folders usually . Click folder stored data press Open. change working directory folder. Even better, code used change directory printed console. can just copy paste snippet code R script next time open script can just run line code instead manually setting directory.","code":"getwd()\n[1] \"/Users/davidarnold/Dropbox/Teaching/EP5Bookdown\"\nsetwd(\"/Users/davidarnold/Dropbox/Teaching/EP5Bookdown/data\") "},{"path":"resume-experiments.html","id":"dataframes","chapter":"6 Resume Experiments","heading":"6.6 Dataframes","text":"Now set working directory, can actually load data start exploring . , use read.csv() command. command used load comma separated values files.look environment now, dataset 4870 observations (obs.) 5 variables. click blue arrow next data frame, variables (first values), revealed, Figure 6.3\nFigure 6.3: Loading Data Frame\nclick name data frame, RStudio open spreadsheet data. similar use browse command using Stata. allows look raw data understand various variables values may take.also ways summarize information data frame. example ncol(), retrieves number columns. nrow() retrieves number rows example, 5 variables, 5 columns. 4,870 observations, 4,780 rows. One thing can quickly get sense data type summary(resume). provides simple summary statistics variable data frame.numeric variables, command provides min, max, 25th percentile (called 1st Quartile), median, mean, 75th percentile (called 3rd Quantile) max. character variables, R just reports length (.e. number observations). dataset, variable X just variable records observation number, goes frmo 1 4870. firstname name application. sex either female male. race either white Black. call main outcome interest. equal 1 fictitious application received callback interview. equal 0 fictitious application receive callback.Although already shown browse raw data, sometimes inconvenient look entire data frame, particularly large dataset. However, often helpful get small snippet data. , can use head() function. shows first six observations data frame:Similarly, can use tail() function show last six observations data frame.Next, talk big distinction R Stata. Stata, always one data set. makes easy reference variables: just type variable name. example, wanted take mean variable call Stata, typed sum call.R, however, can load multiple data frames time. actually really nice feature R, especially trying merge data frames together. means, however, whenever reference variable, need reference variable name, well data frame comes . example, reference variable call within data frame resume, type resume$call.example, wanted look first six observations, call, type:common mistake beginners forget specify data frame variable. can lead confusing errors, always make sure specifying data frame variable R.Now data loaded understand reference variables, begin explore data. First, get sense callback rates fictitious applicants. Remember, variable call equal 1 resume received callback interview. Therefore, take average variable, value equal fraction individuals received callback. follow logic, make sure flip back Chapter 1.5 review true.already know take average vector, use mean() function. column data frame just vector. can use function find callback rate:around 8 percent applicants received callback interview. Low callback rates pretty common settings. Next, look main variable interest. get distribution race can use table() function, prints table frequencies:equal numbers white Black applicants. design. researchers particularly interested understanding racial disparities callback rates. Therefore, designed experiment equal rates white Black applicants.Lastly, talk save data frame. empirical projects, clean data frame start . may need drop certain observations, clean certain variables, overall, make many changes data. never good idea overwrite raw data, done cleaning data, save new data file.save csv file R, can use write.csv function:save data frame resume currently memory csv file named resume1.csv. file saved working directory.also another format data common R: \"RData\". can save file RData file using save() function.often strong argument saving csv file verse RData file. general, however, csv files often portable across different statistical programs. data using R section course therefore stored csv files.","code":"\nresume <- read.csv(\"resume.csv\")\nsummary(resume)       X         firstname             sex           \n Min.   :   1   Length:4870        Length:4870       \n 1st Qu.:1218   Class :character   Class :character  \n Median :2436   Mode  :character   Mode  :character  \n Mean   :2436                                        \n 3rd Qu.:3653                                        \n Max.   :4870                                        \n     race                call        \n Length:4870        Min.   :0.00000  \n Class :character   1st Qu.:0.00000  \n Mode  :character   Median :0.00000  \n                    Mean   :0.08049  \n                    3rd Qu.:0.00000  \n                    Max.   :1.00000  \nhead(resume)  X firstname    sex  race call\n1 1   Allison female white    0\n2 2   Kristen female white    0\n3 3   Lakisha female black    0\n4 4   Latonya female black    0\n5 5    Carrie female white    0\n6 6       Jay   male white    0\nhead(resume$call)[1] 0 0 0 0 0 0mean(resume$call)\n[1] 0.08049281table(resume$race)\n\nblack white \n 2435  2435 \nwrite.csv(resume, file=\"resume1.csv\")\nsave(resume, file=\"resume1.RData\")"},{"path":"resume-experiments.html","id":"logic","chapter":"6 Resume Experiments","heading":"6.7 Logic","text":"Just like Stata, logical statements key concept section course. remind , logical statement statement either true false. R, special type object, called logical objects, objects take either value TRUE FALSE.Sometimes may want subset data observations certain statement true. example, maybe dataset individuals many states, want analyze data California. case, may want subset observations logical statement \"Individual resides California\" true. often going using logical statements R manipulate data.create objects either true false? use logical operators. seen many logical operators Stata portion class. example, double equals sign == logical operator tests whether two things equal. list logical operators using throughout R portion course:! -- ! -- & -- & -- | -- | -- == -- equals== -- equals!= -- equals!= -- equals> -- greater > -- greater < -- less < -- less >= -- greater equal >= -- greater equal <= -- less equal <= -- less equal toNow written different logical operators, see ask logical statements R, save logical objects. begin, start simple logical statement: \"5 equal 6\". way type R type 5==6. statement course false, type R yield FALSE.instead, logical statement \"5 equal 5\", statement yield TRUE.Now write \"5 equal 6\", using ! operator.\"6 equal 6\" course FALSE:Now try understand & operator. use & operator want test two statements true. example, imagine individual 18 lives California. statement \"individual 18 lives California\" true. way might type R age==18 & state==\"CA\". can testing numbers, done . example, statement \"5 equal 5 5 equal 6\" false.contrast, statement \"5 equal 5 5 greater 4\" true.true statements true. statement, either statement true, entire statement true. example, statement \"5 equal 5 5 equal 6\", true either 5==5 | 5==6So far applied logic numbers R. However, can also apply vectors R. particularly important turn data. often want apply logical statements variables data.simple example, create vector graduation dates four entries:can apply logical statements entire vector well. example, imagine want know graduation dates 2010. can type:outputs another vector four entries. entry tells us whether corresponding entry graduationdates vector greater 2010. entries 1, 2, 4, statement FALSE. entry 3, statement TRUE.Next, learn compare two vectors. example, imagine vector birth years supposed individuals graduation data:looking vector, clear something went wrong first individual. individual reported born 2016, graduated 2010. must sort data entry error individual. can quickly identify mistake small data, imagine thousands individuals. possible identify errors manually going data.Instead, can simply test whether birthyears data greater graduationdatesFor logical statement, first element birthyears compared first element graduationdates. Since 2016 greater 2010, first element resulting logical vector TRUE. second element birthyears compared second element graduationdates. Since 1980 less 2001, statement FALSE.far, just observed output logical statements (.e. output displayed console). imagine want save output using future analyses. example, imagine want drop observations birth year greater graduation date. Well, can simply store output new vector assigning name:Now vector problem added environment. check class vector see logical vector.Now, way logic functions R can function numeric variable. Specifically, value TRUE equal number 1. value FALSE equal number 0. Therefore, can treat logical statements way treated binary indicator variables past. example, imagine like compute fraction individuals problem data, defined birth year greater graduation year. take average problem vector.get us right result. problem vector equal TRUE FALSE FALSE FALSE. R interpret vector 1 0 0 0. mean value vector therefore equal 0.25. words, 25 percent individuals, problem data. prefer vector numeric format instead logical, can also change using .numeric() function:just changes information displayed R, actual information .","code":"5==6\n[1] FALSE5==5\n[1] TRUE5==6\n[1] FALSE6!=6\n[1] FALSE5==5 & 5==6\n[1] FALSE5==5 & 5>4\n[1] TRUE5==5 | 5==6\n[1] TRUE\ngraduationdates <- c(2010,2001,2019,2003)graduationdates>2010\n[1] FALSE FALSE  TRUE FALSE\nbirthyears<- c(2016, 1980, 1996, 1990)birthyears > graduationdates\n[1]  TRUE FALSE FALSE FALSE\nproblem <- birthyears > graduationdatesclass(problem)\n[1] \"logical\"mean(problem)\n[1] 0.25as.numeric(problem)\n[1] 1 0 0 0"},{"path":"resume-experiments.html","id":"subsetting-vectors","chapter":"6 Resume Experiments","heading":"6.8 Subsetting Vectors","text":"Often want extract certain elements vector, know subsetting. R, subset vector use []. Inside [] can place number extract certain element. example, type graduationdate[3], extract third element vector graduationdate. referred indexing. example, 3 index number.can also use logic extract certain elements. example, general, type vec[logical statement], elements logical statement true extracted. especially useful using data common want extract elements meet certain condition, example, individuals age 18, counties located California, etc.understand subset vectors R, run examples. First, re-create graduationdates vector.third entry graduationdates 2019. want extract entry, can just typeInstead extracting element, can also use referred negative indexing. put negative front index number, get elements vector, except element. example, see happens type graduationdates[-3]output another vector, now composed three elements. new vector dropped third element graduationdates.can also use indexing subset multiple elements vector. example, imagine like extract first third element graduationdates vector. , create vector indices.recall, c(1,3) vector. c() function concatenates 1 3 vector. way R reads statement \"extract vector graduationdates, 1st 3rd element.Now understand subset vectors using numbers vectors, learn index using logic. Instead entering vector numbers index, can directly enter vector TRUE FALSE. elements entry TRUE extracted. see work, try understand example :vector c(TRUE, FALSE, TRUE, FALSE) TRUE 1st 3rd element. Therefore, first third elements extracted. common way might see used another vector variable. example, imagine also vector contains school student attended.Imagine want retrieve graduation date student attended UCSD. type school==\"UCSD\", get vector 4 elements:first element TRUE. Therefore, use vector subset graduationdate vector, retrieve graduation date student attended UCSD.Next, turn simple examples empirical application section: correspondence studies. First, re-load data fictitious applications Bertrand Mullainathan (2004).Remember, can really think variable dataframe vector. Therefore, really need learn anything new. just need apply learned far new data structure.First, talk use logical subsetting variable. happens type resume$race==\"black\". Well, resume$race vector 4,870 elements. takes values \"white\" \"black\". Therefore, resume$race==\"black\" also vector 4,870 elements. first element TRUE first individual dataset Black-sounding name. first element FALSE first element white-sounding name. can convince typing:studying resulting vector vec. happens add function around vector. example, type sum(resume$race==\"black\"), R add values vector resume$race==\"black\". Recall, R interprets value TRUE equal 1, value FALSE equal 0. type sum(resume$race==\"black\"), just retrieve total number applications Black-sounding names.can subset one variable using logical statement built another variable. example, imagine want compute callback rate Black applicants. Therefore, want compute average call individuals race==\"black\". R, can write statement following:easiest interpret line code starting indexing step. [resume$race==\"black\"] restricts observations applicant Black. resume$call[resume$race==\"black\"] vector call, restricted observations applicant Black. Finally, placing entire block mean(), computing average resume$call applicants resume$race==\"black\". want compare white applicants, can simply type:","code":"\ngraduationdates <- c(2010,2001,2019,2003)graduationdates[3]\n[1] 2019graduationdates[-3]\n[1] 2010 2001 2003graduationdates[c(1,3)]\n[1] 2010 2019graduationdates[c(TRUE, FALSE, TRUE, FALSE)]\n[1] 2010 2019\nschool <- c(\"UCSD\", \"UCB\", \"UCLA\", \"UCR\")school==\"UCSD\"\n[1]  TRUE FALSE FALSE FALSEgraduationdates[school==\"UCSD\"]\n[1] 2010\nresume <- read.csv(\"resume.csv\")\nvec <- resume$race==\"black\"sum(resume$race==\"black\")\n[1] 2435mean(resume$call[resume$race==\"black\"])\n[1] 0.06447639mean(resume$call[resume$race==\"white\"])\n[1] 0.09650924"},{"path":"resume-experiments.html","id":"subsetting-data-frames","chapter":"6 Resume Experiments","heading":"6.9 Subsetting Data Frames","text":"Next, discuss subset data frames. illustrate concept, first going generate small data frame R:understand code produced, look dataframe student:data frame two variables: school graduationdate. first column o f data frame corresponds school second corresponds graduationdate. data frame three total observations. extract specific value data frame, can use indexing. using indexing vector, specify single number, pluck element. Now, data frame, need specify two numbers. First, specify row like extract specify column. example, extract value 3rd row, 1st column, type:output \"UCSD\", third student data frame went UCSD 1st column data frame corresponds variable school.Instead extracting specific elements data frame, can also extract entire columns rows. example, extract first row type:second entry [,] always corresponds column. Since blank, R extracted columns first observation. Similarly, can extract single column leaving rows portion index blank. example, extract second column, type:Now, since row index blank, R extracts values rows, just second column.\ncan also use vector specify observations columns like retrieve. example, imagine think error recording graduation date second observation data frame. careful, like drop individual data frame. words, want extract rows 1 3 data frame, leave row 2. , can subset specifying vector indices. vector contain indices every row want final data frame:can thing negative indexing, seen :Next, discuss use logic subset data frames. example, imagine want extract individuals went UCSD. words, want individuals students$school==\"UCSD\" TRUE. Remember, type R get vector TRUE FALSE statements:, just like vectors, use logical statement subset, retrieve observations statement TRUE. case, statement TRUE first third observation:Indexing data frames vectors way useful skill portable software programs may encounter future. However, R, often easier solutions subsetting data. example, function subset often used subset data frames. learn tidyverse future chapter, function useful subsetting.general syntax subset command :resulting data frame subset data frame named df restricted observations logical statement comma TRUE. example, create data frame UCSD students, type:Remember, save data frame new object R, need assign name.","code":"\nstudents <- data.frame(school=c(\"UCSD\", \"UCB\", \"UCSD\"),\n                       graduationdate=c(2010, 2019, 2015))students\n  school graduationdate\n1   UCSD           2010\n2    UCB           2019\n3   UCSD           2015students[3,1]\n[1] \"UCSD\"students[1,]\n  school graduationdate\n1   UCSD           2010students[,2]\n[1] 2010 2019 2015students[c(1,3),]\n  school graduationdate\n1   UCSD           2010\n3   UCSD           2015students[-2,]\n  school graduationdate\n1   UCSD           2010\n3   UCSD           2015students$school==\"UCSD\"\n[1]  TRUE FALSE  TRUEstudents[students$school==\"UCSD\",]\n  school graduationdate\n1   UCSD           2010\n3   UCSD           2015subset(df, logical statement)subset(students,students$school==\"UCSD\")\n  school graduationdate\n1   UCSD           2010\n3   UCSD           2015\nucsd_students <- subset(students,students$school==\"UCSD\")"},{"path":"resume-experiments.html","id":"conclusion-4","chapter":"6 Resume Experiments","heading":"6.10 Conclusion","text":"Now, put everything learned chapter together study discrimination hiring practices. get started, load resume dataWhen exploring new data frame, often helpful get sense variables values take. look first observations data order remind data:main outcome interest whether individual received callback interview. data frame, 0 represents resume get callback. 1 represents resume get callback. data stored way, can simply take average variable call order retrieve overall callback rate:Remember, whenever specify variable, need specify data frame well variable name. well see later, often helpful multiple data frames loaded memory R.experiment, want understand whether callback rate varies race. proceeding, look distribution race across applications:2435 resumes white-sounding names 2435 resumes Black-sounding names. Remember, fictitious resumes, experimenters designed study equal number applications white Black applicants.Note, retrieve information using logic. type logical statement, R return logical object composed TRUE FALSE. Whenever R sees TRUE interprets 1, whenever R sees FALSE interprets TRUE. example, type:get value 2435. ? Well, resume$race==\"black vector length 4870. Half elements TRUE half FALSE. sum() function adds values vector. TRUE values equal 1 FALSE values equal zero. Therefore, simply counting applications statement TRUE.Now, get main analysis, studies whether callback rates vary race. , first find callback rate applicants Black-sounding names. , utilize logic:using indexing. [resume$race=\"black\"] subsets Black applicants. put inside mean(resume$call[]), R retrieve average resume$call vector, restrict Black applicants. can see, 6.4 percent Black applicants receive callback.now compare white applicants:couple different ways performed analysis. example, sometimes helpful generate different data frames subsets data. done indexing subset function:example, create data frame restricted Black applicants type:, calculate callback rate Black applicants, just reference data frame:Now performed main analysis, summarize findings. Around 9.7 percent white applicants receive callback. Therefore, 3.3 percentage point disparity callback rates. substantial disparity. baseline callback rates quite low setting. 3.3 percentage point disparity implies white applicants 50 percent likely receive callback relative Black applicants:\\[\n\\text{Percent Increase Callback Rate White Applicants}=\\frac{9.7-6.4}{6.4}=0.516\n\\]\nhighlights stark disparity labor-market outcomes. given empirical design: randomizing race across applications, disparities driven factors employees take account making hiring decisions. due names associated applications, names proxy race.empirical design become widely popular. used study discrimination across demographics, example, gender, sexual orientation, disability status. also increased scope, recent experiments sending thousands thousands fictitious resumes order understand discrimination largest employers U.S (see Kline, Rose, Walters 2022).10","code":"\nresume <- read.csv(\"resume.csv\")head(resume)\n  X firstname    sex  race call\n1 1   Allison female white    0\n2 2   Kristen female white    0\n3 3   Lakisha female black    0\n4 4   Latonya female black    0\n5 5    Carrie female white    0\n6 6       Jay   male white    0mean(resume$call)\n[1] 0.08049281table(resume$race)\n\nblack white \n 2435  2435 sum(resume$race==\"black\")\n[1] 2435mean(resume$call[resume$race==\"black\"])\n[1] 0.06447639mean(resume$call[resume$race==\"white\"])\n[1] 0.09650924\nresume_blacknames <- resume[resume$race==\"black\",]\nresume_whitenames <- resume[resume$race==\"white\",]mean(resume_blacknames$call)\n[1] 0.06447639\nmean(resume_whitenames$call)\n[1] 0.09650924"},{"path":"the-rug-rat-race.html","id":"the-rug-rat-race","chapter":"7 The Rug Rat Race","heading":"7 The Rug Rat Race","text":"","code":""},{"path":"the-rug-rat-race.html","id":"time-diaries","chapter":"7 The Rug Rat Race","heading":"7.1 Time Diaries","text":"Figure 7.1 plots somewhat puzzling finding Ramey Ramey (2010). figure shows amount hours per week mothers spend childcare time. college-educated less--college-educated mothers, time spent childcare rose dramatically early 1990s?\nFigure 7.1: Time Spent Childcare Parents, Educational Attainment, 1965-2008\nHowever, rise especially dramatic college-educated mothers. puzzling? Well, time period returns education increasing lot. words, college-educated mothers lot gain working labor market, data, find spending time childcare. section course, explore .order explore question use data American Time Use Survey (ATUS). survey administered Bureau Labor Statistics, agency collects many important statistics concerning U.S. economy.data based \"time diaries\" detailed descriptions activities given day. example, interview held Tuesday, individual report everything Monday (4 Monday 4 Tuesday). useful survey technique get accurate representation much time people spend various activities.digging data, however, going learn bit R programming language. particular, chapter cover important functions data wrangling.","code":""},{"path":"the-rug-rat-race.html","id":"conditional-statements","chapter":"7 The Rug Rat Race","heading":"7.2 Conditional Statements","text":"basic idea behind conditional statements sometimes R want execute code, certain condition true. Conditional statements incredibly important programming languages. example, type something computer get error statement, conditional statement work. certain condition met (error case), computer output error message.way implement idea R use statements. general syntax statements :might bit easier understand syntax simple example. begin, going create object R called door equal locked.Now, going write code return message object door indeed locked.R . Well, set assigned value locked object door, logical statement parentheses door==\"locked\" TRUE. Therefore, run section code, code inside curly brackets {} executed. case, code just prints message.happens change value door unlocked. Well, case door==\"locked\" longer TRUE. Therefore, code inside brackets executed.try another example. Say take random draw three numbers: -1, 0, 1. can take random draw list numbers using sample function:part code -1:1 controls numbers drawn. type -1:1 R, see prints numbers -1,0, 1. second part code ,1 tells R many random samples take. case, just 1. overall, code simply setting x equal either -1, 0, 1 randomly.now write conditional statement depends outcome x. write code returns absolute value x, x less zero.Next, discuss else statements. Sometimes, want execute certain code statement TRUE, code FALSE. example, first example, imagine door object equal locked want R print message Please Come !. way can R else statement:Since set door<-\"locked\", first statement door==\"locked\" TRUE. Therefore, code inside first curly brackets executed. Now, change door unlocked see changes output.now first statement door==\"locked\" FALSE. Therefore, R executes code else {}.important notes conditional statements move . First, else must appear line end curly bracket }. , R know logical statement connect else . Second, R execute code else brackets door==\"locked\" FALSE reason.example, imagine set door <- Locked\". might look like first logical statement door==\"locked\" true, R case sensitive. Therefore, case, statement door==\"locked\" FALSE. implies R print words Please Come !.","code":"if (logical statement) {\n  \n  code to be executed if logical statement is TRUE\n  \n}\ndoor <- \"locked\"if (door==\"locked\") {\n  \n  print(\"sorry, you need a key to enter\")\n  \n}\n[1] \"sorry, you need a key to enter\"\ndoor <- \"unlocked\"\n\nif (door==\"locked\") {\n  \n  print(\"sorry, you need a key to enter\")\n  \n}x <- sample(-1:1,1)\nx\n[1] 0\nif (x<0){\n  abs(x)\n}door <- \"locked\"\nif (door==\"locked\"){\n  print(\"Sorry, you need a key to enter\")\n} else {\n  print(\"Please Come in!\")\n}\n[1] \"Sorry, you need a key to enter\"door <- \"unlocked\"\nif (door==\"locked\"){\n  print(\"Sorry, you need a key to enter\")\n} else {\n  print(\"Please Come in!\")\n}\n[1] \"Please Come in!\""},{"path":"the-rug-rat-race.html","id":"for-loops","chapter":"7 The Rug Rat Race","heading":"7.3 For Loops","text":"data analysis, often find performing operation many times. might performing analysis different variables performing calculation many different numbers.One way perform analysis writing scripts many, many lines code. Much code might exactly , simply changes single number variable. alternative way perform computation loops. Although learning implement loops R, loops concept aware programming language.general form loop R given :tells R loop. set things depend exactly trying . Often, list numbers. next part code { tells R everything follows part loop. end curly bracket } tells R loop .understand concepts concretely. going go simple example. imagine someone demanded use R print 2*equal 1, 2, 3, 4, 5. simpler language, multiply number 1 5 2 print result. going perform call brute force method.First, print 2*1So code roundabout way? directly type 2*1. Well, go loop become clear coding somewhat roundabout way. need understand now exactly code .First, creating object named equal 1. , multiplying object 2 printing result. Now, number 2.Note second part code print(2*) exactly . key writing loop. can continue rest numbersIn block code, thing changing equal . Well, exactly loop .type (1:5) telling R execute print(2*) 5 times. first time, ==1. referred first iteration loop. executed code ==1, move ==2. second iteration loop.great thing can greatly increase efficiency coding. example, imagine wanted print 2*=1:100. go brute force way, 200 lines code (first setting equal given number executing code). loop, one thing changes: first line simply type (1:100).example iterating list numbers 1 5. can also iterate objects R. example, instead iterating sequence numbers, can iterate vector.loop, first iteration, equal 3. second iteration equal 10. third equal 99.Next, going go example loops words instead numbers. example, going print list. say need three sets homework assignments: math, reading, writing. start example, create vector assignments need :can loop contents vector order print -list.introduced new things , go code slowly. First, instead looping numbers , looping words. first iteration equal first entry vector homework. Therefore, equal math first iteration.Next, used knew function cat(). function concatenates prints. Therefore, type cat(\"\",) interpret (first) iteration forming sentence Math. last part ,\"\\n\" telling R display next text line . want skip line word document, just press Enter. R, can type \"\\n\". still unsure \"\\n\" , try take code see result looks.example, looping words homework. alternative way write loop take advantage indexing. following code performs exact process way, just coded slightly different way.loop, looping numbers. length(homework) equal 3. looping numbers 1 3. first iteration, concatenating words \"\" homework[1]. Since homework[1] equal Math, retrieving exact output .want code loop way? Well, setting, really matter write code. However, always case. Using loop index () order subset vectors data frames common practice, important exposed aspect loops well.","code":"for (some set of things) {\n  do some stuff\n}i <-1\nprint(2*i)\n[1] 2i <- 2 \nprint(2*i)\n[1] 4i <- 3 \nprint(2*i)\n[1] 6i <- 4 \nprint(2*i)\n[1] 8i <- 5 \nprint(2*i)\n[1] 10for (i in 1:5){\n  \n  print(2*i)\n\n}\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n[1] 10for (i in c(3,10,99)){\n  print(2*i)\n}\n[1] 6\n[1] 20\n[1] 198\nhomework <- c(\"math\", \"reading\", \"writing\")for (i in homework) {\n  cat(\"Do\", i, \"\\n\")\n}\nDo math \nDo reading \nDo writing for (i in 1:length(homework)) {\n  cat(\"Do\", homework[i], \"\\n\")\n}\nDo math \nDo reading \nDo writing "},{"path":"the-rug-rat-race.html","id":"for-loops-data","chapter":"7 The Rug Rat Race","heading":"7.4 For Loops (Data)","text":"Next, going use loops make calculations data. remind , empirical application going study time spent childcare, particularly mothers, changed time. begin, going load data American Time Use Survey.can see, pretty large datasets. 106020 observations 54 variables. data large, first going look data just subset variables focusing :goal compute time spent childcare time mothers. variable mother2 indicator variable equal one individual mother child currently living household. age holds age individual. childtot total hours spent childcare week. dataset actually holds year data collected. Different years come datasets. important keep mind going forward analysis.goal follow Ramey Ramey (2010) analysis closely possible. order , going make number restrictions data sample individuals studied Ramey Ramey (2010). involve restricting mothers ages 25 34 child houshold. terms variables data frame, individuals age>=25 & age<=34 & mother2==1. logical statement composed three statements, strung together & operators. Therefore, order TRUE, every individual statement must TRUE. able convince statement TRUE mothers ages 25-34.way can subset individuals subset command.Now data frame restricted sample analyzing. goal compute average amount childtot years (case, different values variable dataset). Therefore, need next list years data frame.Luckily, function unique() come handy . unique() retrieves list unique values variable. words, type unique(rr$dataset) get list years data frame.going write loop iterate years. writing full loop, make sure understand code first iteration. First, set equal one.Next, subset data create dataframe contains observations dataset==years[]. Since currently equal 1, subset data observations 1965.Since data frame sub contains observations 1965, can simply take average childtot within data frame retrieve average amount hours spent childcare per week 1965.actually go iterating loop, want keep track number printed corresponds . Therefore, use cat() function clarify exactly printed :Now written code <-1, just need put code inside loop loops 1 total number years. total number years given length(years):can see table, replicates one main findings Ramey Ramey (2010). 1965-1993, average time spent childcare fluctuated 9 13 hours. 1998 onwards, average amount time spent childcare increases dramatically, often 15-17 hours per week average.","code":"rr <- read.csv(\"rugratrace.csv\")\ndim(rr)\n[1] 106020     54head(rr$mother2,rr$age,rr$childtot,rr$dataset)\nError in checkHT(n, dx <- dim(x)): invalid 'n' - must have length one when dim(x) is NULL, got 106020\nmothers2534 <- subset(rr, mother2==1 & age>=25 & age<=34)years <- unique(mothers2534$dataset)\nyears\n [1] 1965 1975 1985 1993 1998 2003 2004 2005 2006 2007 2008\ni <- 1 \nsub <- subset(mothers2534, dataset==years[i])mean(sub$childtot)\n[1] 13.82249# prints out year\ncat(\"Year\", years[i],\"\\n\")\nYear 1965 \n# prints out average childcare for this group\ncat(\"Average childcare per week:\", mean(sub$childtot),\"\\n\")\nAverage childcare per week: 13.82249 \nfor (i in 1:length(years)) {\n  \n  # subset to year i\n  sub <- subset(mothers2534, dataset==years[i])\n\n  # prints out year\n  cat(\"Year\", years[i],\"\\n\")\n  \n  # prints out average childcare for this group\n  cat(\"Average childcare per week:\", mean(sub$childtot),\"\\n\")\n  \n}Year 1965 \nAverage childcare per week: 13.82249 \nYear 1975 \nAverage childcare per week: 9.758393 \nYear 1985 \nAverage childcare per week: 12.86947 \nYear 1993 \nAverage childcare per week: 9.244037 \nYear 1998 \nAverage childcare per week: 14.83095 \nYear 2003 \nAverage childcare per week: 16.02707 \nYear 2004 \nAverage childcare per week: 16.45709 \nYear 2005 \nAverage childcare per week: 15.68486 \nYear 2006 \nAverage childcare per week: 16.23848 \nYear 2007 \nAverage childcare per week: 17.01327 \nYear 2008 \nAverage childcare per week: 16.24845 "},{"path":"the-rug-rat-race.html","id":"tidyverse","chapter":"7 The Rug Rat Race","heading":"7.5 Tidyverse","text":"far using referred base R. utilized functions come standard R. However, one main strengths R active user community. R open-source, users can write packages R make widely available. means functionality R essentially growing every day.One useful collection packages data analysis tidyverse package. collection packages can many things. cover small portion package capable . want get thorough understanding tidyverse, can go R Data Science.using tidyverse, actually install version R. order install package, can use install.packages() function. example, install tidyverse, type:download necessary components tidyverse package onto computer. need install package .Next, order use package given R session, need load memory using library() function:common mistake students forget load package given R session. , might get error \"[function] found\". using functions external package, need make sure load package every session R want use .first concept learn R concept tibble. tibble similar calling data frame. purposes, can think tibble synonymous data frame. main difference data stored presented. us, matter much. larger datasets, sometimes loading tibble can make big difference terms long computations take.start, load rugratrace.csv data frame.convert data frame tibble, can use function as_tibble().following along, may think code actually anything. However, can tell difference print data set.get information printing tibble relative data frame. purposes, matter much load dataset tibble data frame. However, important understand concepts, may see various online resources reference tibble data frame. Generally, class load datasets tibbles now .load dataset tibble directly, can use read_csv function, difference now underscore separates words function, rather period.now discuss functions come tidyverse. first functions introduce filter, select, arrange.filter() function retrieves observations (rows) data meet certain condition. example, maybe want restrict analysis certain demographic. can use filter() function . example, want restrict mothers ages 25 34. can use filter() accomplish . general syntax filter() :example, can create subset dataset want typing:can use logical operators combination filter well. example, imagine subset analysis want years 2003 2008. can use operator %% accomplish .code saying restrict mothers2534 observations dataset range 2003 2008. One important note code overwrite mothers2534. can see tibble printed . want save result filter() need assign resulting tibble name. Otherwise simply printed console actually saved.Next, discuss select(). function choose variables want remain tibble. example, current tibble many variables, really need subset analysis. can select variables really need:Now print mothers2534, four variables now.Finally, last function section arrange(). arrange() function sort data based values variable. example, imagine want observations mothers2534 arranged youngest age oldest age. type:Now, can verify youngest individuals first:oldest last.instead want sort oldest youngest, can use function desc(), stands descending. words dataset sorted highest value lowest value.finish chapter, summarize new functions tidyverse learned. First, read_csv() used order read data sets R tibble. Next, learned three functions useful manipulating data. First, filter() selects observations tibble meet certain condition. case, mothers ages 25 34. Second, select() chooses variables keep tibble. case, kept variables indicated whether individual mother, year data collected, age individual, amount time spent childcare individual. Lastly, arrange() used sort data based value variable. chapter showed use sort data youngest oldest.","code":"\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\nrr <- read.csv(\"rugratrace.csv\")\nrr <- as_tibble(rr)rr\n# A tibble: 106,020  54\n   state   age   sex ethnic under18 under5 ageyngst student\n   <int> <int> <int>  <int>   <int>  <int>    <int>   <int>\n 1    23    26     2     -9       4      2       NA       0\n 2    23    53     2     -9       1      0       NA       0\n 3    23    21     2     -9       2      2       NA       0\n 4    23    24     2     -9       2      2       NA       0\n 5    23    25     2     -9       3      1       NA       0\n 6    23    33     2     -9       4      2       NA       0\n 7    23    26     2     -9       3      2       NA       0\n 8    23    61     1     -9       0      0       NA       0\n 9    23    57     1     -9       0      0       NA       0\n10    23    43     2     -9       2      1       NA       0\n#  with 106,010 more rows, and 46 more variables:\n#   act20 <int>, act21 <int>, act26 <int>, act24 <int>,\n#   act22 <int>, act23 <int>, act27 <int>, act67 <int>,\n#   act25 <int>, act29 <int>, recwght <dbl>, dataset <int>,\n#   fips <int>, married <int>, working <int>,\n#   fulltime <int>, parttime <int>, mother2 <int>,\n#   father2 <int>, dropout <int>, hsonly <int>, \nrr <- read_csv(\"rugratrace.csv\")filter(dataframe, some logical statement)\nmothers2534 <- filter(rr, age<=34 & age>=25 & mother2==1)filter(mothers2534, dataset%in%2003:2008)\n# A tibble: 6,416  54\n   state   age   sex ethnic under18 under5 ageyngst student\n   <int> <int> <int>  <int>   <int>  <int>    <int>   <int>\n 1    NA    32     2     NA       1     NA        2       1\n 2    NA    33     2     NA       3     NA        3       0\n 3    NA    33     2     NA       1     NA        7       0\n 4    NA    32     2     NA       1     NA        2       0\n 5    NA    25     2     NA       1     NA        6       0\n 6    NA    34     2     NA       2     NA        2       0\n 7    NA    31     2     NA       1     NA        2       0\n 8    NA    32     2     NA       2     NA        6       0\n 9    NA    32     2     NA       2     NA       12       0\n10    NA    32     2     NA       2     NA        3       0\n#  with 6,406 more rows, and 46 more variables:\n#   act20 <int>, act21 <int>, act26 <int>, act24 <int>,\n#   act22 <int>, act23 <int>, act27 <int>, act67 <int>,\n#   act25 <int>, act29 <int>, recwght <dbl>, dataset <int>,\n#   fips <int>, married <int>, working <int>,\n#   fulltime <int>, parttime <int>, mother2 <int>,\n#   father2 <int>, dropout <int>, hsonly <int>, \nmothers2534 <- select(mothers2534, dataset, mother2, age, childtot)mothers2534\n# A tibble: 8,115  4\n   dataset mother2   age childtot\n     <int>   <int> <int>    <dbl>\n 1    1965       1    26     7   \n 2    1965       1    25     7.58\n 3    1965       1    33    27.2 \n 4    1965       1    26     4.20\n 5    1965       1    25     6.42\n 6    1965       1    26    14.8 \n 7    1965       1    30    19.2 \n 8    1965       1    33     8.75\n 9    1965       1    30    16.3 \n10    1965       1    29    29.8 \n#  with 8,105 more rows\nmothers2534 <- arrange(mothers2534,age)head(mothers2534$age)\n[1] 25 25 25 25 25 25tail(mothers2534$age)\n[1] 34 34 34 34 34 34mothers2534 <- arrange(mothers2534,desc(age))\nhead(mothers2534$age)\n[1] 34 34 34 34 34 34"},{"path":"the-rug-rat-race.html","id":"the-pipe-operator","chapter":"7 The Rug Rat Race","heading":"7.6 The Pipe Operator","text":"section going going pipe operator: %>%. pipe operator useful tool use working tidyverse. purposes, can use string along number commands. Eventually, see greatly improves efficiency code. First, however, simply need learn mechanically pipe operator .far, R, applying functions objects. example, imagine vector x want take mean. apply mean() function x typing mean(x). pipe operator gives us alternative way write . can type: x %>% mean(). general, function f(), can either write f(x) x %>% f(x).benefit pipe operator. Well, often data analysis, want apply many functions. example, may want restrict certain observations, select certain variables, . can think applying many functions objects sequency. example, first apply f(), g() h() can represent :Using pipe operators, can equivalently write :little easier digest. ? taking object f, applying f, applying g, applying h. read pipe operator, can think . helpful tool remember pipe operator .far, relatively abstract discussion pipe operators. load real data can see works practice.previous chapter, generated new data set (1) filters mothers 25 34. understand pipe operators, first see can perform task using %>%.remind , code filter previous section :Instead, use pipe operator, first supply data frame rr call function (relevant logical statement):useful way think pipe operators using data. First supply tibble data frame, apply functions like data.far, really see benefit pipe operator example. looks just rewritten code. main benefit pipe operator us, however, can string along multiple commands. example, imagine want (1) filter data , (2) select variables need. , two steps:steps efficient. first step create mothers2534. next step, overwrite new mothers2534. total, needed type mothers2534 three different times get final tibble. Now, see pipe operators clean code:Now code bit easier digest. ? First specifying tibble rr, filtering , selecting variables want. Now example, two steps. now imagine want also sort dataset oldest youngest. problem, can just add pipe operator calls arrange() function previous code:resulting code still clear interpretable. Now imagine many, many steps needed perform. soon glad pipe operator!","code":"\nh(g(f(x)))\nx %>% \n  f %>%\n    g %>%\n      hrr <- read_csv(\"rugratrace.csv\")\nRows: 106020 Columns: 54\n Column specification \nDelimiter: \",\"\ndbl (54): state, age, sex, ethnic, under18, under5, agey...\n\n Use `spec()` to retrieve the full column specification for this data.\n Specify the column types or set `show_col_types = FALSE` to quiet this message.\nmothers2534 <- filter(rr, age<=34 & age>=25 & mother2==1)\nmothers2534 <- rr %>% filter(age<=34 & age>=25 & mother2==1)\n# filter to mothers 25-34\nmothers2534 <- filter(rr, age<=34 & age>=25 & mother2==1)\n# select certain variables\nmothers2534 <- select(mothers2534, dataset, mother2, age, childtot)\nmothers2534 <- rr %>%\n  filter(age<=34 & age>=25 & mother2==1) %>%\n  select(dataset, mother2, age, childtot)\nmothers2534 <- rr %>%\n  filter(age<=34 & age>=25 & mother2==1) %>%\n  select(dataset, mother2, age, childtot) %>%\n  arrange(desc(age))"},{"path":"the-rug-rat-race.html","id":"mutate","chapter":"7 The Rug Rat Race","heading":"7.7 Mutate","text":"far, documented parents increased amount time spent childcare 1990s 2000s. Ramey Ramey (2010) document increase even dramatic college-educated mothers.Ramey Ramey (2010) argue increased time spent childcare college-educated mothers driven competitiveness college. provide evidence , break components childcare. Figure 7.2 Ramey Ramey (2010) plots amount time spent childcare college-educated less--college educated mothers type care.\nFigure 7.2: Time Spent Childcare, Age Child Type Care 2003-2008\nfocus top-right panel: mothers children age 5. College-educated mothers (darker gray bars) spending time categories: education, organizing attending activities, chauffeuring. Ramey Ramey (2010) argue may driven college competitiveness. competitive college, students need earn higher scores tests participate extracurricular activities.data, going create variable named collegeprep sum total time spent education time spent travelling children. Right now, however, data set variable. order add , need learn create variables R.two ways create new variables R. first uses base R. general syntax :example, add variable collegeprep addition childeduc childtravel, two variables rugratrace.csv data set.mean amount time spent collegprep 1.2 hours. Recall made restrictions rr, includes everyone data set, including children. explains value much lower values studying mothers children house.can also generate new variables using mutate() function (tidyverse). general syntax :example, want add childprep using mutate() can type:Note first step type rr <-. overwrite rr specifying step, R generate new tibble added variable, saved anywhere.nice thing mutate() relative base R can generate number variables within mutate() command. example, imagine also want create variable captures childcare time spent college prep. words, want add variable childnotcollegeprep = childtot - childcollegeprep tibble. can using following:variables add change, can simply add comma end last line, add new variable . Note, can even call variables created earlier function (.e. childnotcollegeprep can created childcollegeprep created first).Lastly, talk transmute() function. function also generates new variables, time, drops pre-exisiting variables. example, wanted create tibble contains collegeprep childnotcollegeprep, use transmute() function:can see, data set contains two variables created using transmute().","code":"\ndf$newvarname <- expressionrr$collegeprep <- rr$childeduc + rr$childtravel\nsummary(rr$collegeprep)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  0.000   0.000   0.000   1.201   0.000  98.933    1146 \nmutate(dataframe, newvarname = expression)\nrr <- mutate(rr, collegeprep=childtravel+childeduc)rr <- mutate(rr, collegeprep=childtravel+childeduc,\n             childnotcollegeprep=childtot-childcollegeprep)\nError in `mutate()`:\n! Problem while computing `childnotcollegeprep =\n  childtot - childcollegeprep`.\nCaused by error in `mask$eval_all_mutate()`:\n! object 'childcollegeprep' not found\ncollegeprepdat <- transmute(rr,\n                            collegeprep = childeduc + childtravel,\n                            notcollegeprep = childtot - collegeprep)collegeprepdat\n# A tibble: 106,020  2\n   collegeprep notcollegeprep\n         <dbl>          <dbl>\n 1       0               7   \n 2       3.5             1.17\n 3       0.583           2.33\n 4       0              42.6 \n 5       0               7.58\n 6       0              27.2 \n 7       1.17            3.03\n 8       0               0   \n 9       0               0   \n10       0               7   \n#  with 106,010 more rows"},{"path":"the-rug-rat-race.html","id":"group-by-and-summarize","chapter":"7 The Rug Rat Race","heading":"7.8 Group By and Summarize","text":"Often course data analysis want break summary statistics groups. example, maybe dataset individual-level voting records want compute average voting rates state. Maybe data wages time, want compute average wage year. R, convenient way make calculation combing group_by() function summarize() function.begin discussion functions, first illustrate use summarize() command combining group_by(). application utilize data Ramey Ramey (2010) time use.remind , tibble rr contains year observation variable named dataset amount spent childcare variable named childtot.summarize() function used generate summary statistics. example, imagine just want take average level childtot across entire sample. can typing:first part code, summarize(rr, tells R generating summary statistics rr data set. second, part meanchildtot= actually giving name summary statistic. words, can change part code difference name column output. actual code still execute without error. helpful feature summarize() command often want save summary statistics new tibble. multiple summary statistics computed, need able keep track various summary statistics. last part code mean(childtot, na.rm=T) declaring meanchildtot equal average childtot. specified na.rm=T, missing values ignored computation.nice feature summarize function can generate multiple statistics single line. example, imagine like compute mean level childcare median level childcare. can simply add code previous summary command giving median different name:median actually zero taking median entire sample. Many individuals zero time spent childcare, children many households.Next, discuss real usefulness summarize command: summarizing groups. Many times part analysis, might want provide summary statistics group variable. times, may actually want change unit--observation dataset. Using group_by() function summarize() function easy way accomplish .group_by() function tells R subsequent functions done separately values group_by() variable. example, dataset, list years:Therefore, specify group_by(dataset), subsequent functions applied separately value dataset. particular, next summarize(meanchildtot = mean(childtot, na.rm=T)), mean childtot taken separately value dataset. try together now.way can read code : first read rr tibble, group values variable dataset, value dataset, take average childtot, store information name meanchildtot.can also make even complicated summarizing two variables. type group_by(var1,var2), R group unique combination variables. proceeding main example, generate small data frame can understand grouping two variables.type group_by(school,graduationdate), subsequent summarize() command done completely separately unique combinations school graduationdate. data frame, 4 unique combinations -- \"2010\", \"2015\", \"B 2010\", \"B 2015\". see get summarize groups two variables:understand unique combinations generated, change dataset slightly.Now two unique combinations data school graduation date: \"2010\" \"B 2015\". Therefore, summarize groups, two averages taken:Now, return rr tibble. Ramey Ramey (2010), authors compared time spent childcare time separately college-educated less--college educated individuals. can achieve similar analysis grouping dataset college. variable college variable takes value 1 individual graduated college, zero otherwise.understand table, look first row. first row dataset==1965 college==0. words, row corresponds individuals 1965 without college education. value meanchildtot column average amount time spent childcare individuals. move table, see R computed man time spent childcare every unique combination dataset college.Often practice, may want actually save output function. , just need assign name, usual:Now created new tibble totchildbyyearcollege contains averages time college education.also accomplished looping values dataset college computing means storing . included (1) writing much code (2) actually taken longer run. Using group_by() summarize() much efficient way accomplish .","code":"head(rr$dataset,rr$childtot)\nError in checkHT(n, dx <- dim(x)): invalid 'n' - must have length one when dim(x) is NULL, got 106020summarize(rr, meanchildtot = mean(childtot, na.rm=T))\n# A tibble: 1  1\n  meanchildtot\n         <dbl>\n1         4.69summarize(rr, \n          meanchildtot = mean(childtot, na.rm=T),\n          medianchildtot=median(childtot, na.rm = T))\n# A tibble: 1  2\n  meanchildtot medianchildtot\n         <dbl>          <dbl>\n1         4.69              0unique(rr$dataset)\n [1] 1965 1975 1985 1993   NA 1995 1998 2000 2003 2004 2005\n[12] 2006 2007 2008\nrr %>%\n  group_by(dataset) %>%\n  summarize(meanchildtot=mean(childtot,na.rm=T))# A tibble: 14  2\n   dataset meanchildtot\n     <dbl>        <dbl>\n 1    1965         4.27\n 2    1975         3.36\n 3    1985         2.95\n 4    1993         2.09\n 5    1995         4.23\n 6    1998         5.20\n 7    2000        10.5 \n 8    2003         4.85\n 9    2004         4.73\n10    2005         5.17\n11    2006         5.06\n12    2007         4.92\n13    2008         4.97\n14      NA       NaN   \nstudent.df <- data.frame(school=c(\"A\", \"A\", \"A\", \"B\", \"B\", \"B\"),\n                        graduationdate=c(2010,2010,2015,2010,2010,2015),\n                        gpa = c(3.2,3.7,2.9,4.0,3.2,1.8))student.df\n  school graduationdate gpa\n1      A           2010 3.2\n2      A           2010 3.7\n3      A           2015 2.9\n4      B           2010 4.0\n5      B           2010 3.2\n6      B           2015 1.8student.df %>% \n  group_by(school,graduationdate) %>%\n  summarize(mean.gpa=mean(gpa))\n`summarise()` has grouped output by 'school'. You can\noverride using the `.groups` argument.\n# A tibble: 4  3\n# Groups:   school [2]\n  school graduationdate mean.gpa\n  <chr>           <dbl>    <dbl>\n1 A                2010     3.45\n2 A                2015     2.9 \n3 B                2010     3.6 \n4 B                2015     1.8 \nstudent.df <- data.frame(school=c(\"A\", \"A\", \"A\", \"B\", \"B\", \"B\"),\n                        graduationdate=c(2010,2010,2010,2015,2015,2015),\n                        gpa = c(3.2,3.7,2.9,4.0,3.2,1.8))student.df %>% \n  group_by(school,graduationdate) %>%\n  summarize(mean.gpa=mean(gpa))\n# A tibble: 2  3\n# Groups:   school [2]\n  school graduationdate mean.gpa\n  <chr>           <dbl>    <dbl>\n1 A                2010     3.27\n2 B                2015     3   rr %>%\n  group_by(dataset,college) %>%\n  summarize(meanchildtot = mean(childtot, na.rm=T))\n# A tibble: 27  3\n# Groups:   dataset [14]\n   dataset college meanchildtot\n     <dbl>   <dbl>        <dbl>\n 1    1965       0         4.35\n 2    1965       1         3.66\n 3    1975       0         3.33\n 4    1975       1         3.51\n 5    1985       0         2.89\n 6    1985       1         3.13\n 7    1993       0         2.25\n 8    1993       1         1.74\n 9    1995       0         4.39\n10    1995       1         3.85\n#  with 17 more rows\ntotchildbyyearcollege <- rr %>%\n  group_by(dataset, college) %>%\n  summarize(meanchildtot = mean(childtot, na.rm=T))"},{"path":"the-rug-rat-race.html","id":"conclusion-5","chapter":"7 The Rug Rat Race","heading":"7.9 Conclusion","text":"final section put various functions learned together one analysis. reminder, goal compute average time spent childcare mothers time, separately college-education status. words, replicate Figure ?? Ramey Ramey (2010) shows increased time spent childcare beginning 1990s, especially college-educated mothers.review focus tidyverse package. reminder, first time using package need install using install.packages(\"packagename\"). need . However, every time want use package given R session, need load memory using library() function. example, load tidyverse type:begin, first re-load data R tibble using read_csv() function. read_csv() function reads dataset tibble, read.csv() finction reads dataset memory data frame. purposes, much differences formats, tibbles efficient large datasets.first issue Ramey Ramey (2010) analysis particular group individuals: mothers aged 25-34. Therefore, first need create dataset restricted individuals. can achieve using filter() function.Now, focus analysis driving increase time spent childcare. One potential driver increased time spent college prep activities, given increase competitiveness college time. going add variable named collegeprep dataset time spent either traveling children time spent educating children.goal compute amount time spent collegeprep time, college education status. can accomplish using group_by() summarize() functions together.Now look resulting tibble, can use understand trends time spent college prep time:Going forward book, tools learned section important. extremely common need perform data wrangling steps proceeding actual analysis. tidyverse package given us convenient set functions order perform analysis.","code":"\nlibary(tidyverse)rr <- read_csv(\"rugratrace.csv\")\nRows: 106020 Columns: 54\n Column specification \nDelimiter: \",\"\ndbl (54): state, age, sex, ethnic, under18, under5, agey...\n\n Use `spec()` to retrieve the full column specification for this data.\n Specify the column types or set `show_col_types = FALSE` to quiet this message.\nmothers2534 <- filter(rr, mother2==1, age>24, age<35)\nmothers_collegeprep <- mutate(mothers2534, \n                              collegeprep = childeduc + childtravel)collegeprep <- mothers_collegeprep %>%\n  group_by(dataset, college) %>%\n  summarize(meancollegeprep=mean(collegeprep, na.rm=T))\n`summarise()` has grouped output by 'dataset'. You can\noverride using the `.groups` argument.collegeprep <- rr %>%\n  filter(mother2==1, age>24, age<35) %>%\n  mutate(collegeprep = childeduc + childtravel) %>%\n  group_by(dataset, college) %>%\n  summarize(meancollegeprep=mean(collegeprep, na.rm=T))\n`summarise()` has grouped output by 'dataset'. You can\noverride using the `.groups` argument."},{"path":"the-rug-rat-race.html","id":"function-descriptions","chapter":"7 The Rug Rat Race","heading":"Function Descriptions","text":"Tidyverse Functionsread_csv() -- Reads csv memory tibble. tibble similar data frame, efficient many settings.read_csv() -- Reads csv memory tibble. tibble similar data frame, efficient many settings.as_tibble() -- Converts data frame tibble.as_tibble() -- Converts data frame tibble.filter() -- Restricts certain observations data set. example, filter(df, age>18) restrict hypothetical data frame df individuals age>18.filter() -- Restricts certain observations data set. example, filter(df, age>18) restrict hypothetical data frame df individuals age>18.arrange() -- Sorts data based value variable. example, arrange(df, age) sort dataset df youngest oldest.arrange() -- Sorts data based value variable. example, arrange(df, age) sort dataset df youngest oldest.select() -- Selects certain variables dataset. can used restrict variables needed analysis.select() -- Selects certain variables dataset. can used restrict variables needed analysis.mutate() -- used generate new variables.mutate() -- used generate new variables.group_by() -- Groups data based values variable. subsequent commands done separately values variable.group_by() -- Groups data based values variable. subsequent commands done separately values variable.summarize() -- Used generate summary statistics. Often combined group_by() order generate summary statistics groups.summarize() -- Used generate summary statistics. Often combined group_by() order generate summary statistics groups.Functionssample() -- Draws random number list numbers. example, sample(-1:1,1) draws single number list numbers -1:1. words, draw random number -1 1. sample(c(0,1,2),4) draw 4 random numbers list numbers 0,1, 2.sample() -- Draws random number list numbers. example, sample(-1:1,1) draws single number list numbers -1:1. words, draw random number -1 1. sample(c(0,1,2),4) draw 4 random numbers list numbers 0,1, 2.cat() -- Concatenates text R. example, cat(\"Hello\",\"World\") yield phrase \"Hello World\".cat() -- Concatenates text R. example, cat(\"Hello\",\"World\") yield phrase \"Hello World\".unique() -- Retrieves list unique values variable takes .unique() -- Retrieves list unique values variable takes .Installing/Loading Functionsinstall.packages() -- Installs package onto version R. example, install.packages(\"tidyverse\") install tidyverse R. need install package .install.packages() -- Installs package onto version R. example, install.packages(\"tidyverse\") install tidyverse R. need install package .library() -- Loads package memory.library() -- Loads package memory.","code":""},{"path":"automated-pollution-monitoring.html","id":"automated-pollution-monitoring","chapter":"8 Automated Pollution Monitoring","heading":"8 Automated Pollution Monitoring","text":"","code":""},{"path":"automated-pollution-monitoring.html","id":"principal-agent-problem","chapter":"8 Automated Pollution Monitoring","heading":"8.1 Principal Agent Problem","text":"economics political science, concept known principal agent problem. setting arises agent making decisions behalf principal, agent principal's incentives may align.classic example employees firm. employee putting amount effort produce output firm owner. firm owner may want produce much output possible, employee may balance output amount effort takes produce output. Therefore, incentives necessarily align.Another classic example elected officials citizenry. officials make decisions (.e. agents) acting behalf citizens (.e. principal). However, goal elected officials may re-elected, necessarily make optimal decisions citizens. Therefore, setting well, possible incentives necessarily aligned agent principal.section, discussing interesting setting principal agent problem play. China, central government (principal) recently made priority reduce air pollution China. accomplish , provide high-power incentives local governments (agent) achieve pollution targets. However, local government officials also ones collecting pollution data. may costly reduce air pollution, one possibility can cheat report pollution numbers lower actual pollution level.Greenstone et al. (2022)11 studies question examining introduction automatic air pollution monitoring China. automated system implemented order get accurate measures pollution manipulated local officials. basically two potential outcomes analysis. First, local officials reporting accurate numbers. case, expect automatic monitoring change reported pollution numbers. numbers accurate policy, therefore, expect levels similar policy.second possibility reported levels pollution increase automatic monitoring. case, suggests local governments misreporting automatic monitoring appear meeting pollution targets.order measure pollution use reported PM10. measure particulates air 10 micrometers diameter smaller, common metric measure air pollution. focus coding week creating figures R. create figures base R, well utilizing popular graphics package, ggplot2. end, use figures understand whether local officials misreported pollution data China.","code":""},{"path":"automated-pollution-monitoring.html","id":"creating-histograms","chapter":"8 Automated Pollution Monitoring","heading":"8.2 Creating Histograms","text":"section, going learn construct histograms base R. begin, first need load data frame. However, data using station_day_1116.dta Stata dataset. past, loaded either RData csv files.order load Stata data set going make use haven package. order install haven package, need execute install.packages(\"haven\"). , current R session need load package using library() function. Since utilizing tidyverse section, also load tidyverse package.haven package comes function read_dta, read .dta file tibble.first get sense data, look key variables dataset.first variable, pm10_n station number. code station took pollution reading. next variable code_city code city pollution reading taken. dataset panel dataset, means many observations time. case, readings pollution center time. date variable stores information date reading taken. discuss bit later number variable means. pm10 measure pollution. Lastly, auto_date date automation. date, instead local governments reporting pollution statistics, pollution statistics directly reported central government.section goal create histogram shows distribution pm10 across cities. Right now, dataset station-date level. row corresponds pollution level particular station. want transform data dataset every city, average level pollution city.way can use group_by() summarize(), prior chapter. recall, want create tibble summary statistics group, use group_by() combination summarize(). example, group variable code_city summary statistic average level pm10.included na.rm=T, looking pm10 variable, appears missing data. Adding na.rm=T indicates taking average non-missing values.now, unique value code_city, average PM10 level city. dataset use order generate histogram.generate histograms base R, can use hist() function. example, generate histogram pm10:\nFigure 8.1: Histogram Average Pollution Levels Across Cities Part 1\nFirst, review histogram. horizontal axis, numeric variable meanpm10, average level pm10. histogram binned variable discrete intervals. R automatically creates bins use hist() function. , vertical axis, number cities fall within interval presented. , example, look tallest bin, 80-100, can see 35 cities average pollution range.presents information, lot can order improve figure. example, title right now clear. reader may know pm_bycity$meanpm10 means. usual, want improve labels.change axis labels, can use xlab=\"Horizontal Axis Name\" ylab=\"Vertical Axis Name\". add title, can use main=\"Title Name\". main perhaps intuitive name options generates title, can think \"main subject plot\". try re-create plot better axes labels titles.\nFigure 8.2: Histogram Average Pollution Levels Across Cities Part 2\nvarious options useful many different types graphs base R, just histograms. example, later generate scatter plot. can use options generating alternate type graph.Next, discuss ways can customize look plot. example, histograms, one common way customize plot change number bins created. R chooses default number bins, can manually change using breaks option. specify higher number breaks, means want bins. specify lower number, means want fewer bins.illustrate , going put two plots side--side. Learning put two plots side--side generally useful technique well. Often academic articles, might see multiple panels figure information panels closely related. example, going create two plots, one many bins another bins.way tell R create multiple plots use function called par(). Within par() can use argument called mfrow=. mfrow= tells R many rows many columns want plot. example, example, want single row, two separate figures. can think row figures, two columns figures. specify R, can type mfrow=c(1,2).\nFigure 8.3: Histogram Average Pollution Levels Across Cities Part 3\ncan see, specify breaks=20, histogram function generates many bins. specified breaks=4, retrieve 4 bins.generally trade number bins create. left plot, get fine-grained information distribution pm10 across cities. right, information aggregated, perhaps easier read. many cases, default number bins good choice.","code":"\nlibrary(haven)\nlibrary(tidyverse)\n#>  Attaching packages  tidyverse 1.3.2 \n#>  ggplot2 3.4.0       purrr   0.3.5 \n#>  tibble  3.1.8       dplyr   1.0.10\n#>  tidyr   1.2.1       stringr 1.4.1 \n#>  readr   2.1.3       forcats 0.5.2 \n#>  Conflicts  tidyverse_conflicts() \n#>  dplyr::filter() masks stats::filter()\n#>  dplyr::lag()    masks stats::lag()\npm <- read_dta(\"station_day_1116.dta\")\npm %>% select(pm10_n,code_city,date,pm10,auto_date)\n#> # A tibble: 1,433,568  5\n#>    pm10_n code_city  date  pm10 auto_date\n#>     <dbl>     <dbl> <dbl> <dbl>     <dbl>\n#>  1     46    440300 18628  NA       19060\n#>  2     46    440300 18629  NA       19060\n#>  3     46    440300 18630  NA       19060\n#>  4     46    440300 18631  NA       19060\n#>  5     46    440300 18632  NA       19060\n#>  6     46    440300 18633  NA       19060\n#>  7     46    440300 18634  70.1     19060\n#>  8     46    440300 18635 114.      19060\n#>  9     46    440300 18636  62.5     19060\n#> 10     46    440300 18637  90.8     19060\n#> #  with 1,433,558 more rows\npm_bycity <- pm %>%\n  group_by(code_city) %>%\n  summarize(meanpm10 = mean(pm10, na.rm=T))\nhist(pm_bycity$meanpm10)\nhist(pm_bycity$meanpm10, \n     xlab=\"Mean PM10\", \n     ylab=\"Frequency\", \n     main=\"Mean PM10 by City\")\npar(mfrow=c(1,2))\n\n#Smaller bins\nhist(pm_bycity$meanpm10, xlab=\"Mean PM10\", \n     ylab=\"Frequency\", main=\"Mean PM10 by City\",\n     breaks=20)\n\n#Larger bins\nhist(pm_bycity$meanpm10, xlab=\"Mean PM10\", \n     ylab=\"Frequency\", main=\"Mean PM10 by City\",\n     breaks=4)"},{"path":"automated-pollution-monitoring.html","id":"comparing-histograms","chapter":"8 Automated Pollution Monitoring","heading":"8.3 Comparing Histograms","text":"section discuss compare two histograms. particular, plot distribution pollution levels (PM10) implementation automatic monitory.construct histograms, need compute data frame contains average pollution levels city, automation. can accomplish first constructing variable T contains days automation. can filter days automation (T<0). Similarly, can create post-automation data frame filtering observations T>0. can accomplish one step using pipe operator:make sure understand line code. pm %>% pipes data frame pm. mutate(T=date-auto_date) creates new variable data frame. name new variable T. T negative, means date less auto_date. Next filter(T<0) implies restricting observations automation date. Lastly, group_by(code_city) %>% summarize(meanpm10=mean(pm10, na.rm=T)) implies taking averages pm10 city. end, dataset observation separate city, average level pm10 city automation. can use analogous code construct city-level dataset average level pm10 automation.Next, create two plots lie side--side. order , specify par(mfrow=c(1,2)) generating plots.plots easy compare two distributions. One reason scale. can see, x-axis Automation plot goes 200, Automation plot goes 150. order two directly comparable, ensure x-axis y-axis scale. can using xlim() ylim() functions.Now can clearly compare two plots. automation, many pm10 averages seem clustered just 100. automation, distribution sspread , many observations 100, even 200. automation, city average level pm10 200. Therefore, appears distribution reported pm10 shifted right automation.Lastly, can also add additional information plot. example, might helpful quickly able understand average level pm10 varies automation. can add numbers plots using lines() function. way lines function works specify \\((x_1, x_2)\\) \\((y_1,y_2)\\). line starts point \\((x_1,y_1)\\) ends \\((x_2,y_2)\\). Therefore, want vertical line mean can specify lines(c(mean,mean),c(-10,100)). specified c(-10,100) numbers beyond limits graph. Therefore, vertical line appear along entire graph output. understand build lines graphs, helpful change numbers code see results change.","code":"\npm_bycitybefore <- pm %>%\n  mutate(T=date-auto_date) %>%\n  filter(T<0) %>%\n  group_by(code_city) %>%\n  summarize(meanpm10 = mean(pm10, na.rm=TRUE))\npm_bycityafter <- pm %>% \n  mutate(T=date-auto_date) %>%\n  filter(T > 0) %>%\n   group_by(code_city) %>%\n  summarize(meanpm10 = mean(pm10, na.rm=TRUE))\npar(mfrow=c(1,2))\n\n#Plot histogram for before\nhist(pm_bycitybefore$meanpm10, xlab=\"MeanPM10\", \n  ylab=\"Frequency\",\n  main=\"Before Automation\")\n\n#Plot histogram for after\nhist(pm_bycityafter$meanpm10, xlab=\"MeanPM10\", \n  ylab=\"Frequency\",\n  main=\"After Automation\")\n#Create two panes for plots\npar(mfrow=c(1,2))\n#Plot histogram for before\nhist(pm_bycitybefore$meanpm10, xlab=\"MeanPM10\", \n  ylab=\"Frequency\",\n  main=\"Before automation\",\n  xlim=c(0,250), ylim=c(0,50))\n#Plot histogram for after\nhist(pm_bycityafter$meanpm10, xlab=\"MeanPM10\", \n  ylab=\"Frequency\",\n  main=\"After automation\",\n  xlim=c(0,250), ylim=c(0,50))\nmeanbefore <- mean(pm_bycitybefore$meanpm10)\nmeanafter <- mean(pm_bycityafter$meanpm10)\n#Create two panes for plots\npar(mfrow=c(1,2))\n#Plot histogram for before\nhist(pm_bycitybefore$meanpm10, xlab=\"MeanPM10\",\n  ylab=\"Frequency\",\n  main=\"Before Automation\",\n  xlim=c(0,250), ylim=c(0,50))\nlines(c(meanbefore, meanbefore), c(-10, 100), \n  lty=2, col=\"red\")\n#Plot histogram for after\nhist(pm_bycityafter$meanpm10, xlab=\"MeanPM10\", \n  ylab=\"Frequency\",\n  main=\"After Automation\",\n  xlim=c(0,250), ylim=c(0,50))\nlines(c(meanafter, meanafter), c(-10, 100), \n  lty=2, col=\"red\")"},{"path":"automated-pollution-monitoring.html","id":"boxplots","chapter":"8 Automated Pollution Monitoring","heading":"8.4 Boxplots","text":"box plot figure yet seen course. Like histogram, informative distribution variable. particular, box plot shows key statistics interest, including median, 25th precentile 75th percentile. often helpful want show distribution variable varies across different groups. example, maybe interested variation test scores across different classes school. Maybe interested distribution wages across different regions U.S. types instances, box plots might effective way present information.example, going construct box plot shows average daily pm10 levels automation. begin, going use group_by() summarize() construct relevant dataset.example, T days automation. box plot, restricting within year automation date filter(T>-364, T<364). grouped T, resulting dataset single observation per day, corresponding variable average level pollution day. construct box plot can use boxplot() function:box plot elements discuss. First, upper end box 75th percentile (also referred third quartile). box plot, 75th percentile 112. bottom box 25th percentile (also referred 1st quartile). box plot, 25th percentile 80. length box referred interquartile range. difference 75th percentile 25th percentile, example interquartile range 112-80=32.dashed lines extend box referred whiskers. length whiskers equal \\(1.5 \\cdot IQR\\). example, whiskers extend \\(32 \\cdot 1.5 = 48\\). However, whiskers extend length long data range. example, theoretically, bottom whisker extend \\(80-48=32\\). reason figure data point. Instead, whisker extends minimum, 53 figure.can see, upper whisker longer extends \\(112+48=160\\). , can see dots beyond whisker. signifies data beyond values reached whisker. dots give us sense many outliers data.Just figures, can also improve aesthetics box plot various ways. example, box plot adds appropriate labels, changes teh colors box plot, well changes orientation box plot.actually relatively uncommon see single box plot. often useful, however, display box plots across different groups. Therefore, next figure separate box plots automation.First, need add variable indicates whether time automation.plot separate box plots value , specify pm_byday$meanpm10 ~ pm_byday$inside boxplot function, rather just pm_byday$meanpm10.can tell figure? Well, automation pollution levels seem higher. 25th percentile, median, 75th percentile higher. Additionally, minimum maximum levels also shifted dramatically. , minimum pollution level around 53, closer 75. , maximum level topped just 140. day pollution levels almost 200. appears large increase reported pollution levels automation, providing evidence local governments -reported pollution levels automatic monitoring.","code":"\n#Group by day\npm_byday <- pm %>% \n  mutate(\n    T=date-auto_date) %>%\n  filter(T>-364, T<364) %>%\n  group_by(T) %>%\n  summarize(meanpm10 = mean(pm10, na.rm=TRUE))\nboxplot(pm_byday$meanpm10)\nboxplot(pm_byday$meanpm10, xlab=\"Mean PM10\", \nmain=\"Mean PM 10 by city\",\n  col=\"blue\", border=\"darkblue\", \n  pch=16, horizontal=T)\npm_byday$after <- pm_byday$T>0\nboxplot(pm_byday$meanpm10 ~ pm_byday$after, \n        xlab=\"Mean PM10\", ylab=\"After Automation\",\n        main=\"Mean PM 10 by Day Before and After Automation\", \n        col=\"blue\", border=\"darkblue\", \n        pch=16, horizontal=T)"},{"path":"automated-pollution-monitoring.html","id":"scatterplots","chapter":"8 Automated Pollution Monitoring","heading":"8.5 Scatterplots","text":"section analyze pollution changes day--day around automation. order , plot daily pollution levels around time automation. Therefore, continue use data set pm_byday includes average pollution levels across cities every day within year automated pollution monitoringThe general syntax scatterplot :want plot x-axis depicts day relative automation y-axis depicts average pollution levels. Therefore, replace df$xvar pm_bydat$T df$yvar pm_byday$meanpm10. plots add labels clarify plot.many ways can customize plot. example, change markers appear . pch stands plot character. make circles solid instead hollow, can specify pch=16.particularly helpful reader understand automation occurring. Therefore, add dashed line 0 (time automation occurs) add additional text order label line. can add lines lines() function. add text need specify (1) place text (2) text appear. example, place text (x=0,y=260), type text(0,260, \"Text \"). try scatter plot additionally show change color text.Finally, discuss actually save plots. Saving plots consists three steps:\n1. Use pdf() png() create file plot created\n2. Run code create plot\n3. Type dev.() tell R done creating plots.example, say want save PDF plot want named Automation.pdf. first step execute code:create file named Automation.pdf current working directory. eventually hold plot. Next, need re-run code create plot:Now plot generated, need execute:forget execute dev.() next plot execute also written Automation.pdf.","code":"\nplot(df$xvar, df$yvar)\nplot(pm_byday$T, pm_byday$meanpm10,\n    xlab=\"Days Relative to Automation\",\n    ylab=\"Mean PM10\", \n    main=\"Automation and Mean PM10\",)\nplot(pm_byday$T, pm_byday$meanpm10,\n    xlab=\"Days Relative to Automation\",\n    ylab=\"Mean PM10\", \n    main=\"Automation and Mean PM10\",\n    pch=16)\nplot(pm_byday$T, pm_byday$meanpm10, \n     xlab=\"Days After Automation\",\n     ylab=\"Mean PM10\", main=\"Automation and Mean PM10\",\n     pch=16, ylim=c(0,300))\nlines(c(0,0), c(-10,250), col=\"red\", lty=2)\ntext(0,260, \"Automation\", col=\"red\")\npdf(\"Automation.pdf\")\nplot(pm_byday$T, pm_byday$meanpm10, \n     xlab=\"Days After Automation\",\n     ylab=\"Mean PM10\", main=\"Automation and Mean PM10\",\n     pch=16, ylim=c(0,300))\nlines(c(0,0), c(-10,250), col=\"red\", lty=2)\ntext(0,260, \"Automation\", col=\"red\")\ndev.off()"},{"path":"automated-pollution-monitoring.html","id":"dates","chapter":"8 Automated Pollution Monitoring","heading":"8.6 Dates","text":"Dates can often difficult work performing data analysis. section going study package called lubridate can helpful working dates R.lubridate() new package, first need install lubridate via install.packages(\"lubridate\") load memory using library() function:lubridate() comes variety functions. example, functions tell day today:even exact time right now:importantly, lubridate allows R interpret strings text dates. us, means make graph R understand observation January 1, 2012 taken observation taken March 3rd, 2014, example.Additionally, lubridate() can interpret dates come variety formats. example, can use ymd() function convert string text holds date \"year-month-day\" date object.also month-day-year function:day-month-year function:also important can add days times lubridate functions. example, 12 days January 22nd 2012 :15 months :5 years :Often date actually narrow need analysis. example, maybe need year portion date variable. can extract portion using year() function:similar functions extracting month(month()), day month (mday()), day week wday().Now sense lubridate() load return pollution monitoring data frame pm. data frame, look variable date.look intuitive. 18628. Well, actually artifact Stata stores dates (recall originally read data frame R via Stata dataset). stores dates number days since January 1, 1960.words, 18628 represents observation taken 18628 days January 1, 1960. can figure day exactly using functions lubridate:18628 days January 1, 1960 January 1, 2011.Next, going study rainfall time. Rain may actually reduce amount pollution air. rain falls, attracts particles air hitting ground., turn reducing amount pollution air. Therefore, may expect see natural changes pollution throughout year due changes amount rain. understand seasonality rainfall China, going make plots show rainfall varies time.First, get sense rainfall varies month. create data frame month year, average level rain. variable rain data frame Daily Rain mm. create data frame going utilize group_by() summarize().first part code rdate = ymd(\"1960-01-01\") + days(date) creates new variable named rdate captures date observation. next month=month(rdate) extracts month observation taken. rest code summarizes amount rainfall month. end, data frame 12 observations, month associated average value rain fall. now plot values time.China experiences much rainfall May September (.e. months 5 9). Therefore, might expect less pollution months. can also see varies day--day. Instead aggregating month, generate data frame shows amount Daily Rain across China day.Now plot rainfall time:looking rain ? Well, automatic monitoring happened implemented end rainy season? case, might expect see increased pollution levels automatic monitoring due changing seasons. words, rain fall potential confounding factor. need check change pollution levels around automatic pollution monitoring driven changing seasons. return question later section.","code":"\nlibrary(lubridate)\n#> Loading required package: timechange\n#> \n#> Attaching package: 'lubridate'\n#> The following objects are masked from 'package:base':\n#> \n#>     date, intersect, setdiff, union\ntoday()\n#> [1] \"2023-01-31\"\nnow()\n#> [1] \"2023-01-31 08:06:20 PST\"\nymd(\"2012-01-22\")\n#> [1] \"2012-01-22\"\nmdy(\"January 22nd, 2012\")\n#> [1] \"2012-01-22\"\ndmy(\"22-Jan-2012\")\n#> [1] \"2012-01-22\"\nymd(\"2012-01-22\") + days(12)\n#> [1] \"2012-02-03\"\nymd(\"2012-01-22\") + months(15)\n#> [1] \"2013-04-22\"\nymd(\"2012-01-22\") + years(5)\n#> [1] \"2017-01-22\"\nyear(ymd(\"2012-01-22\"))\n#> [1] 2012\nhead(pm$date)\n#> [1] 18628 18629 18630 18631 18632 18633\nymd(\"1960-01-01\")+days(18628)\n#> [1] \"2011-01-01\"\npm_bymonth <- pm %>%\n  mutate(rdate = ymd(\"1960-01-01\") + days(date),\n    month = month(rdate)) %>%\n    group_by(month) %>%\n    summarize(meanrain = mean(rain, na.rm=TRUE))\nplot(pm_bymonth$month, pm_bymonth$meanrain, \n     col=\"blue\",\n     pch=16, \n     xlab=\"Date\", \n     ylab=\"Mean Daily Rain (mm)\")\npm_byrdate <- pm %>%\n  mutate(rdate = ymd(\"1960-01-01\") + days(date)) %>%\n  group_by(rdate) %>%\n  summarize(meanrain = mean(rain, na.rm=TRUE))\nplot(pm_byrdate$rdate, pm_byrdate$meanrain, \n     col=\"blue\",\n     pch=16, \n     xlab=\"Date\", \n     ylab=\"Mean Daily Rain (mm)\")"},{"path":"automated-pollution-monitoring.html","id":"ggplot2","chapter":"8 Automated Pollution Monitoring","heading":"8.7 ggplot2","text":"ggplot2 popular graphics package R. two major strengths. First, can use ggplot2 create variety figures. Second, syntax create different types figures similar. Therefore, learn generate figures, able quickly learn create many different types figures. \"gg\" ggplot2 stands grammar graphics, goal ggplot2: consistent language build figures.philosophy ggplot2 want build graph layers. first layer always data, always need specify data going plotted. specify aesthetics, purposes can think variables plot. variable horizontal axis? variables vertical axis. , specify type plot. bar graph? line plot. ggplot2, type graph determined geometric function use. basic syntax ggplot2 command components shown :replace section <DATA> whatever data using. next example, recreate scatterplot shows average daily pollution levels relative automation date. recall, already constructed dataframe named pm_byday contains every day relative automation (within year), average levels pollution, measured PM10. However, example later section, also utilize variable captures mean rain day. Therefore, construct day-level dataset contains day relative automation, average levels PM10 rain:example, replace <DATA> pm_byday.<GEOM_FUNCTION> determine type plot appears. example, geom_point geometric function create scatterplot. section, also discuss geom_boxplot geom_histogram.mapping = aes(<MAPPINGS>) tells R variables appear graph. case, want x-axis show variable T. recall, T represents day relative automation. negative number indicates automation yet occurred, positive indicates already occurred. y-axis want display mean PM10 level, stored variable meanpm10. Therefore, way specify aesthetics mapping=aes(x=T,y=meanpm10)., many ways customize plot. next code, specify color=blue markers, well add axes labels overall title.Finally, can even color dots different variables. example, day relative automation, can color marker much rain . way, can see automation date occurred around change rain season. , variation pollution may driven variation rain.graph, days lot rain colored light blue, days less rain colored dark blue. can see, around automation, dots mostly dark blue. indicates period relatively less rain. However, seem stark change just around time automation. words, amount rain seems relatively stable just just automation pollution monitoring.Next, going visualize data alternative way: using box plot. use geom_boxplot function.shows us median level pollution, 25th percentile 75th percentile overall, real goal understand statistics change automation. split graph variable equal 1 automation already occurred, zero occurred yet.","code":"ggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\n }\npm_byday <- pm %>%\n  mutate(T=date-auto_date) %>%\n  filter(T>-364, T<364) %>%\n  group_by(T) %>%\n  summarize(meanpm10 = mean(pm10, na.rm=TRUE),\n    meanrain=mean(rain, na.rm=TRUE)) %>%\n  mutate(after=T>0)\nggplot(data = pm_byday) + \n  geom_point(mapping = aes(x = T, y = meanpm10))\nggplot(data = pm_byday) + \n  geom_point(mapping = aes(x = T, y = meanpm10), color=\"blue\") +\n  xlab(\"Time After Automation\") + \n  ylab(\"Mean PM10\") + \n  ggtitle(\"Time After Automation and PM10\")\nggplot(data = pm_byday) +\n  geom_point(mapping = aes(x = T, y = meanpm10,color=meanrain)) +\n  xlab(\"Time After Automation\") + ylab(\"Mean PM10\") +\n  ggtitle(\"Time After Automation and PM10\")\nggplot(data=pm_byday) + \n  geom_boxplot(mapping=aes(y=meanpm10))\nggplot(data=pm_byday) + \n  geom_boxplot(mapping=aes(x=after,y=meanpm10))"},{"path":"automated-pollution-monitoring.html","id":"bar-plots","chapter":"8 Automated Pollution Monitoring","heading":"8.8 Bar Plots","text":"section going discuss construct bar plots. going focus one aspect Greenstone et al. (2022) discussed depth yet. particular, going study automation rolled .far, discussed automation date: date pollution levels automatically reported central government. Different cities, however, different automated pollution dates. implemented policy first certain set cities, second another set cities. section, going try figure logic implementation . particular, target high-polluting cities first place implement automated pollution monitoring.first thing create city-level dataset. similar city-level datasets already created, also include variable phase city included automation. two possible values phase, 1 first automation wave 2 second wave automation.Note code restricted period time automation (filter(T<0)). function unique(phase) takes unique value phase city (value vary within city). look resulting data frame:Next, create bar plot using ggplot shows number cities phase 1 vs. phase 2. continue use syntax learned ggplot. looking figure, look code generate figure:phase either 1 2, want R understand phase categorical variable. default, R interpret continuous variable. common bar plots. need tell R showing statistics across groups (case groups unique values phase variable).specified stat=\"count\" example, want display total number cities value phase. words, want count number cities phase. Now understand code, look figure .can see figure, half cities sample (60) phase 1 automation, slightly half phase 2. need next improve appearence graph. label .factor(phase) particularly helpful reader. add custom labels axes.introduced new things code . First, specified fill=\"darkblue\". tells ggplot instead gray bars, want bars dark blue. Next, also specified theme_minimal(). nice thing ggplot also comes number themes can change overall look graph. theme_minimual() makes plot look minimalistic, taking background.Next, going study whether cities polluted likely first wave. use threshold 80 define whether city heavily polluted . add variable named above80 data frame:Now, can use variable add another dimension bar plot. particular, can show fraction automation phase 1 pollution levels 80 vs. fraction pollution levels 80. , take code fill=\"darkblue\" add fill=above80 inside aes().slight evidence high-polluting cities likely wave 1. However, reasonably even split. number high-polluting cities waves automation.Next, explore customizations. First, plotted referred stacked bar plot. divided cities four groups: (phase 1 + high polluters), (phase 1 + low polluters), (phase 2 + high polluters), (phase 2 + low polluters). phase, high low polluters stacked atop . can instead specify show counts separately four groups.use option position = position_dodge() inside geom_bar().way displaying data may little bit easily visualize certain cases. last customization make change legend. can change text legend, well position graph. change position, can use add theme(legend.position = \"top\") graph. change text legend, can use scale_fill_discrete() option. example, imagine want title legend \"Mean PM10 80\" also want flip order \"TRUE\" appears \"FALSE\". code accomplishes goals specifying proper elements inside scale_fill_discrete().","code":"\npm_bycity <- pm %>%\n  mutate(T = date - auto_date) %>%\n  filter(T < 0) %>%\n  group_by(code_city) %>%\n  summarize(meanpm10 = mean(pm10, na.rm=TRUE),\n            phase = unique(phase))\nhead(pm_bycity)\n#> # A tibble: 6  3\n#>   code_city meanpm10 phase\n#>       <dbl>    <dbl> <dbl>\n#> 1    110100    108.      1\n#> 2    120100     93.3     1\n#> 3    130100     92.5     1\n#> 4    130200     84.8     1\n#> 5    130300     66.4     1\n#> 6    130400     94.7     1\nggplot(pm_bycity, aes(x=factor(phase))) + \n  geom_bar(stat=\"count\")\nggplot(pm_bycity, aes(x=factor(phase))) + \n  geom_bar(stat=\"count\", fill=\"darkblue\") + \n  xlab(\"Wave of Automation\") + \n  ylab(\"Number of Cities\") + \n  ggtitle(\"Number of Cities by Wave of Automation\") + \n  theme_minimal()\npm_bycity$above80 <- pm_bycity$meanpm10 > 80\nggplot(pm_bycity, aes(x=factor(phase),fill=above80)) + \n  geom_bar(stat=\"count\") + \n  xlab(\"Wave of Automation\") + \n  ylab(\"Number of Cities\") + \n  ggtitle(\"Number of Cities by Wave of Automation\") + \n  theme_minimal()\nggplot(pm_bycity, aes(x=factor(phase),fill=above80)) + \n  geom_bar(stat=\"count\",position = position_dodge()) + \n  xlab(\"Wave of Automation\") + \n  ylab(\"Number of Cities\") + \n  ggtitle(\"Number of Cities by Wave of Automation\") + \n  theme_minimal()\nggplot(pm_bycity, aes(x=factor(phase), fill=above80)) + \n  geom_bar(stat=\"count\", position = position_dodge()) + \n  xlab(\"Wave of Automation\") + \n  ylab(\"Number of Cities\") + \n  ggtitle(\"Number of Cities by Wave of Automation\") + \n  theme_minimal() + theme(legend.position = \"top\") + \n  scale_fill_discrete(name=\"Mean PM10 Above 80\", \n                      limits=c(\"TRUE\", \"FALSE\"))"},{"path":"automated-pollution-monitoring.html","id":"conclusion-6","chapter":"8 Automated Pollution Monitoring","heading":"8.9 Conclusion","text":"section learned make variety plots, starting base R, moving onto ggplot2. final section, going put everything learned together. illustrate concepts, going create side--side histograms, first base R ggplot2.first review data frame began . started Stata dataset named station_day_1116.dta. order load data R used haven package.main goal study pollution measurements change response automatic pollution monitoring. China, local officials incentives manipulate pollution reports. Automated system implemented order get accurate measures pollution manipulated local officials. see pollution measurements increase around time automation, good evidence manipulation automated pollution monitoring took place.understand distribution pollution measurements going plot distribution mean PM10 day automated monitoring. going convert data frame day-level dataset average levels pollution per day.Recall, T=date-auto_date capture days relative automation. example value -10 indicate ten days automation. value 10 indicate ten days since automation. create side--side histograms using base R first generate two additional data frames, one automation one automation:Now, generate side--side histograms base R need use par(mfrow=c(1,2)), indicates one row plots 2 columns.Additionally, construct figure make sure (1) include proper labels, (2) make sure limits axes histograms (3) graph different color.Next, discuss make similar plot ggplot2. order , actually load external package gridExtra. install external package type install.packages(\"gridExtra\"). need load memory.Now, first thing going create histograms. Instead plot displayed immediately, save plot name. allow us combine plots together using gridExtra. convenient aspect using ggplot problem actually create intermediate data frame by_day_before by_day_after. can directly use pm_byday. start creating histogram automation.execute code , see plot appear Plots panel. plot stored object named plotbefore environment. type plotbefore execute code, see plot Plots window.One aspect code seen option alpha=0.2. parameter change transparency histogram. words, figure blue, slightly transparent. Values closer zero transparent. Adding transparency graphs can sometimes help, particularly overlapping features.Next, create plotafter. difference replace filter(==FALSE) filter(==TRUE).Now can combine two single plot.can see graphs, distribution pollution seems shifted right automation. appears local officials may misreporting pollution levels prior policy. specific finding important general lesson. many situations, one's making decisions (officials) may incentives trying implement policy (central government). principal-agent problem pervasive many settings, paper shown solution cases: technology. interested topic, make sure check Greenstone et al. (2022) results!","code":"\nlibrary(haven)\npm <- read_dta(\"station_day_1116.dta\")\npm_byday <- pm %>% \n  mutate(\n    T=date-auto_date) %>%\n  filter(T>-364, T<364) %>%\n  group_by(T) %>%\n  summarize(meanpm10 = mean(pm10, na.rm=TRUE),\n    meanrain = mean(rain, na.rm=TRUE)) %>%\n  mutate(after=T>0)\nby_day_before <- pm_byday %>%\n  filter(after==FALSE) \nby_day_after<- pm_byday %>%\n  filter(after==TRUE) \npar(mfrow=c(1,2))\nhist(by_day_before$meanpm10, col=\"blue\",\n  xlab=\"Mean PM10\", \n  main=\"Before Automation\",\n  xlim=c(0,200))\n\nhist(by_day_after$meanpm10, col=\"red\", \n  xlab=\"Mean PM10\", \n  main=\"After Automation\", \n  xlim=c(0,200))\nlibrary(gridExtra)\nplotbefore <- pm_byday %>%\n  filter(after==FALSE) %>%\n  ggplot(aes(x=meanpm10)) + \n  geom_histogram(fill=\"blue\", alpha=.2) + \n  xlab(\"Mean PM10\") + \n  ggtitle(\"Before Automation\") + \n  xlim(0,200)\nplotafter <- pm_byday %>%\n  filter(after==TRUE) %>%\n  ggplot(aes(x=meanpm10)) + \n  geom_histogram(fill=\"red\", alpha=.2) + \n  xlab(\"Mean PM10\") + \n  ggtitle(\"After Automation\") + \n  xlim(0,200)\ngrid.arrange(plotbefore, plotafter, ncol=2)\n#> Warning: Removed 2 rows containing missing values (`geom_bar()`).\n#> Removed 2 rows containing missing values (`geom_bar()`)."},{"path":"automated-pollution-monitoring.html","id":"function-descriptions-1","chapter":"8 Automated Pollution Monitoring","heading":"Function Descriptions","text":"Packages Usedtidyverse -- Includes suite packages, including ggplot2 used graphics chapter.tidyverse -- Includes suite packages, including ggplot2 used graphics chapter.haven -- Allows users read Stata files R.haven -- Allows users read Stata files R.lubridate() -- package useful handling variables contain dates R.lubridate() -- package useful handling variables contain dates R.gridExtra() -- Allows user put ggplot2 plots side--sidegridExtra() -- Allows user put ggplot2 plots side--sideBase R plot functionshist() -- Generates histogram.hist() -- Generates histogram.boxplot() -- Generates boxplot.boxplot() -- Generates boxplot.plot() -- Generates scatterplot.plot() -- Generates scatterplot.par(mfrow=c(r,c)) -- Allows user place base R plots grid. r number rows c number columns. example, par(mfrow=c(1,2)) imply 1 row 2 columns plots.par(mfrow=c(r,c)) -- Allows user place base R plots grid. r number rows c number columns. example, par(mfrow=c(1,2)) imply 1 row 2 columns plots.ggplot2 Geometric functionsgeom_histogram() -- Generates histogram.geom_histogram() -- Generates histogram.geom_boxplot() -- Generates boxplot.geom_boxplot() -- Generates boxplot.geom_point() -- Generates scatterplotgeom_point() -- Generates scatterplotgeom_bar() -- Generates barplot.geom_bar() -- Generates barplot.","code":""},{"path":"voting.html","id":"voting","chapter":"9 Voting","heading":"9 Voting","text":"","code":""},{"path":"voting.html","id":"u.s.-presidential-election","chapter":"9 Voting","heading":"9.1 2000 U.S. Presidential Election","text":"chapter, discuss two different voting applications. first empirical application study 2000 U.S. Presidential Election. election extremely close. end, George W. Bush won election 271 electoral votes Al Gore's 266.Florida, election particularly close. end, Bush won Florida just 537 votes, thus secured Florida's 25 electoral votes election. However, Florida, one county's results became particularly controversial: Palm Beach County. Many voters county complained confusing ballot. claimed layout made accidentally voted Pat Buchanan (Reform Party Candidate) rather Al Gore.Figure 9.1 shows Ballot used Palm Beach County:\nFigure 9.1: Butterfly Ballot\ntop left, can see first listed candidates George W. Bush Dick Cheney (Republican Candidates). next candidates actually Pat Buchanan Ezola Foster (Reform Party Candidates). However, filling quickly, can see someone might accidentally think second selection corresponds Al Gore Joe Lieberman.enough people made mistake, actually consequences wins election. Remember, 537 votes separated George W. Bush Al Gore.understand scope problem, going replicate analysis Wand et al. (2001)12. Specifically, voters confused ballot, expect unusually high number votes Reform party candidates Palm Beach.\ndefine \"unsually high number votes\". operationalize idea use number votes county Reform party 1996 (prior election) predict many votes county cast Reform party 2000. Palm Beach County many votes predicted, maybe Butterfly ballot reason .begin exploring question, load data.row data county Florida. row, counts different presidential candidates 1996 2000. example, 1996, 40,144 votes cast Alachua county Bill Clinton. 25,303 cast Bob Dole. note application data drawn Kosuke Imai's Textbook: Quantitative Social Science: Introduction.13Our eventual goal use votes cast Ross Perot (Reform Party Candidate) 1996, predict many votes county cast Pat Buchanan 2000 (Reform Party Candidate).logic behind design might think counties relatively consistent voting patterns. Maybe Palm Beach County county happens heavily support Reform Party Candidates. true, see high number votes Reform Party Candidates 1996 2000.begin exploration, visualize relationship voting Ross Perot 1996 Pat Buchanan 2000. construct scatter plot Perot96 horizontal axis Buchanan00 vertical axis.\nFigure 9.2: Relationship Buchanan Perot Votes\ncan see one point top right graph seems bit outlier. county much higher number votes Buchanan county. order see county , add option label=county inside aes(). label marker county represents. adjust location size marker, also specify geom_text(vjust=1.5,size=3).\nFigure 9.3: Relationship Buchanan Perot Votes\nchose (vjust=1.5,size=3) make labels easily readable. can play around parameters see changes appearance figure.Now markers labeled can see Palm Beach indeed outlier county. important next question big outlier Palm Beach? enough potentially change outcome election. answer question concretely, first learn estimate linear regression R.","code":"\nflorida <- read_csv(\"florida.csv\")\n#> Rows: 67 Columns: 7\n#>  Column specification \n#> Delimiter: \",\"\n#> chr (1): county\n#> dbl (6): Clinton96, Dole96, Perot96, Bush00, Gore00, Buc...\n#> \n#>  Use `spec()` to retrieve the full column specification for this data.\n#>  Specify the column types or set `show_col_types = FALSE` to quiet this message.\nhead(florida)\n#> # A tibble: 6  7\n#>   county   Clinton96 Dole96 Perot96 Bush00 Gore00 Buchanan00\n#>   <chr>        <dbl>  <dbl>   <dbl>  <dbl>  <dbl>      <dbl>\n#> 1 Alachua      40144  25303    8072  34124  47365        263\n#> 2 Baker         2273   3684     667   5610   2392         73\n#> 3 Bay          17020  28290    5922  38637  18850        248\n#> 4 Bradford      3356   4038     819   5414   3075         65\n#> 5 Brevard      80416  87980   25249 115185  97318        570\n#> 6 Broward     320736 142834   38964 177323 386561        788"},{"path":"voting.html","id":"linear-regression","chapter":"9 Voting","heading":"9.2 Linear Regression","text":"section going review linear regression, estimate linear regression help us understand whether butterfly ballot impacted 2000 U.S. Presidential Election. like thorough review linear regression, look back Chapter 4 includes longer introduction linear regression.review briefly, can write general linear regression equation :\\[\nY_i = \\beta_0 + \\beta_1 cdot X_i + \\varepsilon_i\n\\]\\(Y_i\\) dependent variable interest, \\(X_i\\) independent variable interest, \\(\\beta_0\\) intercept, \\(\\beta_1\\) slope coefficient \\(\\varepsilon_i\\) error term.data, choose estimates \\(\\beta_0\\) \\(\\beta_1\\), denote, \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\), minimize sum squared errors $_{=1}N(Y_i-)2.\\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) tell us? talk \\(\\hat{\\beta}_1\\) first. tells us changes \\(X\\) associated changes \\(Y\\). value positive, increases \\(X\\) associated increase \\(Y\\). negative, increases \\(X\\) associated decreases \\(Y\\). interpretation \\(\\hat{\\beta}_1\\) 1-unit change \\(X\\) associated \\(\\hat{\\beta}_1\\)-unit change \\(Y\\).estimates intercept slope, can also use form predictions. example, predicted value \\(Y\\) individual observable \\(X_i=5\\) equal :\\[\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 5\n\\]\ngood place re-introduce empirical application. goal use number votes Perot 1996 predict number votes Buchanan 2000. One way estimate linear regression following form:\\[\nBuchanan00_i = \\beta_0 + \\beta_1 \\cdot Perot96_i + \\varepsilon_i\n\\]estimate regression equation, can use estimates predict number Buchanan votes Palm Beach County compare actual number votes Palm Beach County. example, hypothetically, predict Buchanan receive 1000 votes Palm Beach, actually received 2000, 1000 votes Palm Beach expected.R, function use estimate linear regressions lm() function. basic syntax command :lm.fit stores estimates linear regression object R.lm.fit stores estimates linear regression object R.yvar replaced dependent variableyvar replaced dependent variablexvar replaced independent variablexvar replaced independent variabledf dataframe environment.df dataframe environment.empirical application, code estimates linear regression Buchanan00 dependent variable Perot96 independent variable:view basic estimates regression, can type:displays estimate intercept (\\(\\hat{\\beta}_0=1.346\\)) slope coefficient (\\(\\hat{\\beta}_1=0.036\\)). intercept tells us best fitted line intersects vertical axis \\(X=0\\). terms example, best fitted line equal 1.346 \\(Perot96_i=0\\). Another way interpret , hypothetically, county cast zero votes Perot 1996, expect county cast 1.346 votes Buchanan 2000.interpret \\(\\hat{\\beta}_1\\), say: 1-unit increase X predicts \\(\\hat{\\beta}_1\\) unit increase Y. terms variables, 1 additional vote Perot 1996, predicts 0.035 votes Buchanan 2000.next section, discuss interpret regression, particular, can visualize results.","code":"\nlm.fit <- lm(yvar ~ xvar, data=df)\nlm.florida <- lm(Buchanan00 ~ Perot96, data=florida)\nlm.florida\n#> \n#> Call:\n#> lm(formula = Buchanan00 ~ Perot96, data = florida)\n#> \n#> Coefficients:\n#> (Intercept)      Perot96  \n#>     1.34575      0.03592"},{"path":"voting.html","id":"plotting-regression-lines","chapter":"9 Voting","heading":"9.3 Plotting Regression Lines","text":"prior section, estimated\\[\nBuchanan00_i = \\beta_0 + \\beta_1 \\cdot Perot96_i + \\varepsilon_i\n\\]\nfound \\(\\hat{\\beta}_0=1.346\\) \\(\\hat{\\beta}_1=0.036\\). coefficients tells us intercept slope line best fits data. section show plot best fit line base R ggplot.begin, create scatter plot Buchanan00 vertical axis Perot96 horizontal axis. also going specify pch=16 code change style markers. pch stands plot character R can used change shape appearance points scatterplot.\nFigure 9.4: Relationship Buchanan Perot Votes\nNow goal add linear regression line plot. , use abline() function. abline() can used add lines plots. pass outcome linear regression abline(), fitted line regression added plot. words, add abline(lm.florida), best-fitted line regression added plot:\nFigure 9.5: Relationship Buchanan Perot Votes\ncan also customize appearance line ways. example, lty=2 make line dashed instead solid. lty stands line type can use variety (see help file plot search lty see different options). col=\"red\" turn line red.\nFigure 9.6: Relationship Buchanan Perot Votes\nNow learn plot regression line ggplot. ggplot, plot command can actually estimate regression . order add plot geom_smooth() function. general geom_smooth adds fitted lines curves plot. want linear regression line, can specify geom_smooth(method=\"lm\"). option lm stands linear model., want specify se=FALSE. default geom_smooth() adds measures uncertainty regression line, cover book. Lastly, use additional options change type color line, . full code generate scatter plot linear regression line appears :\nFigure 9.7: Relationship Buchanan Perot Votes\n","code":"\nplot(florida$Perot96, florida$Buchanan00, \n     pch=16,\n     xlab=\"Votes for Perot in 1996\", \n     ylab=\"Votes for Buchanan in 2000\")\nplot(florida$Perot96, florida$Buchanan00, \n     pch=16,\n     xlab=\"Votes for Perot in 1996\", \n     ylab=\"Votes for Buchanan in 2000\")\n    abline(lm.florida)\nplot(florida$Perot96, florida$Buchanan00, \n     pch=16,\n     xlab=\"Votes for Perot in 1996\", \n     ylab=\"Votes for Buchanan in 2000\")\n    abline(lm.florida, lty=2, col=\"red\")\nggplot(florida, mapping=aes(x=Perot96, y=Buchanan00)) + \n  geom_point() + \n  xlab(\"Votes for Perot in 1996\") + \n  ylab(\"Votes for Buchanan in 2000\") + \n  geom_smooth(method=\"lm\", se=FALSE, linetype=\"dashed\",color=\"red\")\n#> `geom_smooth()` using formula = 'y ~ x'"},{"path":"voting.html","id":"predicted-values-and-residuals","chapter":"9 Voting","heading":"9.4 Predicted Values and Residuals","text":"Now estimated plotted linear regression line, review form predicted values residuals.general, predicted value given :\\[\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot X_i\n\\]gives us best guess value \\(Y_i\\) given value \\(X_i\\), based linear regression estimates. best fit line plotted prior section. can use equation form predicted values. example, imagine tell Alachua county 1996 cast 8,072 votes Ross Perot. Based number linear regression, expect number Pat Buchanan votes cast?\\[\n\\hat{Buchanan00_i}= 1.346 + 0.036 \\cdot 8072 \\approx 292\n\\]words, expect Alachua county cast 292 votes Pat Buchanan. Now, just prediction. using single variable (Ross Perot votes) predict number Buchanan votes. reality, number Buchanan votes likely depends variety factors. Therefore, model error associated , refer residual.residual given difference actual value predicted value. example, Alachua county cast 263 votes Buchanan. residual therefore given :\\[\n\\hat{residual}_i = Y_i - \\hat{Y}_i=263-292 = -29\n\\]Now, manually every single observation, much easier automatically R. allow us construct residual every county data. particularly interested residual Palm Beach county. residual tell us many votes cast Palm Beach Buchanan relative expected based county's voting patterns Perot.estimate linear regression Buchanan00 dependent variable Perot96 independent variable.lm.florida known list object. contains lot information linear regression. can use names() function figure lm.florida contains:many information stored use course. two elements useful us. can see one names fitted.values another residual. Fitted values another name predicted values.can extract information lm.florida using dollar sign operator. example, want add new variable dataset fitted value observation, type:predBuchanan00 name new variable equal fitted values regression estimation. can similarly extract residual lm.florida object add data frame florida.Now, look data try understand process far.Alachua county number Buchanan votes 2000 263. predicted Buchanan votes 291.251 (view data frame able see decimal places). Therefore residual -28.3. reason slightly different formed manually rounded coefficients regression. R computation, much precise , explains predicted values residuals slightly different manual computation.Now order dataset magnitude residual. Observations large residuals predictions different predicted value. sort data based value variable, can use arrange() function.purposes want sort based absolute value residual. Therefore, specify arrange(abs(residuals)). However, addition, want largest residuals top. default, arrange sorts data lowest highest. change , can ask data presented descending order, specifying arrange(desc(abs(residuals))).Additionally, can combine arrange functions using pipe operator. example, really interested variables moment. first select variables, display data largest residual (absolute value) lowest residual.Palm Beach county cast 3407 votes Buchanan. Based linear regression, expected 1105 votes Buchanan. Therefore, 2302 votes expected.Now recall George Bush won Florida 537 votes. Now, excess votes Palm Beach driven design butterfly ballots, seeing excess voting Buchanan individuals confused design accidentally selecting Buchanan instead Gore.true, Al Gore actually received approximately 2,000 votes. enough make winner Florida, given closeness election, winner overall. words, using simple data analysis techniques, found convincing evidence poorly designed ballot single Florida county enough flip 2000 presidential election!","code":"\nlm.florida <- lm(Buchanan00 ~ Perot96, data=florida)\nnames(lm.florida)\n#>  [1] \"coefficients\"  \"residuals\"     \"effects\"      \n#>  [4] \"rank\"          \"fitted.values\" \"assign\"       \n#>  [7] \"qr\"            \"df.residual\"   \"xlevels\"      \n#> [10] \"call\"          \"terms\"         \"model\"\nflorida$predBuchanan00 <- lm.florida$fitted.values\nflorida$residuals <- lm.florida$residuals#> # A tibble: 67  4\n#>    county    Buchanan00 predBuchanan00 residuals\n#>    <chr>          <dbl>          <dbl>     <dbl>\n#>  1 Alachua          263          291.     -28.3 \n#>  2 Baker             73           25.3     47.7 \n#>  3 Bay              248          214.      34.0 \n#>  4 Bradford          65           30.8     34.2 \n#>  5 Brevard          570          908.    -338.  \n#>  6 Broward          788         1401.    -613.  \n#>  7 Calhoun           90           24.0     66.0 \n#>  8 Charlotte        182          281.     -98.9 \n#>  9 Citrus           270          262.       8.49\n#> 10 Clay             186          119.      66.8 \n#> #  with 57 more rows\nflorida %>% \n  select(county,Buchanan00,predBuchanan00,residuals) %>%\n  arrange(desc(abs(residuals)))#> # A tibble: 67  4\n#>    county     Buchanan00 predBuchanan00 residuals\n#>    <chr>           <dbl>          <dbl>     <dbl>\n#>  1 PalmBeach        3407          1105.     2302.\n#>  2 Broward           788          1401.     -613.\n#>  3 Lee               305           662.     -357.\n#>  4 Brevard           570           908.     -338.\n#>  5 Miami-Dade        560           889.     -329.\n#>  6 Pinellas         1013          1330.     -317.\n#>  7 Sarasota          305           538.     -233.\n#>  8 Orange            446           655.     -209.\n#>  9 Escambia          502           310.      192.\n#> 10 St.Lucie          124           306.     -182.\n#> #  with 57 more rows"},{"path":"voting.html","id":"get-out-to-vote-experiments","chapter":"9 Voting","heading":"9.5 Get Out to Vote Experiments","text":"Next, going shift gears another voting application. One notorious issue politics getting people vote. particular, notoriously difficult get voters turn vote primary elections. Primaries incredibly important. select candidates run general election. Yet voter turnout pretty low.example, California, voter turnout general elections 2000-2012 65.5 percent among registered voters. contrast, voter turnout primary elections period around 40.8 percent.One reason lack voting may due lack mobilization. Maybe simple information encouragement voting primaries incentivize individuals vote. Hill Kousser (2016)14, authors test idea implementing large-scale experiment 2014 congressional primary elections California.study, Hill Koussser particularly interested targeting individuals vote general elections, primaries. perhaps individuals treatment successful. engaged politics big occasions (general elections), engaged occasions (primary elections).specific, Hill Kousser sent three types letters individuals advance 2014 primary electionLetter 1: Basic information electionLetter 2: Basic information plus information California top two primariesLetter 3: Basic information plus information turnout respondent's partyThe letters aimed testing different reasons people may may vote. Letter 1 tests whether basic information election occurring helps promote turnout. Letter 2 aimed providing information one particularly confusing element Congressional campaigns California. 2012, law California passed made \"top-two\" primary state. means individual California can vote candidate primary, regardless party affiliation. top two candidates votes continue general election November. system possible top two candidates come party. Hill Kousser wanted test whether lack understanding new system prevented voting.last letter also included information voter turnout among respondents party. authors hypothesize seeing low voter turnout among party may especially effective incentivizing individual vote. words, individuals registered Republican provided Republican voter turnout prior election, individuals registered Democrats provided voter turnout Democrats prior election.One impressive aspect experiment scale. total, Hill Kousser (2016) sent around 150,000 letters Californians election. allow us get precise answers informational interventions impact voter turnout. Additionally, given voting records public data, also large control group experiment: individuals receive one three letters.","code":""},{"path":"voting.html","id":"regression-binary-variables","chapter":"9 Voting","heading":"9.6 Regression (Binary Variables)","text":"study impacts receiving mailer voter turnout, going turn linear regression. purposes, study impact receiving mailer. Therefore, conceptually, think individuals received letter treatment group, individuals receive letter control group. estimate impact receiving mailer estimate linear regression following form:\\[\nY_i = \\beta_0 + \\beta_1 \\cdot X_i + \\varepsilon_i\n\\]\\(Y_i\\) equal 1 individual voted zero otherwise, \\(X_i\\) equal one individual received mailer zero otherwise. bit different examples far, example \\(Y_i\\) \\(X_i\\) binary, meaning take two values.interpretation \\(\\beta_1\\) example. general rule : \\(X_i\\) increases 1, associated \\(\\beta_1\\)-unit change \\(Y_i\\). \\(X_i=0\\) control 1 treatment, 1-unit increase \\(X_i\\) going control treatment. words, moving individual control receiving letter associated \\(\\beta_1\\) unit change \\(Y_i\\). \\(Y_i\\) also binary. imagine moment \\(\\beta_1=0.01\\). \\(0.01\\) change \\(Y_i\\) imply? Well, \\(\\beta_1 \\cdot 100=0.01 \\cdot 100 = 1\\) percentage point increase \\(Y_i\\).Another way think interpretation predicted values. Imagine now \\(\\beta_0=0.5\\) \\(\\beta_1=0.01\\). Next, ask predicted value \\(Y_i\\) individual control (.e. \\(X_i=0\\)):\\[\n\\hat{Y}_i = \\beta_0 + \\beta_1 \\cdot X_i = 0.5 + 0.01 \\cdot 0 = 0.50\n\\]simpler terms, expect 50 percent individuals control vote. Recall take average binary variable takes values zero one, average equal fraction individuals 1. Therefore, case, value 0.5 indicates half individuals value \\(Y_i=1\\). Put even simply, half individuals voted. Next, ask predicted value \\(Y_i\\) individual treatment:\\[\n\\hat{Y}_i = \\beta_0 + \\beta_1 \\cdot X_i = 0.5 + 0.01 \\cdot 1 = 0.51\n\\]\nTherefore, expect 51 percent individuals vote treatment. Therefore, impact going control treatment 1 percentage point increase probability voting. need review difference percent percentage point, check Chapter 1.8. Now understand interpretation linear regression binary variables, go implement regression R assess receiving mailer impacted voter turnout.data application comes csv file named turnout.csv. load R tibble.large dataset. data includes information 3.8 million individuals California. take look small slice data:first variable LocalityCode indicates county individual voting . Party registered party individual. yvar main dependent variable. variable equal 1 individual voted 2014 primary election zero individual vote. treatment.assign indicates whether individual control group (.e. receive letter) one treatment arms. last variable mailer equal 1 individual received mailer, zero individual control group.application, want test whether receiving mailer impacts probability individual votes. Therefore, regress variable yvar variable mailer. Just , need save estimates new object.Now look results:interpret regression. , compute regression predicts regarding voting individual control:\\[\n\\hat{Y}_i = 0.093 + 0.005 \\cdot 0 = 0.093\n\\]simple terms, 9.3 percent individuals control group voted election. individuals treatment, get:\\[\n\\hat{Y}_i = 0.093 + 0.005 \\cdot 0 = 0.093+ 0.005 = 0.098\n\\]Therefore, treatment group, 9.8 percent individuals turned vote. Therefore, treatment expected increase voting rates 0.5 percentage points.","code":"\nturnout <- read_csv(\"turnout.csv\")\n#> Rows: 3872268 Columns: 5\n#>  Column specification \n#> Delimiter: \",\"\n#> chr (2): Party, treatment.assign\n#> dbl (3): LocalityCode, yvar, mailer\n#> \n#>  Use `spec()` to retrieve the full column specification for this data.\n#>  Specify the column types or set `show_col_types = FALSE` to quiet this message.#> # A tibble: 3,872,268  5\n#>    LocalityCode Party  yvar treatment.assign mailer\n#>           <dbl> <chr> <dbl> <chr>             <dbl>\n#>  1            1 REP       0 Control               0\n#>  2            1 NPP       0 Control               0\n#>  3            1 DEM       1 Control               0\n#>  4            1 REP       0 Control               0\n#>  5            1 DEM       0 Control               0\n#>  6            1 NPP       0 Control               0\n#>  7            1 DEM       0 Control               0\n#>  8            1 DEM       0 Control               0\n#>  9            1 REP       0 Control               0\n#> 10            1 NPP       0 Control               0\n#> #  with 3,872,258 more rows\nlm.turnout <- lm(yvar ~ mailer, data=turnout)\nlm.turnout\n#> \n#> Call:\n#> lm(formula = yvar ~ mailer, data = turnout)\n#> \n#> Coefficients:\n#> (Intercept)       mailer  \n#>    0.093125     0.004899"},{"path":"voting.html","id":"conclusion-7","chapter":"9 Voting","heading":"9.7 Conclusion","text":"chapter, introduced two voting examples. first application, showed poorly designed ballot may changed 2000 U.S. Presidential election. second, showed massive campaign sent mailers 150,000 Californians increased voter turnout primary elections.\napplications, used regression analyze data. first application, regression gave us way form predictions. particular, need form prediction Palm Beach County regarding many votes Pat Buchanan (Reform Party Candidate) receive. important election, individuals claimed ballot confusing, led people mistakenly vote Pat Buchanan rather Al Gore (check Figure (fig:butterfly) see agree).However, still needed way data see whether appears \"many\" individuals voted Pat Buchanan. , predicted county-level votes Pat Buchanan using number votes cast Ross Perot 1996. likely good predictor Ross Perot Pat Buchanan party (Reform Party).Using regression estimates, predicted Palm Beach county cast 1105 votes Buchanan. actuality, Palm Beach cast 3407 votes Buchanan. words, Palm Beach 2000 votes Buchanan expected. discrepancy large enough potentially swing election. George Bush won Florida 537 votes. 2000 votes Buchanan cast Al Gore, Al Gore won Florida election overall.second application, used regression estimate treatment effect: effect receiving mailer voter turnout. notoriously difficult get individuals vote primary elections. Hill Kousser (2016), authors sent astounding 150,000 letters Californians, providing different forms information election. scope project allowed authors detect even small changes voting behavior. end, found mailer increased voting 0.5 percentage points. may seem small, applied population California, implies thousands additional voters.","code":""},{"path":"voting.html","id":"function-descriptions-2","chapter":"9 Voting","heading":"Function Descriptions","text":"Functions Usedlm -- Estimates linear regression R. General syntax given lm.fit <- lm(yvar ~ xvar, data=df). lm.fit object created contains results linear regression.\nlm stores many important results linear regression, including coefficients, fitted.values, residuals. extract element type lm.fit$coefficients.\nlm -- Estimates linear regression R. General syntax given lm.fit <- lm(yvar ~ xvar, data=df). lm.fit object created contains results linear regression.lm stores many important results linear regression, including coefficients, fitted.values, residuals. extract element type lm.fit$coefficients.abline -- adds linear regression line plots Base R.abline -- adds linear regression line plots Base R.geom_smooth(method=\"lm\",se=FALSE) -- adds linear regression line plots ggplot2.geom_smooth(method=\"lm\",se=FALSE) -- adds linear regression line plots ggplot2.","code":""},{"path":"unconditional-cash-transfers.html","id":"unconditional-cash-transfers","chapter":"10 Unconditional Cash Transfers","heading":"10 Unconditional Cash Transfers","text":"","code":""},{"path":"unconditional-cash-transfers.html","id":"motivation","chapter":"10 Unconditional Cash Transfers","heading":"10.1 Motivation","text":"central goal development economics understand ways alleviate global poverty. However, exact design policies often debate.example, imagine charity government funds disburse individuals poverty. go ? One response make transfers conditional. words, give individuals cash purchase particular items. example, many programs allocate funds used food education.Another way disburse funds unconditionally. gives individuals autonomy decide like spend money . week, use data Haushofer Shapiro (2016)15, studies impact unconditional cash transfers Kenya economic psychological well-recipients. study made possible due charitable organization, Give Directly, charity co-found Paul Niehaus, professor Economics UC San Diego.study made possible due charitable organization, Give Directly, charity co-found Paul Niehaus, professor Economics UC San Diego. main goal charity alleviate poverty giving funds directly individuals.particular, Haushofer Shapiro randomize couple aspects including (1) whether household receives unconditional cash transfer (2) timing transfer (monthly vs. lump sum) (3) size transfer (small $400 large $1500). key strength study can also study wide range outcomes, including assets, expenditures, food security, health, education expenditures, psychological well .able explore fraction outcomes, exercise focus total assets. can interpret assets amount savings individual particular point time. get started, head R load data.","code":""},{"path":"unconditional-cash-transfers.html","id":"the-data","chapter":"10 Unconditional Cash Transfers","heading":"10.2 The Data","text":"data experiment provided uct.csv file. First, going load tidyverse.Now read uct.csv file:940 observations 19 columns (.e. variables). first variable treat equal 1 individual treated group (received cash payment) zero individual control group (receive cash payment). Two variables particularly interested asset_before asset_after. order understand impact cash transfer, compare changes assets treated workers relative change assets control group experienced.Although focus assets, mentioned previously, wide range variables data. variable, measurement cash payments made. Given range variables data, allow comprehensive study individuals outcomes impacted cash transfers.last columns, also information exact treatment. example, variable treatXlarge equal 1 individual treatment group individual received large cash transfer ($1500). variable treatXmonthlyXsmall equal 1 individual treatment group received small monthly payments. Although study impact particular treatment, design experiment also allows authors understand size timing payments impacts overall welfare.Next, start analyzing data. However, , introduce new concept: user-written functions. far, every function used either come base R package loaded. next section, learn write functions.","code":"\nlibrary(tidyverse)uct <- read_csv(\"uct.csv\")\nRows: 940 Columns: 19\n Column specification \nDelimiter: \",\"\ndbl (19): treat, village, treatXfemalerec, asset_after, ...\n\n Use `spec()` to retrieve the full column specification for this data.\n Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"unconditional-cash-transfers.html","id":"functions-part-1","chapter":"10 Unconditional Cash Transfers","heading":"10.3 Functions (Part 1)","text":"clarify write functions, first review function . function takes input, performs operations input, provides output. example, mean() function R, input vector numbers, output mean vector.data analysis, often find repeating process . Functions can help situations. find repeating analysis copying pasting code, just changing small elements, might time put analysis function.precisely R provided mean() function. calculate means times. Therefore, makes sense function quickly compute mean. analysis complicated? standard function performs ? case, can write function.go creating function R. basically four things need decide. First, function name ? Similar choosing variable names, want something relatively simple, also descriptive. Second, inputs? mean() function input list numbers. inputs can anything: vector, character, dataframe. Lastly, decide function input create output.first introduce general syntax, writing simple function:can little abstract, example inputs, operations, outputs understand. manually create mean function.name function mymean. Therefore, want use can specify mymean(). input named num. input going list numbers. function list numbers. adds sum(num) divides number elements list numbers. example, imagine list numbers 3, 5 7. sum(num)=3+5+7=15 length(num)=3, implying output<-5. last line tells R return number output. Note code create output within curly brackets. curly brackets tell R function starts ends.can actually see works creating vector R.Now apply function mymean() vector myvec.Now understand basics creating function, create function empirical application week studies impact unconditional cash transfers Kenya.goal compare assets treated control households. course, can without creating function, just need take mean asset_after conditional treat==1 compare mean asset_after conditional treat==0.treatment group, average, individuals 791 dollars assets receiving cash transfer. measurements taken 9 months treatment.control group, average, individuals 495 dollars assets. result tells us 9 months program began, treated individuals still higher savings control individuals. indicates individuals immediately consume transfer payment, instead save future.One thing notice two lines code wrote perform analysis vary single number. first, set treat==1 second set treat==0. find writing code multiple times, just changing single element two, might good time write function.now write function varies one input: treatment value. , depending whatever treatment value pass function, function compute mean individuals treatment value. name function treatment input treatvalue.function allowing user pass treatvalue, case either zero 1. , based treatvalue function calculate mean assets individuals treatvalue. example, want calculate mean assets treatment group can type:want calculate mean assets control group, type:Now, often many different ways code function. example, imagine instead using base R, want calculate using tidyverse.Now, can use function way . Functions allow us flexibly decide operations want performed, also implement operations.section, made relatively simple function, function report means given treatment value. next, going expand simple example create complicated function. complicated analysis, using functions may really save time improve clarity.","code":"functionname <- function(input) {\n  \n  Perform operations to produce output \n  \n  return(output)\n  \n}\nmymean <-function(num) {\n  \n  output <- sum(num)/length(num)\n  \n  return(output)\n  \n}\nmyvec <- c(3,5,7)mymean(myvec)\n[1] 5mean(uct$asset_after[uct$treat==1])\n[1] 790.9253mean(uct$asset_after[uct$treat==0])\n[1] 494.8041\ntreatmean <- function(treatvalue){\n  \n  output <- mean(uct$asset_after[uct$treat==treatvalue])\n  \n  return(output)\n\n}treatmean(1)\n[1] 790.9253treatmean(0)\n[1] 494.8041\ntreatmean <- function(treatvalue){\n  output <- uct %>%\n    filter(treat==treatvalue) %>%\n    summarize(assetmean=mean(asset_after))\n  return(output)\n}treatmean(1)\n# A tibble: 1  1\n  assetmean\n      <dbl>\n1      791.\ntreatmean(0)\n# A tibble: 1  1\n  assetmean\n      <dbl>\n1      495."},{"path":"unconditional-cash-transfers.html","id":"functions-part-2","chapter":"10 Unconditional Cash Transfers","heading":"10.4 Functions (Part 2)","text":"section going extend knowledge functions R, particular focus using strings inputs functions. focus ideas, talk goal section. going write function takes single variable name (\"asset\"``` \"total_rev\"``) computes mean variable unconditional cash transfer program, treated control workers. Figure 10.1 depicts goal function. can input single word (e.g. \"asset\") retrieve table depicted output.\nFigure 10.1: Goal Function\nInstead blanks cell, fill averages. , example, cell top left identified treatment individuals policy implemented. supply word \"asset\" function, first cell average asset_after individuals treat==1. know last section, 791. function section, want function automatically fill numbers. order , need understand can supply single string (e.g. \"asset\") R pull relevant variables calculate relevant means treated control individuals.might useful? write function, can simply change variable name retrieve comparisons treated control individuals. lot variables dataset cover wide range economic psychological outcomes, may helpful function can quickly perform analysis different variables.understand function work, first write one variable: asset. code written can put function make general. start, create object named variable equal string \"asset\".Next, object variable, want create variable names asset_after asset_before, names variables dataset. , need learn concatenate two strings. Concatenating strings simply means combining . words, concatenate \"asset\" \"_after\" get \"asset_after\". function allows us R called paste() function. see works:Now look new object:look closely, see actually problem. space \"asset\" \"_after\". problem. want use variableafter reference particular column data frame. column named \"asset _after\". need tell paste() automatically add space two strings. way can using option called sep=. option sep= stands separator. allows us control put two strings. want space two strings, can specify sep=\"\".similarly, also construct variablebefore:Next, want get average two variables treated control individuals. many ways approach R. Today, going first construct two dataframes: one treated individuals one control individuals. going pull relevant variables dataframes take average variables.start, separate treated control data:Next, going use pull() function grab relevant columns. contrasts ways referenced variables dataframe. past, used either $ operator tidyverse used select() times. difference variable name now quotation marks. value variablebefore \"asset_before\". used dollar sign reference variable, type uct$asset_before, uct$\"asset_before\". Instead, going use pull() function, allows us use strings reference variables. example, reference asset_after uct_treat data frame can type pull(uct_treat,variableafter). want take mean , can put entire expression inside mean() function.Now, repeat process 3 remaining elements table want create.treated individuals, assets started 382 increased 791. control individuals, assets started 389 increased 495. groups saw increase assets time period, treated individuals saw much larger increase. evidence unconditional cash transfer causal impact savings.now, successfully constructed table, assets. want able change input time construct different variables. copy paste code repeat different variables, result long file code basically repeats process . much efficient put code function.display function discuss components:function named treat.mean.var takes variable input. pass variable data frame, example asset nondurable_expend. reach line starts output, code exactly . code subsets two separate dataframes, one treated individuals one control individuals, calculates mean variable program.remaining code simply puts together output way able understand. line output <- c(treatafter,treatbefore, controlafter, controlbefore) takes four means puts single vector. code names(output)<- c(\"Treat \", \"Treat \", \"Control \", \"Control \") associating entry vector name. allow us understand function presenting us without look function figure everytime execute .give new function try using \"asset\" input:Now function set , look another variable: nondurable expenditure. Nondurable expenditure goods consumed short time period. example, food common nondurable good.treatment, nondurable expenditures went 174 191. contrast, control, nondurable expenditures actually decreased, starting 184 falling 158. Therefore, appears unconditional cash transfers also impact amount individuals spending nondurable consumption.Haushofer Shapiro (2016) variety outcomes measure. Now written function analysis, can quickly perform analysis across different variables. functions can save time performing data analysis, also improve clarity code write, making concise.","code":"\nvariable <- \"asset\"\nvariableafter <- paste(variable,\"_after\")variableafter\n[1] \"asset _after\"variableafter <- paste(variable,\"_after\",sep=\"\")\nvariableafter\n[1] \"asset_after\"variablebefore <- paste(variable,\"_before\",sep=\"\")\nvariablebefore\n[1] \"asset_before\"\nuct_treat <- filter(uct, treat==1)\nuct_control <- filter(uct, treat==0)mean(pull(uct_treat,variableafter))\n[1] 790.9253# mean of variable after for control \nmean(pull(uct_control,variableafter))\n[1] 494.8041\n\n# mean of variable before for treatment \nmean(pull(uct_treat,variablebefore))\n[1] 382.3934\n\n# mean of variable before for control \nmean(pull(uct_control,variablebefore))\n[1] 389.371\ntreat.mean.var <- function(variable){\n  \n  variableafter <- paste(variable, \"_after\", sep=\"\")\n  variablebefore <- paste(variable, \"_before\", sep=\"\")\n  \n  #Separate out control and treated data\n  uct_treat <- filter(uct, treat==1)\n  uct_control <- filter(uct, treat==0)\n  \n  #Use the pull function \n  #Mean of variable after for treated\n  treatafter <- mean(pull(uct_treat, variableafter))\n  \n  #Mean of variable after for control\n  controlafter <- mean(pull(uct_control, variableafter))\n  \n  #Mean of variable before for treated\n  treatbefore <- mean(pull(uct_treat, variablebefore))\n  \n  #Mean of variable before for control\n  controlbefore <- mean(pull(uct_control, variablebefore))\n  \n  #Create output\n  output <- c(treatafter, treatbefore, controlafter, \n              controlbefore)\n  names(output) <- c(\"Treat After\", \"Treat Before\", \n                     \"Control After\", \"Control Before\")\n  \n  return(output)\n}treat.mean.var(\"asset\")\n   Treat After   Treat Before  Control After Control Before \n      790.9253       382.3934       494.8041       389.3710 treat.mean.var(\"nondurable_expend\")\n   Treat After   Treat Before  Control After Control Before \n      191.2080       174.3455       157.6120       183.5800 "},{"path":"unconditional-cash-transfers.html","id":"conclusion-8","chapter":"10 Unconditional Cash Transfers","heading":"10.5 Conclusion","text":"section going wrap functions reviewing construct R. perform similar analysis previous section, however, section use linear regression analyze data. set analysis, first write regression equation estimate:\\[\nY_i = \\beta_0 + \\beta_1 \\cdot Treat_i + \\varepsilon_i\n\\]dependent variable change outcome. example, interested understanding treatment impacted change assets household, set \\(Y_i\\) equal asset_after-asset_before. estimate \\(\\hat{\\beta}_1\\) gives us differential change assets treated control individuals. example, \\(\\hat{\\beta}_1=100\\), indicates treated individuals experienced larger increase assets individuals control group. example, control group experienced 200 increase assets two time periods studying, treated group experienced 300 increase assets two time periods. case, assets went 100 treated group relative control group.study, number variables format variable_after variable_before. Instead linear regression different variable names, instead going write function takes input \"variable'', estimates regression variable.understand build function, first estimate regression asset. give us idea can generally function. First, need create \\(Y_i\\) variable, equal asset_after-asset_before.Note, added new variable y dataframe. currently vector global environment. previous examples, estimated linear regression variables within dataframe. section, show can also estimate linear regression pair vectors. , create vector treat variable treat within dataframe uct.Now can estimate linear regression. Unlike prior lm() models estimated, need specify dataframe, now directly utilizing vectors global environment. One done adding y dataframe usual, illustration shows alternative way accomplish analysis.see slope coefficient \\(\\hat{\\beta}_1 = 303.10\\). tells us treated group experienced additional 303 dollar gain relative control group. look intercept, know case expected gain individuals control group. words, individuals control group experienced 105 dollar increase assets. However, slope coefficient tells us treated individuals experienced additional 303 dollar gain, total gain (105+303=408).Now generally using strings. , start creating string object called variable equal \"asset\".Next, use paste create \"asset_before\" \"asset_after\".Now can create \\(Y_i\\) equal asset_after-asset_before. can use pull() function grab columns dataframe corresponding variable names:Now can run linear regression just :now figured pass string \"asset\" object named variable perform analysis. Therefore, need make small changes put code function. name function lm.var perform linear regression input variable.review, first line lm.var <- function(variable) { specifies things. First, name function lm.var. Second, function tells R creating new function. variable name input. change time execute function. make sure works, use \"asset\". already performed analysis know return.appears function working properly. Now, can repeat analysis many different variables. example, see looks like total_rev., see positive impact treated households. see bigger increase treated group terms household revenue relative control group.Concept check: Try nondurable_expend food_security make sure understand resulting output.section, studied effect unconditional cash transfers households Kenya using data Haushofer Shapiro (2016). key strength Haushofer Shapiro ability collect wide variety outcomes. paper allows us understand unconditional cash transfers impact economic well , also psychological well .\npaper relates broader burgeoning literature universal basic income (UBI). merits UBI debated intensely, politics, popular press, academia. basic concept UBI individuals receive guaranteed cash payment. guarantee individuals certain level income, fear lead worse economic outcomes individuals choose reduce work.date, examples UBI payments occur large scale extended time. example, Haushofer Shapiro (2016), authors give payments hundred households course year. However, charity, GiveDirectly, researchers currently implementing large-scale UBI program rural Kenya (see Banerjee et al. 2020). program, researchers providing entire villages rural Kenya universal basic income experiment 12 years. results large-scale experiment tremendously informative regarding pros cons UBI.","code":"\ny <- uct$asset_after-uct$asset_before\ntreat <- uct$treatlm.1 <- lm(y ~ treat)\nlm.1\n\nCall:\nlm(formula = y ~ treat)\n\nCoefficients:\n(Intercept)        treat  \n      105.4        303.1  \nvariable  <- \"asset\"\nvariableafter <- paste(variable, \"_after\",sep=\"\")\nvariablebefore <- paste(variable, \"_before\",sep=\"\")\ny <- pull(uct,variableafter) - pull(uct,variablebefore)\ntreat <- pull(uct,treat)lm.1 <- lm(y ~ treat)\nlm.1\n\nCall:\nlm(formula = y ~ treat)\n\nCoefficients:\n(Intercept)        treat  \n      105.4        303.1  \nlm.var <- function(variable){\n  \n  #Create a before and after variable\n  variableafter <- paste(variable, \"_after\", sep=\"\")\n  variablebefore <- paste(variable, \"_before\", sep=\"\")\n  \n  #Create a y variable\n  y <- pull(uct, variableafter) - pull(uct, variablebefore)\n  treat <- pull(uct, treat)\n  \n  #Run the linear regression\n  lm.1 <- lm(y ~ treat)\n  return(lm.1)\n}lm.var(\"asset\")\n\nCall:\nlm(formula = y ~ treat)\n\nCoefficients:\n(Intercept)        treat  \n      105.4        303.1  lm.var(\"total_rev\")\n\nCall:\nlm(formula = y ~ treat)\n\nCoefficients:\n(Intercept)        treat  \n      -39.9         49.0  "},{"path":"unconditional-cash-transfers.html","id":"function-descriptions-3","chapter":"10 Unconditional Cash Transfers","heading":"Function Descriptions","text":"Functions Usedpaste() -- Concatenates strings together. example, paste(\"Hello\",\"World\") results text \"Hello World\".paste() -- Concatenates strings together. example, paste(\"Hello\",\"World\") results text \"Hello World\".pull() -- Extracts columns dataframe. example, given dataframe df variable varname, can extract variable specifying, pull(df,\"varname\").pull() -- Extracts columns dataframe. example, given dataframe df variable varname, can extract variable specifying, pull(df,\"varname\").function() -- Function specify new function R.function() -- Function specify new function R.","code":""},{"path":"references-1.html","id":"references-1","chapter":"References","heading":"References","text":"","code":""}]
